{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 06: Computer Vision - Optical Character Recognition (OCR)\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Extract text from images using OCR\n",
    "- Use TrOCR (Transformer-based OCR) models\n",
    "- Handle both printed and handwritten text\n",
    "- Process documents and receipts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Prerequisites\n\n### Hardware Requirements\n\n| Model Option | Model Name | Size | Min RAM | Recommended Setup | Notes |\n|--------------|------------|------|---------|-------------------|-------|\n| **small (CPU-friendly)** | microsoft/trocr-small-printed | 558MB | 4GB | 4GB RAM, CPU | English printed text, learning |\n| **large/SOTA (GPU-optimized)** | PaddleOCR | ~3.5GB | 8GB | 12GB VRAM (RTX 4080) | 80+ languages, production-grade |\n\n### Software Requirements\n- Python 3.8+\n- Libraries: `transformers`, `torch`, `PIL`\n- Optional: `paddlepaddle`, `paddleocr` (for large/SOTA option)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "**OCR (Optical Character Recognition)** extracts text from images.\n",
    "\n",
    "**Use Cases:**\n",
    "- Document digitization\n",
    "- Receipt processing\n",
    "- License plate recognition\n",
    "- Form extraction\n",
    "\n",
    "**TrOCR:**\n",
    "- Vision Transformer (encoder) + Text Transformer (decoder)\n",
    "- Trained on printed and handwritten text\n",
    "- State-of-the-art accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Expected Behaviors\n\n### First Time Running\n- **Model Download**: ~558MB for TrOCR-small (~3-5 minutes)\n- Downloads both vision encoder and text decoder\n- Cached for subsequent runs\n\n### Setup Cell Output\n```\nPyTorch version: 2.x.x\nCUDA available: True/False\n```\n\n### Model Loading\n```\nModel loaded on: cpu (or cuda)\n```\n- **CPU**: 8-12 seconds to load\n- **GPU**: 4-6 seconds\n\n### OCR Output\n- Returns extracted text as a string\n- Example: `\"Hello World\"` from an image containing that text\n\n### Input Requirements\n- **Best results**: Single line of text, cropped close\n- **Image types**: Printed text (TrOCR), or multi-language/complex layouts (PaddleOCR)\n- **Resolution**: Higher resolution = better accuracy\n- **Background**: Clean backgrounds work best\n\n### Accuracy Expectations\n- **Printed text (trocr-small-printed)**:\n  - Clean, high-res: 92-95% accuracy\n  - Low-res or blurry: 70-85% accuracy\n- **PaddleOCR**:\n  - Clean printed text: 94-97% accuracy\n  - Multi-language support: 80+ languages\n  - Complex layouts: Excellent (tables, forms)\n\n### Performance\n- **Single text region**:\n  - CPU: 1-3 seconds\n  - GPU: 0.3-0.8 seconds\n- **Longer than classification** due to sequence generation\n\n### Common Issues\n- **Multi-line text**: Works best on single lines; process line-by-line (or use PaddleOCR)\n- **Rotated text**: May need rotation correction first (PaddleOCR handles this)\n- **Small text**: Upscale image before OCR\n- **Background noise**: Pre-process to remove noise\n\n### Model Variants\n- **TrOCR-small-printed**: Lightweight, for English typed/printed text (this notebook)\n- **PaddleOCR**: Production-grade, 80+ languages, complex layouts (covered below)\n\n### Expected Output Quality\n- Should match actual text closely\n- May have minor errors with unusual fonts\n- Punctuation generally preserved\n- Case sensitivity maintained",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nfrom transformers import TrOCRProcessor, VisionEncoderDecoderModel, set_seed\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set seed for reproducibility\nset_seed(1103)\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# CHOOSE YOUR MODEL:\n\n# Option 1: small model (CPU-friendly, English printed text)\nMODEL_NAME = \"microsoft/trocr-small-printed\"  # 558MB\n\n# Option 2: large/SOTA model (GPU-optimized, 80+ languages, production-grade)\n# To use PaddleOCR instead, skip to the PaddleOCR section below (cells 14+)\n# PaddleOCR offers: 80+ languages, better accuracy, layout analysis\n# Note: Handwritten text OCR available in PaddleOCR or via trocr-base-handwritten (not covered here)\n\nprint(f\"Selected model: {MODEL_NAME}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and processor\n",
    "processor = TrOCRProcessor.from_pretrained(MODEL_NAME)\n",
    "model = VisionEncoderDecoderModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model loaded on: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(image):\n",
    "    \"\"\"Extract text from an image.\"\"\"\n",
    "    pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "    \n",
    "    generated_ids = model.generate(pixel_values)\n",
    "    text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Extract text from a sample image\n",
    "url = \"https://fki.tic.heia-fr.ch/static/img/a01-122-02.jpg\"  # Handwritten sample\n",
    "image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n",
    "\n",
    "text = extract_text(image)\n",
    "print(f\"\\nExtracted text: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Process multiple text regions\n",
    "def ocr_multiple_regions(image_urls):\n",
    "    \"\"\"Process multiple text images.\"\"\"\n",
    "    for i, url in enumerate(image_urls, 1):\n",
    "        try:\n",
    "            img = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n",
    "            text = extract_text(img)\n",
    "            print(f\"\\nImage {i}: {text}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nImage {i} error: {e}\")\n",
    "\n",
    "# Test with sample URLs\n",
    "sample_urls = [\n",
    "    \"https://fki.tic.heia-fr.ch/static/img/a01-122-02.jpg\"\n",
    "]\n",
    "ocr_multiple_regions(sample_urls)"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Advanced: Multi-language OCR\nprint(\"\\n=== MULTI-LANGUAGE OCR EXAMPLE ===\")\n\n# Initialize OCR for different languages\n# ocr_chinese = PaddleOCR(lang='ch', use_gpu=torch.cuda.is_available())\n# ocr_french = PaddleOCR(lang='fr', use_gpu=torch.cuda.is_available())\n# ocr_german = PaddleOCR(lang='de', use_gpu=torch.cuda.is_available())\n\n# Supported languages (partial list)\nsupported_langs = [\n    'en', 'ch', 'fr', 'de', 'ja', 'ko', 'es', 'pt', 'ru', 'ar',\n    'hi', 'th', 'vi', 'id', 'ms', 'tl', 'nl', 'it', 'pl', 'tr'\n]\n\nprint(\"Supported languages (partial list):\")\nprint(\", \".join(supported_langs))\nprint(\"\\nSee https://github.com/PaddlePaddle/PaddleOCR for full list\")\nprint(\"\\nTo use a different language, initialize with: PaddleOCR(lang='ch') for Chinese, etc.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Visualize OCR results\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\nfig, ax = plt.subplots(1, figsize=(12, 8))\nax.imshow(img)\n\n# Draw bounding boxes\nfor line in result[0]:\n    bbox, (text, confidence) = line\n    # bbox is [[x1,y1], [x2,y2], [x3,y3], [x4,y4]]\n    points = np.array(bbox)\n    \n    # Create polygon\n    polygon = patches.Polygon(\n        points,\n        linewidth=2,\n        edgecolor='red',\n        facecolor='none'\n    )\n    ax.add_patch(polygon)\n    \n    # Add text label\n    ax.text(\n        points[0][0],\n        points[0][1] - 5,\n        f\"{text[:20]}... ({confidence:.2f})\" if len(text) > 20 else f\"{text} ({confidence:.2f})\",\n        fontsize=8,\n        color='red',\n        bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7)\n    )\n\nax.axis('off')\nax.set_title('PaddleOCR Detection Results', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nDetected {len(result[0])} text regions\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Compare with TrOCR\nprint(\"\\n=== COMPARISON: TrOCR vs PaddleOCR ===\")\nprint(f\"{'Aspect':<20} {'TrOCR':<25} {'PaddleOCR'}\")\nprint(\"-\"*70)\nprint(f\"{'Model Size':<20} {'558MB (small)':<25} {'~3.5GB'}\")\nprint(f\"{'Parameters':<20} {'334M':<25} {'0.9B'}\")\nprint(f\"{'Languages':<20} {'English focused':<25} {'80+ languages'}\")\nprint(f\"{'Accuracy':<20} {'Good':<25} {'Excellent'}\")\nprint(f\"{'Speed (CPU)':<20} {'Fast':<25} {'Moderate'}\")\nprint(f\"{'Best For':<20} {'Simple docs':<25} {'Complex layouts'}\")\nprint(\"=\"*70)\n\nprint(\"\\n**When to use PaddleOCR:**\")\nprint(\"  - Multi-language documents\")\nprint(\"  - Complex layouts (tables, forms)\")\nprint(\"  - Production applications requiring high accuracy\")\nprint(\"  - When you have sufficient GPU memory (4GB+ VRAM)\")\n\nprint(\"\\n**When to use TrOCR:**\")\nprint(\"  - Simple English documents\")\nprint(\"  - Limited hardware (CPU-only)\")\nprint(\"  - Faster inference needed\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Load test image (use an image with text)\n# Option 1: Use local file\n# test_image_path = \"sample_data/document.jpg\"\n# img = Image.open(test_image_path)\n\n# Option 2: Use URL\nimport sys\nsys.path.append('..')  # Add parent directory to path for shared_utils\nfrom shared_utils import load_image_from_url\nimg = load_image_from_url(\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/2c/Sample_receipt.jpg/800px-Sample_receipt.jpg\")\nimg_array = np.array(img)\n\n# Run OCR\nprint(\"Running PaddleOCR...\")\nresult = ocr.ocr(img_array, cls=True)\n\n# Display results\nprint(\"\\n\" + \"=\"*70)\nprint(\"PADDLEOCR RESULTS\")\nprint(\"=\"*70)\n\nfor idx, line in enumerate(result[0]):\n    bbox, (text, confidence) = line\n    print(f\"{idx+1}. {text}\")\n    print(f\"   Confidence: {confidence:.4f}\")\n    print(f\"   Bounding box: {bbox}\")\n    print()\n\nprint(\"=\"*70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Install PaddleOCR (if not already installed)\n# !pip install paddlepaddle paddleocr\n\nfrom paddleocr import PaddleOCR\nimport cv2\nimport numpy as np\nfrom PIL import Image\n\n# Initialize PaddleOCR\nprint(\"Initializing PaddleOCR (this may take a moment)...\")\nocr = PaddleOCR(\n    use_angle_cls=True,  # Enable text angle classification\n    lang='en',           # Language: 'en', 'ch', 'fr', 'de', etc.\n    use_gpu=torch.cuda.is_available()\n)\n\nprint(\"PaddleOCR initialized\")\nprint(f\"Using: {'GPU' if torch.cuda.is_available() else 'CPU'}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Using MNIST dataset (12MB, handwritten digits 0-9)\nimport torchvision.datasets as datasets\nfrom PIL import Image\n\nprint(\"Downloading MNIST test dataset...\")\n# Download test set (will cache after first download)\nmnist_test = datasets.MNIST(root='./data', train=False, download=True)\n\nprint(f\"Loaded {len(mnist_test)} test images\\n\")\n\n# Test OCR on a few MNIST digits\nprint(\"=== MNIST Digit Recognition ===\")\nfor i in range(5):\n    img, true_digit = mnist_test[i]\n    \n    # Convert grayscale to RGB (TrOCR expects RGB)\n    img_rgb = img.convert(\"RGB\")\n    \n    # Extract text using OCR\n    extracted_text = extract_text(img_rgb)\n    \n    # Check if prediction matches\n    match = \"✓\" if extracted_text.strip() == str(true_digit) else \"✗\"\n    \n    print(f\"{match} Image {i+1}: True digit = {true_digit}, Predicted = '{extracted_text.strip()}'\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Custom Images**: Create images with text and test OCR accuracy\n",
    "2. **Handwritten vs Printed**: Compare models on different text types\n",
    "3. **Document Processing**: Extract text from a multi-line document\n",
    "4. **Error Analysis**: Test with challenging fonts or low quality images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here for exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "✅ **TrOCR** uses transformers for OCR tasks\n",
    "\n",
    "✅ Separate models for **printed** vs **handwritten** text\n",
    "\n",
    "✅ Works best on **cropped text regions**\n",
    "\n",
    "✅ Can be combined with object detection for full document processing\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Try **Notebook 07**: Audio Speech Recognition\n",
    "- Explore [Document AI](https://huggingface.co/models?pipeline_tag=document-question-answering)\n",
    "- Combine with text detection for end-to-end document OCR\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [TrOCR Paper](https://arxiv.org/abs/2109.10282)\n",
    "- [OCR Guide](https://huggingface.co/docs/transformers/tasks/optical_character_recognition)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}