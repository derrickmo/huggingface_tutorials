{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 05: Computer Vision - Object Detection\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand object detection vs classification\n",
    "- Detect and localize multiple objects in images\n",
    "- Use DETR (DEtection TRansformer) models\n",
    "- Visualize bounding boxes and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "### Hardware Requirements\n",
    "\n",
    "| Model Option | Model Name | Size | Min RAM | Recommended Setup | Notes |\n",
    "|--------------|------------|------|---------|-------------------|-------|\n",
    "| **CPU (Small)** | facebook/detr-resnet-50 | 159MB | 8GB | 8GB RAM, CPU | Manageable on CPU |\n",
    "| **GPU (Medium)** | facebook/detr-resnet-101 | 232MB | 8GB | 10GB VRAM (RTX 4080) | Better accuracy |\n",
    "\n",
    "### Software Requirements\n",
    "- Python 3.8+\n",
    "- Libraries: `transformers`, `torch`, `PIL`\n",
    "- See `requirements.txt` for full list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "**Object Detection** finds and localizes multiple objects in an image.\n",
    "\n",
    "**Difference from Classification:**\n",
    "- Classification: \"What is in the image?\" â†’ \"cat\"\n",
    "- Detection: \"What and where?\" â†’ \"cat at (50, 100, 200, 300)\"\n",
    "\n",
    "**Use Cases:**\n",
    "- Autonomous driving\n",
    "- Security surveillance\n",
    "- Retail analytics\n",
    "- Sports analysis\n",
    "\n",
    "**DETR (DEtection TRansformer):**\n",
    "- End-to-end transformer for object detection\n",
    "- No need for anchor boxes or NMS\n",
    "- Simpler than traditional methods"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Expected Behaviors\n\n### First Time Running\n- **Model Download**: ~159MB for DETR-ResNet-50 (~2-3 minutes)\n- Smaller than classification models due to efficient architecture\n- Cached for future use\n\n### Setup Cell Output\n```\nPyTorch version: 2.x.x\nCUDA available: True/False\n```\n\n### Model Loading\n```\nLoading facebook/detr-resnet-50...\nModel loaded successfully!\n```\n- **CPU**: 5-8 seconds\n- **GPU**: 3-5 seconds\n\n### Detection Output Format\n```python\n[\n  {\n    'label': 'person',\n    'score': 0.9987,\n    'box': {'xmin': 50, 'ymin': 100, 'xmax': 200, 'ymax': 300}\n  },\n  ...\n]\n```\n\n### Bounding Box Format\n- **xmin, ymin**: Top-left corner coordinates\n- **xmax, ymax**: Bottom-right corner coordinates\n- Coordinates in pixels relative to original image size\n\n### Performance Expectations\n- **Detection time**:\n  - CPU: 5-10 seconds per image\n  - GPU: 1-2 seconds per image\n- **Slower than classification** due to finding multiple objects and locations\n\n### Confidence Thresholds\n- **threshold=0.9**: Only very confident detections (may miss some objects)\n- **threshold=0.7**: Balanced (recommended default)\n- **threshold=0.5**: More detections, but some may be incorrect\n- Lower thresholds = more false positives\n\n### Expected Detections\n- Detects **COCO dataset classes** (80 categories):\n  - People, vehicles, animals\n  - Furniture, electronics\n  - Food items, sports equipment\n- Full list: [COCO Categories](https://cocodataset.org/#explore)\n\n### Detection Quality\n- **Single, clear object**: Detects with 90%+ confidence\n- **Multiple objects**: Detects most, may miss small/occluded ones\n- **Crowded scenes**: May merge nearby objects or miss some\n- **Small objects**: Lower detection rate (<32x32 pixels)\n\n### Common Observations\n- Can detect **multiple objects** of same class (e.g., 3 people)\n- Bounding boxes may not be pixel-perfect\n- Occasionally detects background as object (false positive)\n- Works best with well-lit, unoccluded objects",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import required libraries\nimport torch\nfrom transformers import AutoImageProcessor, AutoModelForObjectDetection, pipeline, set_seed\nfrom PIL import Image, ImageDraw, ImageFont\nimport requests\nfrom io import BytesIO\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set seed for reproducibility\nset_seed(1103)\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHOOSE YOUR MODEL:\n",
    "\n",
    "# Option 1: CPU-friendly (recommended for beginners)\n",
    "MODEL_NAME = \"facebook/detr-resnet-50\"  # 159MB\n",
    "\n",
    "# Option 2: GPU-optimized (uncomment if you have RTX 4080 or similar)\n",
    "# MODEL_NAME = \"facebook/detr-resnet-101\"  # 232MB, better accuracy\n",
    "\n",
    "print(f\"Selected model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_from_url(url):\n",
    "    \"\"\"Load an image from a URL.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    return img\n",
    "\n",
    "def draw_bounding_boxes(image, detections, threshold=0.9):\n",
    "    \"\"\"\n",
    "    Draw bounding boxes on an image.\n",
    "    \"\"\"\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    \n",
    "    for detection in detections:\n",
    "        if detection['score'] < threshold:\n",
    "            continue\n",
    "        \n",
    "        box = detection['box']\n",
    "        label = detection['label']\n",
    "        score = detection['score']\n",
    "        \n",
    "        # Draw rectangle\n",
    "        x1, y1, x2, y2 = box['xmin'], box['ymin'], box['xmax'], box['ymax']\n",
    "        draw.rectangle([x1, y1, x2, y2], outline=\"red\", width=3)\n",
    "        \n",
    "        # Draw label\n",
    "        text = f\"{label}: {score:.2f}\"\n",
    "        draw.text((x1, y1-15), text, fill=\"red\")\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Using Pipeline (Simplest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create object detection pipeline\nprint(f\"Loading {MODEL_NAME}...\")\ndetector = pipeline(\n    \"object-detection\",\n    model=MODEL_NAME,\n    device=0 if torch.cuda.is_available() else -1\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and detect objects\n",
    "image_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample.png\"\n",
    "image = load_image_from_url(image_url)\n",
    "\n",
    "print(f\"Image size: {image.size}\")\n",
    "\n",
    "# Detect objects\n",
    "detections = detector(image, threshold=0.9)\n",
    "\n",
    "print(f\"\\n=== DETECTED {len(detections)} OBJECTS ===\")\n",
    "for i, detection in enumerate(detections, 1):\n",
    "    print(f\"{i}. {detection['label']:15s} - Score: {detection['score']:.4f}\")\n",
    "    print(f\"   Box: {detection['box']}\")\n",
    "\n",
    "# Visualize\n",
    "image_with_boxes = draw_bounding_boxes(image.copy(), detections, threshold=0.9)\n",
    "print(\"\\nImage with bounding boxes drawn (display in Jupyter)\")\n",
    "image_with_boxes  # Will display in Jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting Confidence Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different confidence thresholds\n",
    "thresholds = [0.5, 0.7, 0.9]\n",
    "\n",
    "for threshold in thresholds:\n",
    "    detections = detector(image, threshold=threshold)\n",
    "    print(f\"\\nThreshold {threshold}: {len(detections)} objects detected\")\n",
    "    for det in detections[:5]:  # Show top 5\n",
    "        print(f\"  - {det['label']:15s} ({det['score']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Using Model and Processor Directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processor and model\n",
    "processor = AutoImageProcessor.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForObjectDetection.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Move to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model loaded on: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect with more control\n",
    "image = load_image_from_url(\"https://images.unsplash.com/photo-1444212477490-ca407925329e?w=600\")  # street scene\n",
    "\n",
    "# Process image\n",
    "inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Run detection\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Post-process\n",
    "target_sizes = torch.tensor([image.size[::-1]]).to(device)  # (height, width)\n",
    "results = processor.post_process_object_detection(\n",
    "    outputs, \n",
    "    target_sizes=target_sizes, \n",
    "    threshold=0.7\n",
    ")[0]\n",
    "\n",
    "print(f\"\\n=== DETECTED OBJECTS ===\")\n",
    "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "    label_name = model.config.id2label[label.item()]\n",
    "    print(f\"{label_name:15s} - {score.item():.4f}\")\n",
    "    print(f\"  Box: {box.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Model statistics\nprint(\"\\n=== YOLOV8 MODEL FAMILY ===\")\n\nmodels_info = {\n    'YOLOv8n': {'size': '6MB', 'params': '3.2M', 'mAP': '37.3', 'speed': '0.99ms'},\n    'YOLOv8s': {'size': '22MB', 'params': '11.2M', 'mAP': '44.9', 'speed': '1.20ms'},\n    'YOLOv8m': {'size': '52MB', 'params': '25.9M', 'mAP': '50.2', 'speed': '1.83ms'},\n    'YOLOv8l': {'size': '87MB', 'params': '43.7M', 'mAP': '52.9', 'speed': '2.39ms'},\n    'YOLOv8x': {'size': '218MB', 'params': '68.2M', 'mAP': '53.9', 'speed': '3.53ms'},\n}\n\nprint(f\"{'Model':<10} {'Size':<10} {'Params':<10} {'mAP@50-95':<12} {'Speed (GPU)'}\")\nprint(\"-\"*60)\nfor name, info in models_info.items():\n    print(f\"{name:<10} {info['size']:<10} {info['params']:<10} {info['mAP']:<12} {info['speed']}\")\n\nprint(\"\\nðŸ“Š Choose based on your hardware and speed requirements\")\nprint(\"   - Nano (n): Edge devices, real-time video on CPU\")\nprint(\"   - Small (s): Balanced option, good for most use cases\")\nprint(\"   - Medium (m): Higher accuracy, still fast\")\nprint(\"   - Large (l): Production systems with GPU\")\nprint(\"   - XLarge (x): Maximum accuracy, requires powerful GPU\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Advanced: Object tracking in video\nprint(\"\\n=== VIDEO OBJECT TRACKING ===\")\n\n# Process video (if you have a video file)\n# results = yolo_model.track(\n#     source=\"path/to/video.mp4\",\n#     show=True,  # Display results\n#     save=True,  # Save output video\n#     tracker=\"bytetrack.yaml\"  # Tracking algorithm\n# )\n\nprint(\"To process video:\")\nprint(\"  results = yolo_model.track(source='video.mp4', show=True)\")\nprint(\"\\nYOLO can track objects across frames!\")\nprint(\"Useful for:\")\nprint(\"  - People counting\")\nprint(\"  - Vehicle tracking\")\nprint(\"  - Sports analytics\")\nprint(\"  - Surveillance systems\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Performance comparison: DETR vs YOLO\nimport time\n\nprint(\"\\n=== PERFORMANCE COMPARISON ===\")\n\n# Measure YOLO inference time\nimg = test_image_yolo\n\nstart = time.perf_counter()\nfor _ in range(10):\n    _ = yolo_model(img, verbose=False)\nyolo_time = (time.perf_counter() - start) / 10\n\nprint(f\"YOLO average inference time: {yolo_time*1000:.2f} ms\")\nprint(f\"YOLO FPS: {1/yolo_time:.1f}\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"MODEL COMPARISON\")\nprint(\"=\"*70)\nprint(f\"{'Aspect':<20} {'DETR':<25} {'YOLOv8'}\")\nprint(\"-\"*70)\nprint(f\"{'Model Size':<20} {'159MB (ResNet-50)':<25} {'6-218MB (n-x)'}\")\nprint(f\"{'Speed (GPU)':<20} {'~50ms':<25} {f'~{yolo_time*1000:.0f}ms'}\")\nprint(f\"{'FPS (GPU)':<20} {'~20':<25} {f'~{1/yolo_time:.0f}'}\")\nprint(f\"{'Accuracy (COCO)':<20} {'42.0 mAP':<25} {'45-53 mAP'}\")\nprint(f\"{'Best For':<20} {'Research':<25} {'Production'}\")\nprint(f\"{'Training':<20} {'Complex':<25} {'Easy'}\")\nprint(\"=\"*70)\n\nprint(\"\\nðŸ’¡ **When to use YOLO:**\")\nprint(\"  - Real-time applications (video, webcam)\")\nprint(\"  - Production deployments\")\nprint(\"  - Edge devices (mobile, embedded)\")\nprint(\"  - Need for speed over research features\")\n\nprint(\"\\nðŸ’¡ **When to use DETR:**\")\nprint(\"  - Research projects\")\nprint(\"  - Transformer-based architectures\")\nprint(\"  - When speed is not critical\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Visualize detections\nimport matplotlib.pyplot as plt\n\n# YOLO provides built-in visualization\nannotated_img = results[0].plot()  # Returns annotated image as numpy array\n\n# Display\nplt.figure(figsize=(12, 8))\nplt.imshow(cv2.cvtColor(annotated_img, cv2.COLOR_BGR2RGB))\nplt.axis('off')\nplt.title('YOLOv8 Object Detection', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Run detection on test image\n# Use the same image as before for comparison\ntest_image_yolo = load_image_from_url(image_url)\n\n# Run YOLO detection\nresults = yolo_model(test_image_yolo)\n\n# Get detections\ndetections_yolo = results[0]\n\nprint(\"=\"*70)\nprint(\"YOLO DETECTION RESULTS\")\nprint(\"=\"*70)\n\n# Print detections\nfor i, box in enumerate(detections_yolo.boxes):\n    class_id = int(box.cls[0])\n    confidence = float(box.conf[0])\n    bbox = box.xyxy[0].tolist()  # [x1, y1, x2, y2]\n    \n    class_name = yolo_model.names[class_id]\n    \n    print(f\"{i+1}. {class_name}\")\n    print(f\"   Confidence: {confidence:.4f}\")\n    print(f\"   Bounding box: [{bbox[0]:.1f}, {bbox[1]:.1f}, {bbox[2]:.1f}, {bbox[3]:.1f}]\")\n    print()\n\nprint(f\"Total detections: {len(detections_yolo.boxes)}\")\nprint(\"=\"*70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Install YOLOv8 (if not already installed)\n# !pip install ultralytics\n\nfrom ultralytics import YOLO\nimport cv2\nfrom PIL import Image\nimport numpy as np\n\n# Load YOLOv8 model\nprint(\"Loading YOLOv8...\")\n\n# Choose model size:\n# yolov8n.pt = Nano (6MB, fastest)\n# yolov8s.pt = Small (22MB, fast)\n# yolov8m.pt = Medium (52MB, balanced)\n# yolov8l.pt = Large (87MB, accurate)\n# yolov8x.pt = XLarge (218MB, most accurate)\n\nMODEL_SIZE = \"yolov8s.pt\"  # Change to n/s/m/l/x\nyolo_model = YOLO(MODEL_SIZE)\n\nprint(f\"âœ“ YOLOv8-{MODEL_SIZE[6]} loaded\")\nprint(f\"Model size: {MODEL_SIZE}\")\nprint(f\"Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Method 3: Real-Time Detection with YOLOv8\n\n**YOLOv8** (You Only Look Once) is the state-of-the-art real-time object detector:\n- **Industry standard** - Most widely used in production\n- **Blazing fast** - 100+ FPS on GPU\n- **Multiple sizes** - Nano (6MB) to XLarge (218MB)\n- **Better accuracy** - Than DETR on most benchmarks\n- **Easy to use** - Simple API from Ultralytics\n\n**When to use**: Production applications, real-time video, deployment on edge devices.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Count Objects by Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count_objects(image, threshold=0.7):\n",
    "    \"\"\"Count detected objects by category.\"\"\"\n",
    "    detections = detector(image, threshold=threshold)\n",
    "    labels = [d['label'] for d in detections]\n",
    "    counts = Counter(labels)\n",
    "    \n",
    "    print(f\"\\n=== OBJECT COUNTS (threshold={threshold}) ===\")\n",
    "    for label, count in counts.most_common():\n",
    "        print(f\"{label:15s}: {count}\")\n",
    "    \n",
    "    return counts\n",
    "\n",
    "# Test\n",
    "test_image = load_image_from_url(\"https://images.unsplash.com/photo-1449965408869-eaa3f722e40d?w=600\")  # food\n",
    "counts = count_objects(test_image, threshold=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Filter by Object Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_specific_objects(image, target_objects=['person', 'car', 'dog'], threshold=0.7):\n",
    "    \"\"\"\n",
    "    Find specific types of objects in an image.\n",
    "    \"\"\"\n",
    "    detections = detector(image, threshold=threshold)\n",
    "    \n",
    "    found = {obj: [] for obj in target_objects}\n",
    "    \n",
    "    for detection in detections:\n",
    "        label = detection['label']\n",
    "        if label in target_objects:\n",
    "            found[label].append(detection)\n",
    "    \n",
    "    print(\"\\n=== SPECIFIC OBJECT SEARCH ===\")\n",
    "    for obj, instances in found.items():\n",
    "        print(f\"\\n{obj.upper()}: {len(instances)} found\")\n",
    "        for inst in instances:\n",
    "            print(f\"  Score: {inst['score']:.3f}, Box: {inst['box']}\")\n",
    "    \n",
    "    return found\n",
    "\n",
    "# Test\n",
    "test_image = load_image_from_url(image_url)\n",
    "found_objects = find_specific_objects(test_image, target_objects=['person', 'car', 'cup'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Multi-Image Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze multiple images\n",
    "image_urls = [\n",
    "    \"https://images.unsplash.com/photo-1606041011872-596597976b25?w=400\",  # street\n",
    "    \"https://images.unsplash.com/photo-1504674900247-0877df9cc836?w=400\",  # food\n",
    "]\n",
    "\n",
    "for i, url in enumerate(image_urls, 1):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"IMAGE {i}\")\n",
    "    print('='*50)\n",
    "    \n",
    "    try:\n",
    "        img = load_image_from_url(url)\n",
    "        detections = detector(img, threshold=0.8)\n",
    "        \n",
    "        print(f\"Found {len(detections)} objects:\")\n",
    "        for det in detections:\n",
    "            print(f\"  - {det['label']:15s} ({det['score']:.3f})\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## State-of-the-Art Open Models (Not Covered)\n\nWhile we've covered DETR and YOLOv8, there are several groundbreaking detection models that push the boundaries of what's possible. These include self-supervised models, open-vocabulary detectors, and universal segmentation systems.\n\n### Top SOTA Object Detection Models\n\n#### 1. ðŸ¦• DINO (Facebook/Meta)\n**Self-supervised vision transformer for detection**\n- **Why it's special**: Self-supervised pre-training, no labeled data needed for initial training\n- **Performance**: 63.3 AP on COCO, excellent feature representations\n- **Model Card**: [facebook/dinov2-large](https://huggingface.co/facebook/dinov2-large)\n- **Paper**: [DINO: Emerging Properties in Self-Supervised Vision Transformers](https://arxiv.org/abs/2104.14294)\n- **Size**: 1.1GB (ViT-Large backbone)\n\n#### 2. ðŸ” Grounding DINO\n**Open-vocabulary object detection with text prompts**\n- **Why it's special**: Detect ANY object described in text, zero-shot detection\n- **Performance**: Combines DINO + text understanding, detects novel categories\n- **Model Card**: [IDEA-Research/grounding-dino-base](https://huggingface.co/IDEA-Research/grounding-dino-base)\n- **Paper**: [Grounding DINO: Marrying DINO with Grounded Pre-Training](https://arxiv.org/abs/2303.05499)\n- **Size**: 680MB\n\n#### 3. ðŸŽ¯ SAM (Segment Anything Model - Meta)\n**Universal image segmentation at pixel level**\n- **Why it's special**: Segments ANY object with prompts (points, boxes, text), 1B+ mask dataset\n- **Performance**: Zero-shot segmentation, works on any domain\n- **Model Card**: [facebook/sam-vit-huge](https://huggingface.co/facebook/sam-vit-huge)\n- **Paper**: [Segment Anything](https://arxiv.org/abs/2304.02643)\n- **Size**: 2.4GB (ViT-Huge), also 375MB (ViT-Base) available\n\n#### 4. ðŸ“ DETA (Detection with Text Assistance)\n**Object detection enhanced with text prompts**\n- **Why it's special**: Uses text descriptions to improve detection accuracy\n- **Performance**: 63.5 AP on COCO, better than DETR with similar size\n- **Model Card**: Available on GitHub (IDEA-Research/DETA)\n- **Paper**: [NMS Strikes Back](https://arxiv.org/abs/2212.06137)\n- **Size**: ~300MB\n\n### Why Not Covered?\n\nThese advanced models require:\n- **GPU Memory**: 16-48GB VRAM for SAM-huge, 12GB+ for others\n- **Inference Time**: 5-30 seconds per image (vs 1-2s for DETR/YOLO)\n- **Complex Setup**: Some require additional dependencies (CLIP, specialized processors)\n- **Specialized Use Cases**: Most useful for specific applications rather than general learning\n\nDETR and YOLO provide solid foundations before exploring these advanced techniques!\n\n### Learning Path Recommendation\n\n1. **Start here**: Master DETR and YOLOv8 (this notebook)\n2. **Next step**: Try SAM for segmentation tasks (use ViT-Base version)\n3. **Open-vocabulary**: Experiment with Grounding DINO for flexible detection\n4. **Research**: Explore DINO for self-supervised learning projects\n\n### Benchmarks & Leaderboards\n\n- **COCO Object Detection mAP** (val2017):\n  - DETR-ResNet-50: 42.0 mAP\n  - YOLOv8-large: 52.9 mAP\n  - DINO (ViT-Large): 63.3 mAP\n  - DETA: 63.5 mAP\n  - Grounding DINO: ~57 mAP (zero-shot capable)\n\n- **SAM** (Segmentation):\n  - 91.6 mIoU on SA-V dataset\n  - Works on 11M+ images, 1.1B+ masks\n\n- **Explore rankings**: [Papers With Code - Object Detection on COCO](https://paperswithcode.com/sota/object-detection-on-coco)\n\n### Quick Comparison Table\n\n| Model | Size | Speed | mAP | Best For |\n|-------|------|-------|-----|----------|\n| **DETR** â­ | 159MB | Medium | 42.0 | Learning transformers |\n| **YOLOv8** â­ | 6-218MB | Very Fast | 37-54 | Real-time, production |\n| **DINO** | 1.1GB | Slow | 63.3 | High accuracy, research |\n| **Grounding DINO** | 680MB | Slow | 57.0 | Open-vocabulary detection |\n| **SAM** | 375MB-2.4GB | Very Slow | N/A | Universal segmentation |\n| **DETA** | 300MB | Medium | 63.5 | Text-assisted detection |\n\n### Special Capabilities\n\n**ðŸŒŸ What makes these models unique:**\n\n- **SAM**: Click on any object to segment it, no training needed\n- **Grounding DINO**: Detect \"a red car with scratches\" without training on that specific class\n- **DINO**: Learn powerful features without any labels, excellent for transfer learning\n- **DETA**: Improve detection by describing what you're looking for\n\n**ðŸ’¡ Tip**: For production, stick with YOLOv8. For research or specialized applications (zero-shot detection, segmentation), explore SAM and Grounding DINO!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Custom Images**: Test with your own images. What objects are detected?\n",
    "\n",
    "2. **Threshold Tuning**: Find the optimal threshold for different image types.\n",
    "\n",
    "3. **Object Counter**: Build a function that counts total objects across multiple images.\n",
    "\n",
    "4. **Model Comparison**: Compare DETR-ResNet-50 vs DETR-ResNet-101 (if you have GPU).\n",
    "\n",
    "5. **Visualization**: Enhance the bounding box drawing with different colors per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here for exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "âœ… **Object detection** finds both what and where objects are\n",
    "\n",
    "âœ… **DETR** is a modern transformer-based detector\n",
    "\n",
    "âœ… **Confidence threshold** controls detection sensitivity\n",
    "\n",
    "âœ… **Bounding boxes** are in (xmin, ymin, xmax, ymax) format\n",
    "\n",
    "âœ… Detection is more computationally intensive than classification\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Try **Notebook 06**: OCR for text extraction from images\n",
    "- Explore other detection models on [HuggingFace Hub](https://huggingface.co/models?pipeline_tag=object-detection)\n",
    "- Learn about instance segmentation for pixel-level detection\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [DETR Paper](https://arxiv.org/abs/2005.12872)\n",
    "- [Object Detection Guide](https://huggingface.co/docs/transformers/tasks/object_detection)\n",
    "- [COCO Dataset](https://cocodataset.org/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}