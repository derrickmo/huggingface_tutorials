{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 04: Computer Vision - Image Classification\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand image classification with deep learning\n",
    "- Load and use pre-trained vision models\n",
    "- Classify images into predefined categories\n",
    "- Work with the Vision Transformer (ViT) architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "### Hardware Requirements\n",
    "\n",
    "| Model Option | Model Name | Size | Min RAM | Recommended Setup | Notes |\n",
    "|--------------|------------|------|---------|-------------------|-------|\n",
    "| **CPU (Small)** | google/vit-base-patch16-224 | 346MB | 4GB | 4GB RAM, CPU | Good accuracy |\n",
    "| **GPU (Medium)** | google/vit-large-patch16-224 | 1.2GB | 6GB | 8GB VRAM (RTX 4080) | Better accuracy |\n",
    "\n",
    "### Software Requirements\n",
    "- Python 3.8+\n",
    "- Libraries: `transformers`, `torch`, `PIL`\n",
    "- See `requirements.txt` for full list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "**Image Classification** assigns labels to images from a predefined set of categories.\n",
    "\n",
    "**Use Cases:**\n",
    "- Object recognition\n",
    "- Medical image diagnosis\n",
    "- Content moderation\n",
    "- Quality control in manufacturing\n",
    "- Wildlife monitoring\n",
    "\n",
    "**Vision Transformer (ViT):**\n",
    "- Applies transformer architecture to images\n",
    "- Splits image into patches\n",
    "- Treats patches like tokens in NLP\n",
    "- Achieves state-of-the-art results"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Expected Behaviors\n\n### First Time Running\n- **Model Download**: ~346MB for vit-base (~2-4 minutes)\n- Downloads model and image processor\n- Cached in `~/.cache/huggingface/hub/`\n\n### Setup Cell Output\n```\nPyTorch version: 2.x.x\nCUDA available: True/False\nGPU: NVIDIA GeForce RTX 4080 (if available)\n```\n\n### Model Loading\n```\nLoading google/vit-base-patch16-224...\nModel loaded successfully!\n```\n- **CPU**: 3-7 seconds\n- **GPU**: 2-4 seconds\n\n### Classification Output Format\n```python\n[\n  {'label': 'Egyptian cat', 'score': 0.8932},\n  {'label': 'tabby cat', 'score': 0.0854},\n  {'label': 'tiger cat', 'score': 0.0124}\n]\n```\n\n### ImageNet Classes\n- Model trained on ImageNet with **1000 classes**\n- Classes include animals, vehicles, objects, food, etc.\n- Full list: [ImageNet Classes](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a)\n\n### Expected Accuracy\n- **Clear objects** (single cat, car, etc.): 80-95% confidence on top prediction\n- **Multiple objects**: May focus on most prominent object\n- **Unusual angles/lighting**: Lower confidence (60-80%)\n- **Objects not in ImageNet**: May misclassify (model limited to training data)\n\n### Performance Benchmarks\n- **Single image**:\n  - CPU: 200-500ms\n  - GPU: 20-50ms\n- **Batch of 10 images**:\n  - CPU: 1-2 seconds\n  - GPU: 100-200ms\n\n### Image Loading\n- Accepts URLs, local file paths, or PIL Image objects\n- Automatically resizes images to 224x224 pixels\n- Converts to RGB if needed\n- **Common error**: \"Connection timeout\" for slow/blocked URLs\n\n### Top-K Predictions\n- `top_k=5` returns 5 most likely classes\n- Scores sum to approximately 1.0 (probabilities)\n- Lower-ranked predictions have exponentially lower scores\n\n### Common Observations\n- Works best on **centered, well-lit objects**\n- Background clutter reduces confidence\n- Some classes are very specific (e.g., 150+ dog breeds)\n- May confuse similar-looking objects (e.g., wolves vs dogs)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import required libraries\nimport torch\nfrom transformers import AutoImageProcessor, AutoModelForImageClassification, pipeline, set_seed\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set seed for reproducibility\nset_seed(1103)\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHOOSE YOUR MODEL:\n",
    "\n",
    "# Option 1: CPU-friendly (recommended for beginners)\n",
    "MODEL_NAME = \"google/vit-base-patch16-224\"  # 346MB, ViT base\n",
    "\n",
    "# Option 2: GPU-optimized (uncomment if you have RTX 4080 or similar)\n",
    "# MODEL_NAME = \"google/vit-large-patch16-224\"  # 1.2GB, better accuracy\n",
    "\n",
    "print(f\"Selected model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function: Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_from_url(url):\n",
    "    \"\"\"\n",
    "    Load an image from a URL.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    return img\n",
    "\n",
    "def display_image(img, title=\"Image\"):\n",
    "    \"\"\"\n",
    "    Display an image with a title.\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "    print(f\"Size: {img.size}, Mode: {img.mode}\")\n",
    "    # In Jupyter, this will display the image\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Using Pipeline (Simplest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create image classification pipeline\nprint(f\"Loading {MODEL_NAME}...\")\nclassifier = pipeline(\n    \"image-classification\",\n    model=MODEL_NAME,\n    device=0 if torch.cuda.is_available() else -1\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and classify a sample image\n",
    "image_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n",
    "image = load_image_from_url(image_url)\n",
    "\n",
    "# Display image\n",
    "display_image(image, \"Input Image\")\n",
    "\n",
    "# Classify\n",
    "results = classifier(image, top_k=5)\n",
    "\n",
    "print(\"\\n=== TOP 5 PREDICTIONS ===\")\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"{i}. {result['label']:30s} - {result['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with multiple images\n",
    "test_urls = [\n",
    "    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png\",\n",
    "    \"https://images.unsplash.com/photo-1552053831-71594a27632d?w=400\",  # dog\n",
    "    \"https://images.unsplash.com/photo-1511367461989-f85a21fda167?w=400\"   # banana\n",
    "]\n",
    "\n",
    "for i, url in enumerate(test_urls, 1):\n",
    "    try:\n",
    "        img = load_image_from_url(url)\n",
    "        results = classifier(img, top_k=3)\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Image {i}:\")\n",
    "        for result in results:\n",
    "            print(f\"  {result['label']:25s} - {result['score']:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image {i}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Using Model and Processor Directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processor and model\n",
    "processor = AutoImageProcessor.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForImageClassification.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Move to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model loaded on: {device}\")\n",
    "print(f\"Number of classes: {model.config.num_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify with more control\n",
    "import torch.nn.functional as F\n",
    "\n",
    "image = load_image_from_url(\"https://images.unsplash.com/photo-1517849845537-4d257902454a?w=400\")  # dog\n",
    "\n",
    "# Process image\n",
    "inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Get predictions\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probabilities = F.softmax(logits, dim=-1)[0]\n",
    "\n",
    "# Get top 5 predictions\n",
    "top_probs, top_indices = torch.topk(probabilities, k=5)\n",
    "\n",
    "print(\"\\n=== DETAILED PREDICTIONS ===\")\n",
    "for prob, idx in zip(top_probs, top_indices):\n",
    "    label = model.config.id2label[idx.item()]\n",
    "    print(f\"{label:30s} - {prob.item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Batch Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify multiple images efficiently\n",
    "image_urls = [\n",
    "    \"https://images.unsplash.com/photo-1514888286974-6c03e2ca1dba?w=400\",  # cat\n",
    "    \"https://images.unsplash.com/photo-1546527868-ccb7ee7dfa6a?w=400\",  # car\n",
    "    \"https://images.unsplash.com/photo-1501594907352-04cda38ebc29?w=400\"   # birds\n",
    "]\n",
    "\n",
    "images = [load_image_from_url(url) for url in image_urls]\n",
    "\n",
    "# Batch process\n",
    "inputs = processor(images=images, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    predictions = outputs.logits.argmax(dim=-1)\n",
    "\n",
    "print(\"=== BATCH CLASSIFICATION ===\")\n",
    "for i, pred_idx in enumerate(predictions):\n",
    "    label = model.config.id2label[pred_idx.item()]\n",
    "    print(f\"Image {i+1}: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Confidence Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_with_confidence_threshold(image, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Only return predictions above a confidence threshold.\n",
    "    \"\"\"\n",
    "    results = classifier(image, top_k=10)\n",
    "    \n",
    "    confident_predictions = [r for r in results if r['score'] >= threshold]\n",
    "    \n",
    "    if confident_predictions:\n",
    "        print(f\"\\nPredictions with >{threshold*100}% confidence:\")\n",
    "        for pred in confident_predictions:\n",
    "            print(f\"  {pred['label']:30s} - {pred['score']:.4f}\")\n",
    "    else:\n",
    "        print(f\"\\nNo predictions above {threshold*100}% confidence\")\n",
    "        print(\"Top prediction:\")\n",
    "        print(f\"  {results[0]['label']:30s} - {results[0]['score']:.4f}\")\n",
    "    \n",
    "    return confident_predictions\n",
    "\n",
    "# Test\n",
    "image = load_image_from_url(\"https://images.unsplash.com/photo-1518791841217-8f162f1e1131?w=400\")  # cat\n",
    "classify_with_confidence_threshold(image, threshold=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Local Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have local images in sample_data/\n",
    "import os\n",
    "\n",
    "sample_data_path = \"../sample_data\"\n",
    "\n",
    "if os.path.exists(sample_data_path):\n",
    "    image_files = [f for f in os.listdir(sample_data_path) \n",
    "                   if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    \n",
    "    if image_files:\n",
    "        print(\"=== CLASSIFYING LOCAL IMAGES ===\")\n",
    "        for img_file in image_files[:3]:  # Limit to 3\n",
    "            img_path = os.path.join(sample_data_path, img_file)\n",
    "            img = Image.open(img_path)\n",
    "            results = classifier(img, top_k=3)\n",
    "            \n",
    "            print(f\"\\n{img_file}:\")\n",
    "            for result in results:\n",
    "                print(f\"  {result['label']:25s} - {result['score']:.4f}\")\n",
    "    else:\n",
    "        print(\"No images found in sample_data/. Add some .jpg or .png files to test!\")\n",
    "else:\n",
    "    print(\"sample_data/ directory not found. You can add images there for testing.\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Using CIFAR-10 dataset (170MB, 10 classes, 32x32 color images)\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\n\nprint(\"Downloading CIFAR-10 test dataset...\")\n# Download test set (will cache after first download)\ncifar10_test = datasets.CIFAR10(root='./data', train=False, download=True)\n\n# CIFAR-10 class names\ncifar_classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n                 'dog', 'frog', 'horse', 'ship', 'truck']\n\nprint(f\"Loaded {len(cifar10_test)} test images\\n\")\n\n# Classify a few CIFAR-10 images\nprint(\"=== CIFAR-10 Classification ===\")\nfor i in range(5):\n    img, true_label = cifar10_test[i]\n    \n    # Classify the image\n    results = classifier(img, top_k=3)\n    \n    print(f\"\\nImage {i+1}:\")\n    print(f\"  True class: {cifar_classes[true_label]}\")\n    print(f\"  Predictions:\")\n    for j, pred in enumerate(results, 1):\n        print(f\"    {j}. {pred['label']:30s} - {pred['score']:.4f}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## State-of-the-Art Open Models (Not Covered)\n\nWhile Vision Transformer (ViT) is excellent, there are several cutting-edge image classification models that push the boundaries of accuracy and efficiency. These models represent the latest advances in computer vision research.\n\n### Top SOTA Image Classification Models\n\n#### 1. üî∑ ConvNeXt (Facebook/Meta)\n**Modern pure convolutional architecture rivaling transformers**\n- **Why it's special**: Modernized ConvNet design matching ViT performance without attention\n- **Performance**: 87.8% ImageNet accuracy (ConvNeXt-XL), faster inference than ViT\n- **Model Card**: [facebook/convnext-large-224](https://huggingface.co/facebook/convnext-large-224)\n- **Paper**: [A ConvNet for the 2020s](https://arxiv.org/abs/2201.03545)\n- **Size**: 800MB (ConvNeXt-Large)\n\n#### 2. ‚ö° EfficientNetV2 (Google)\n**Optimized scaling for speed and accuracy**\n- **Why it's special**: Progressive learning strategy, extremely efficient training/inference\n- **Performance**: 87.3% ImageNet accuracy at 5x faster training than EfficientNet-B7\n- **Model Card**: [google/efficientnet-b7](https://huggingface.co/google/efficientnet-b7)\n- **Paper**: [EfficientNetV2: Smaller Models and Faster Training](https://arxiv.org/abs/2104.00298)\n- **Size**: 264MB (EfficientNetV2-L)\n\n#### 3. ü™ü Swin Transformer (Microsoft)\n**Hierarchical vision transformer with shifted windows**\n- **Why it's special**: Computes attention in local windows, scales to high-resolution images\n- **Performance**: 87.3% ImageNet accuracy, excellent for downstream tasks\n- **Model Card**: [microsoft/swin-large-patch4-window7-224](https://huggingface.co/microsoft/swin-large-patch4-window7-224)\n- **Paper**: [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030)\n- **Size**: 800MB (Swin-Large)\n\n#### 4. üé® BEiT (Microsoft)\n**BERT pre-training approach for images**\n- **Why it's special**: Masked image modeling (like BERT for text), strong transfer learning\n- **Performance**: 88.6% ImageNet accuracy (BEiT-Large), excellent fine-tuning capability\n- **Model Card**: [microsoft/beit-large-patch16-224](https://huggingface.co/microsoft/beit-large-patch16-224)\n- **Paper**: [BEiT: BERT Pre-Training of Image Transformers](https://arxiv.org/abs/2106.08254)\n- **Size**: 1.2GB (BEiT-Large)\n\n#### 5. üìö DeiT (Facebook)\n**Data-efficient image transformer**\n- **Why it's special**: Distillation-based training, achieves great results with less data\n- **Performance**: 85.2% ImageNet accuracy, trains 3x faster than ViT\n- **Model Card**: [facebook/deit-base-distilled-patch16-224](https://huggingface.co/facebook/deit-base-distilled-patch16-224)\n- **Paper**: [Training data-efficient image transformers](https://arxiv.org/abs/2012.12877)\n- **Size**: 346MB (DeiT-Base)\n\n### Why Not Covered?\n\nThese models require:\n- **GPU Memory**: 12-24GB VRAM for large variants\n- **Inference Time**: 2-5x slower than ViT-base on CPU\n- **Specialized Use Cases**: Benefits most apparent at scale or on specific domains\n- **Training Resources**: Fine-tuning requires significant compute\n\nViT provides an excellent balance of performance and accessibility for learning!\n\n### Learning Path Recommendation\n\n1. **Start here**: Master ViT (this notebook)\n2. **Next step**: Try ConvNeXt or Swin for better accuracy\n3. **Efficiency focus**: Experiment with EfficientNetV2 for deployment\n4. **Research**: Explore BEiT for transfer learning projects\n\n### Benchmarks & Leaderboards\n\n- **ImageNet-1K Top-1 Accuracy** (224x224 resolution):\n  - ViT-Base: 81.8%\n  - DeiT-Base: 85.2%\n  - ConvNeXt-Large: 87.8%\n  - Swin-Large: 87.3%\n  - BEiT-Large: 88.6%\n\n- **Explore rankings**: [Papers With Code - ImageNet](https://paperswithcode.com/sota/image-classification-on-imagenet)\n\n### Quick Comparison Table\n\n| Model | Size | Speed | Accuracy | Best For |\n|-------|------|-------|----------|----------|\n| **ViT-Base** ‚≠ê | 346MB | Fast | 81.8% | Learning, general use |\n| **DeiT-Base** | 346MB | Fast | 85.2% | Data-efficient training |\n| **EfficientNetV2** | 264MB | Very Fast | 87.3% | Production deployment |\n| **ConvNeXt-Large** | 800MB | Medium | 87.8% | High accuracy, pure CNN |\n| **Swin-Large** | 800MB | Medium | 87.3% | High-res images, detection |\n| **BEiT-Large** | 1.2GB | Slow | 88.6% | Transfer learning, fine-tuning |\n\n**üí° Tip**: For real-world applications, ConvNeXt and Swin offer the best accuracy-efficiency trade-off with GPU acceleration!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Custom Images**: Test with your own images. How accurate is the model?\n",
    "\n",
    "2. **Ambiguous Images**: Try images that could fit multiple categories. What does the model predict?\n",
    "\n",
    "3. **Model Comparison**: If you have GPU, compare ViT-base with ViT-large. Is the larger model better?\n",
    "\n",
    "4. **Batch Size**: Experiment with batch processing different numbers of images. How does speed change?\n",
    "\n",
    "5. **Other Models**: Try ResNet-50 (`microsoft/resnet-50`) instead of ViT. Compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here for exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "‚úÖ **Vision Transformer (ViT)** treats images as sequences of patches\n",
    "\n",
    "‚úÖ **ImageNet pre-training** enables recognition of 1000+ object categories\n",
    "\n",
    "‚úÖ **Batch processing** improves efficiency for multiple images\n",
    "\n",
    "‚úÖ **Confidence scores** indicate prediction certainty\n",
    "\n",
    "‚úÖ Models work on both URLs and local files\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Try **Notebook 05**: Object Detection for locating multiple objects\n",
    "- Explore other vision models on [HuggingFace Hub](https://huggingface.co/models?pipeline_tag=image-classification)\n",
    "- Learn about fine-tuning on custom datasets\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Vision Transformer Paper](https://arxiv.org/abs/2010.11929)\n",
    "- [Image Classification Guide](https://huggingface.co/docs/transformers/tasks/image_classification)\n",
    "- [ImageNet Classes](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}