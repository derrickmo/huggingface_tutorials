{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 11: Performance, Caching, and Cost Analysis\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Measure inference latency and throughput\n",
    "- Understand HuggingFace's caching system\n",
    "- Estimate computational costs and memory requirements\n",
    "- Optimize model performance\n",
    "- Manage cache and storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "### Hardware Requirements\n",
    "\n",
    "This notebook can run on any system used for previous notebooks.\n",
    "\n",
    "| Requirement | Specification |\n",
    "|-------------|---------------|\n",
    "| **CPU** | Any modern CPU |\n",
    "| **RAM** | 4GB minimum |\n",
    "| **Storage** | 10GB+ free (for cache analysis) |\n",
    "| **GPU** | Optional (for performance comparison) |\n",
    "\n",
    "### Software Requirements\n",
    "- Python 3.8+\n",
    "- Libraries: `transformers`, `torch`, `psutil`\n",
    "- See `requirements.txt` for full list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Behaviors\n",
    "\n",
    "### What You'll Learn\n",
    "- How to measure latency accurately\n",
    "- Where HuggingFace stores cached models\n",
    "- How to calculate storage and compute costs\n",
    "- Techniques to optimize inference speed\n",
    "\n",
    "### Performance Metrics\n",
    "- **Latency**: Time for single prediction (milliseconds)\n",
    "- **Throughput**: Predictions per second\n",
    "- **Memory**: RAM/VRAM usage during inference\n",
    "- **Storage**: Disk space for cached models\n",
    "\n",
    "### Cache Behavior\n",
    "- First model load: Downloads to cache (~1-10 seconds per MB)\n",
    "- Subsequent loads: Reads from cache (instant)\n",
    "- Cache location: `~/.cache/huggingface/hub/`\n",
    "- Models persist until manually deleted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "**Performance Optimization** is crucial for deploying ML models in production.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Latency**: How fast is a single prediction?\n",
    "- **Throughput**: How many predictions per second?\n",
    "- **Caching**: Reusing downloaded models to save time and bandwidth\n",
    "- **Cost**: Computational resources (time, memory, money)\n",
    "\n",
    "**Why This Matters:**\n",
    "- User experience depends on response time\n",
    "- Cloud costs scale with compute time\n",
    "- Cache management prevents storage issues\n",
    "- Understanding trade-offs helps choose the right model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import required libraries\nimport torch\nimport time\nimport os\nimport psutil\nfrom pathlib import Path\nfrom transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, set_seed\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set seed for reproducibility\nset_seed(1103)\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding the Cache System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache Location and Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find HuggingFace cache directory\n",
    "from huggingface_hub import scan_cache_dir\n",
    "\n",
    "# Get cache info\n",
    "cache_info = scan_cache_dir()\n",
    "\n",
    "print(f\"=== HUGGINGFACE CACHE ===\")\n",
    "print(f\"Cache location: {cache_info.cache_dir}\")\n",
    "print(f\"Number of repos: {len(cache_info.repos)}\")\n",
    "print(f\"Total size: {cache_info.size_on_disk / (1024**3):.2f} GB\")\n",
    "\n",
    "# Show cached models\n",
    "print(f\"\\n=== CACHED MODELS ===\")\n",
    "for i, repo in enumerate(cache_info.repos[:10], 1):  # Show first 10\n",
    "    print(f\"{i}. {repo.repo_id}\")\n",
    "    print(f\"   Size: {repo.size_on_disk / (1024**2):.2f} MB\")\n",
    "    print(f\"   Last accessed: {repo.last_accessed}\")\n",
    "    print()\n",
    "\n",
    "if len(cache_info.repos) > 10:\n",
    "    print(f\"... and {len(cache_info.repos) - 10} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Caching Works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate cache behavior\n",
    "MODEL_NAME = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "print(\"=== FIRST LOAD (may download if not cached) ===\")\n",
    "start = time.time()\n",
    "classifier = pipeline(\"sentiment-analysis\", model=MODEL_NAME, device=-1)\n",
    "first_load_time = time.time() - start\n",
    "print(f\"Time: {first_load_time:.2f} seconds\")\n",
    "\n",
    "# Delete the pipeline to clear memory\n",
    "del classifier\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "print(\"\\n=== SECOND LOAD (from cache) ===\")\n",
    "start = time.time()\n",
    "classifier = pipeline(\"sentiment-analysis\", model=MODEL_NAME, device=-1)\n",
    "second_load_time = time.time() - start\n",
    "print(f\"Time: {second_load_time:.2f} seconds\")\n",
    "\n",
    "print(f\"\\n⚡ Speedup: {first_load_time/second_load_time:.1f}x faster from cache\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze cache usage\n",
    "from huggingface_hub import scan_cache_dir\n",
    "\n",
    "cache_info = scan_cache_dir()\n",
    "\n",
    "# Group by size\n",
    "print(\"=== MODELS BY SIZE ===\")\n",
    "sorted_repos = sorted(cache_info.repos, key=lambda x: x.size_on_disk, reverse=True)\n",
    "\n",
    "for i, repo in enumerate(sorted_repos[:5], 1):\n",
    "    size_mb = repo.size_on_disk / (1024**2)\n",
    "    print(f\"{i}. {repo.repo_id:50s} {size_mb:8.2f} MB\")\n",
    "\n",
    "# Show cleanup strategy\n",
    "print(\"\\n=== CACHE CLEANUP OPTIONS ===\")\n",
    "print(\"To delete specific model:\")\n",
    "print(\"  from huggingface_hub import scan_cache_dir\")\n",
    "print(\"  cache_info = scan_cache_dir()\")\n",
    "print(\"  to_delete = cache_info.repos[0]  # or find by name\")\n",
    "print(\"  delete_strategy = cache_info.delete_revisions(to_delete.repo_id)\")\n",
    "print(\"  delete_strategy.execute()\")\n",
    "print(\"\\nTo clear entire cache:\")\n",
    "print(\"  rm -rf ~/.cache/huggingface/hub/  # WARNING: Deletes all models!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Measuring Latency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Inference Latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accurate latency measurement\n",
    "def measure_latency(pipeline_fn, input_data, num_warmup=3, num_runs=10):\n",
    "    \"\"\"\n",
    "    Measure average latency with warmup.\n",
    "    \n",
    "    Args:\n",
    "        pipeline_fn: HuggingFace pipeline\n",
    "        input_data: Input to the pipeline\n",
    "        num_warmup: Warmup iterations\n",
    "        num_runs: Measurement iterations\n",
    "    \n",
    "    Returns:\n",
    "        dict with latency statistics\n",
    "    \"\"\"\n",
    "    # Warmup\n",
    "    for _ in range(num_warmup):\n",
    "        _ = pipeline_fn(input_data)\n",
    "    \n",
    "    # Measure\n",
    "    times = []\n",
    "    for _ in range(num_runs):\n",
    "        start = time.perf_counter()\n",
    "        _ = pipeline_fn(input_data)\n",
    "        end = time.perf_counter()\n",
    "        times.append(end - start)\n",
    "    \n",
    "    return {\n",
    "        'mean_ms': sum(times) / len(times) * 1000,\n",
    "        'min_ms': min(times) * 1000,\n",
    "        'max_ms': max(times) * 1000,\n",
    "        'std_ms': (sum((t - sum(times)/len(times))**2 for t in times) / len(times))**0.5 * 1000\n",
    "    }\n",
    "\n",
    "# Test with sentiment analysis\n",
    "test_text = \"This is a test sentence for measuring latency.\"\n",
    "stats = measure_latency(classifier, test_text)\n",
    "\n",
    "print(\"=== LATENCY STATISTICS ===\")\n",
    "print(f\"Mean:   {stats['mean_ms']:.2f} ms\")\n",
    "print(f\"Min:    {stats['min_ms']:.2f} ms\")\n",
    "print(f\"Max:    {stats['max_ms']:.2f} ms\")\n",
    "print(f\"StdDev: {stats['std_ms']:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Size vs Latency Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different model sizes\n",
    "models_to_compare = [\n",
    "    (\"distilbert-base-uncased-finetuned-sst-2-english\", \"DistilBERT\", \"268MB\"),\n",
    "    (\"bert-base-uncased\", \"BERT-base\", \"440MB\")\n",
    "]\n",
    "\n",
    "print(\"=== MODEL COMPARISON ===\")\n",
    "print(f\"{'Model':<15} {'Size':<10} {'Latency (ms)':<15} {'Relative Speed'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "baseline_latency = None\n",
    "for model_name, display_name, size in models_to_compare:\n",
    "    try:\n",
    "        # Load model\n",
    "        pipe = pipeline(\"text-classification\", model=model_name, device=-1)\n",
    "        \n",
    "        # Measure\n",
    "        stats = measure_latency(pipe, test_text, num_warmup=2, num_runs=5)\n",
    "        latency = stats['mean_ms']\n",
    "        \n",
    "        if baseline_latency is None:\n",
    "            baseline_latency = latency\n",
    "            relative = \"1.0x (baseline)\"\n",
    "        else:\n",
    "            relative = f\"{latency/baseline_latency:.2f}x\"\n",
    "        \n",
    "        print(f\"{display_name:<15} {size:<10} {latency:>10.2f} ms   {relative}\")\n",
    "        \n",
    "        # Clean up\n",
    "        del pipe\n",
    "    except Exception as e:\n",
    "        print(f\"{display_name:<15} {size:<10} Error: {str(e)[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU vs GPU Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare CPU vs GPU performance\n",
    "if torch.cuda.is_available():\n",
    "    print(\"=== CPU vs GPU COMPARISON ===\")\n",
    "    \n",
    "    # CPU\n",
    "    pipe_cpu = pipeline(\"sentiment-analysis\", model=MODEL_NAME, device=-1)\n",
    "    stats_cpu = measure_latency(pipe_cpu, test_text)\n",
    "    \n",
    "    # GPU\n",
    "    pipe_gpu = pipeline(\"sentiment-analysis\", model=MODEL_NAME, device=0)\n",
    "    stats_gpu = measure_latency(pipe_gpu, test_text)\n",
    "    \n",
    "    print(f\"CPU: {stats_cpu['mean_ms']:.2f} ms\")\n",
    "    print(f\"GPU: {stats_gpu['mean_ms']:.2f} ms\")\n",
    "    print(f\"\\n⚡ GPU is {stats_cpu['mean_ms']/stats_gpu['mean_ms']:.1f}x faster\")\n",
    "    \n",
    "    del pipe_cpu, pipe_gpu\n",
    "else:\n",
    "    print(\"GPU not available. Skipping CPU vs GPU comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Throughput and Batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single vs Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare single vs batch processing\n",
    "num_texts = 100\n",
    "texts = [f\"Sample text number {i} for throughput testing.\" for i in range(num_texts)]\n",
    "\n",
    "print(\"=== THROUGHPUT COMPARISON ===\")\n",
    "\n",
    "# Single processing\n",
    "start = time.perf_counter()\n",
    "for text in texts:\n",
    "    _ = classifier(text)\n",
    "single_time = time.perf_counter() - start\n",
    "single_throughput = num_texts / single_time\n",
    "\n",
    "print(f\"Single processing:\")\n",
    "print(f\"  Time: {single_time:.2f} seconds\")\n",
    "print(f\"  Throughput: {single_throughput:.2f} texts/second\")\n",
    "\n",
    "# Batch processing\n",
    "start = time.perf_counter()\n",
    "_ = classifier(texts)\n",
    "batch_time = time.perf_counter() - start\n",
    "batch_throughput = num_texts / batch_time\n",
    "\n",
    "print(f\"\\nBatch processing:\")\n",
    "print(f\"  Time: {batch_time:.2f} seconds\")\n",
    "print(f\"  Throughput: {batch_throughput:.2f} texts/second\")\n",
    "\n",
    "print(f\"\\n⚡ Batch processing is {batch_throughput/single_throughput:.1f}x faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Memory Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAM Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure memory usage\n",
    "import gc\n",
    "\n",
    "# Get baseline memory\n",
    "gc.collect()\n",
    "process = psutil.Process()\n",
    "baseline_memory = process.memory_info().rss / (1024**2)  # MB\n",
    "\n",
    "print(f\"Baseline memory: {baseline_memory:.2f} MB\")\n",
    "\n",
    "# Load model and measure\n",
    "print(\"\\nLoading model...\")\n",
    "test_pipe = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\", device=-1)\n",
    "after_load_memory = process.memory_info().rss / (1024**2)\n",
    "\n",
    "model_memory = after_load_memory - baseline_memory\n",
    "print(f\"Memory after loading: {after_load_memory:.2f} MB\")\n",
    "print(f\"Model memory usage: {model_memory:.2f} MB\")\n",
    "\n",
    "# Run inference and measure\n",
    "_ = test_pipe(\"Test text\")\n",
    "after_inference_memory = process.memory_info().rss / (1024**2)\n",
    "\n",
    "print(f\"\\nMemory after inference: {after_inference_memory:.2f} MB\")\n",
    "print(f\"Inference overhead: {after_inference_memory - after_load_memory:.2f} MB\")\n",
    "\n",
    "del test_pipe\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Memory (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"=== GPU MEMORY USAGE ===\")\n",
    "    \n",
    "    # Clear GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "    baseline_gpu = torch.cuda.memory_allocated() / (1024**2)\n",
    "    \n",
    "    print(f\"Baseline GPU memory: {baseline_gpu:.2f} MB\")\n",
    "    \n",
    "    # Load model on GPU\n",
    "    gpu_pipe = pipeline(\"sentiment-analysis\", model=MODEL_NAME, device=0)\n",
    "    after_load_gpu = torch.cuda.memory_allocated() / (1024**2)\n",
    "    \n",
    "    print(f\"GPU memory after loading: {after_load_gpu:.2f} MB\")\n",
    "    print(f\"Model size on GPU: {after_load_gpu - baseline_gpu:.2f} MB\")\n",
    "    \n",
    "    # Run inference\n",
    "    _ = gpu_pipe(\"Test text\")\n",
    "    after_inference_gpu = torch.cuda.memory_allocated() / (1024**2)\n",
    "    \n",
    "    print(f\"GPU memory after inference: {after_inference_gpu:.2f} MB\")\n",
    "    \n",
    "    # Peak memory\n",
    "    peak_gpu = torch.cuda.max_memory_allocated() / (1024**2)\n",
    "    print(f\"Peak GPU memory: {peak_gpu:.2f} MB\")\n",
    "    \n",
    "    del gpu_pipe\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"GPU not available. Skipping GPU memory analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Cost Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloud Cost Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate cloud costs\n",
    "def estimate_cloud_cost(latency_seconds, requests_per_day, compute_cost_per_hour=0.50):\n",
    "    \"\"\"\n",
    "    Estimate daily cloud computing cost.\n",
    "    \n",
    "    Args:\n",
    "        latency_seconds: Average inference time per request\n",
    "        requests_per_day: Number of requests per day\n",
    "        compute_cost_per_hour: Cloud compute cost (e.g., AWS, GCP)\n",
    "    \n",
    "    Returns:\n",
    "        dict with cost breakdown\n",
    "    \"\"\"\n",
    "    total_compute_seconds = latency_seconds * requests_per_day\n",
    "    total_compute_hours = total_compute_seconds / 3600\n",
    "    daily_cost = total_compute_hours * compute_cost_per_hour\n",
    "    monthly_cost = daily_cost * 30\n",
    "    yearly_cost = daily_cost * 365\n",
    "    \n",
    "    return {\n",
    "        'compute_hours_per_day': total_compute_hours,\n",
    "        'daily_cost': daily_cost,\n",
    "        'monthly_cost': monthly_cost,\n",
    "        'yearly_cost': yearly_cost\n",
    "    }\n",
    "\n",
    "# Example: Sentiment analysis service\n",
    "latency = 0.05  # 50ms average\n",
    "requests = 10000  # 10k requests per day\n",
    "\n",
    "costs = estimate_cloud_cost(latency, requests, compute_cost_per_hour=0.50)\n",
    "\n",
    "print(\"=== COST ESTIMATION ===\")\n",
    "print(f\"Scenario: {requests:,} requests/day, {latency*1000:.0f}ms per request\")\n",
    "print(f\"Compute cost: $0.50/hour (example)\")\n",
    "print(f\"\\nEstimated costs:\")\n",
    "print(f\"  Daily:   ${costs['daily_cost']:.2f}\")\n",
    "print(f\"  Monthly: ${costs['monthly_cost']:.2f}\")\n",
    "print(f\"  Yearly:  ${costs['yearly_cost']:.2f}\")\n",
    "\n",
    "# Compare with faster model\n",
    "print(\"\\n=== OPTIMIZATION IMPACT ===\")\n",
    "faster_latency = latency * 0.5  # 2x faster model\n",
    "faster_costs = estimate_cloud_cost(faster_latency, requests, compute_cost_per_hour=0.50)\n",
    "savings = costs['yearly_cost'] - faster_costs['yearly_cost']\n",
    "\n",
    "print(f\"With 2x faster model:\")\n",
    "print(f\"  Yearly cost: ${faster_costs['yearly_cost']:.2f}\")\n",
    "print(f\"  Savings: ${savings:.2f}/year ({savings/costs['yearly_cost']*100:.0f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storage Costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate storage costs\n",
    "def estimate_storage_cost(model_size_gb, storage_cost_per_gb_month=0.02):\n",
    "    \"\"\"\n",
    "    Estimate monthly storage cost.\n",
    "    \n",
    "    Args:\n",
    "        model_size_gb: Model size in GB\n",
    "        storage_cost_per_gb_month: Storage cost (e.g., S3, GCS)\n",
    "    \n",
    "    Returns:\n",
    "        Monthly storage cost\n",
    "    \"\"\"\n",
    "    return model_size_gb * storage_cost_per_gb_month\n",
    "\n",
    "print(\"=== STORAGE COST COMPARISON ===\")\n",
    "models = [\n",
    "    (\"distilgpt2\", 0.082),\n",
    "    (\"distilbert\", 0.268),\n",
    "    (\"gpt2-medium\", 1.5),\n",
    "    (\"bart-large\", 1.6)\n",
    "]\n",
    "\n",
    "print(f\"{'Model':<20} {'Size (GB)':<12} {'Monthly Cost'}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for name, size in models:\n",
    "    cost = estimate_storage_cost(size, storage_cost_per_gb_month=0.02)\n",
    "    print(f\"{name:<20} {size:<12.3f} ${cost:.4f}\")\n",
    "\n",
    "print(\"\\nNote: Storage costs are minimal compared to compute costs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Optimization Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare FP32 vs INT8 (quantized)\n",
    "print(\"=== QUANTIZATION COMPARISON ===\")\n",
    "print(\"Note: Full quantization example requires additional setup.\")\n",
    "print(\"\\nBenefits of quantization:\")\n",
    "print(\"  - 4x smaller model size (FP32 → INT8)\")\n",
    "print(\"  - 2-4x faster inference\")\n",
    "print(\"  - Minimal accuracy loss (<1% typically)\")\n",
    "print(\"\\nTrade-offs:\")\n",
    "print(\"  - Slight accuracy degradation\")\n",
    "print(\"  - Not all models support quantization\")\n",
    "print(\"  - May require calibration dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficient Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast tokenizers\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "print(\"=== TOKENIZER COMPARISON ===\")\n",
    "\n",
    "tokenizer_slow = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "tokenizer_fast = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "test_texts = [\"Sample text for tokenization speed test.\"] * 1000\n",
    "\n",
    "# Slow tokenizer\n",
    "start = time.perf_counter()\n",
    "for text in test_texts:\n",
    "    _ = tokenizer_slow(text)\n",
    "slow_time = time.perf_counter() - start\n",
    "\n",
    "# Fast tokenizer\n",
    "start = time.perf_counter()\n",
    "for text in test_texts:\n",
    "    _ = tokenizer_fast(text)\n",
    "fast_time = time.perf_counter() - start\n",
    "\n",
    "print(f\"Slow tokenizer: {slow_time:.3f} seconds\")\n",
    "print(f\"Fast tokenizer: {fast_time:.3f} seconds\")\n",
    "print(f\"\\n⚡ Fast tokenizer is {slow_time/fast_time:.1f}x faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Cache Analysis**: Explore your cache directory and calculate total storage used by all models.\n",
    "\n",
    "2. **Latency Profiling**: Measure latency for image classification and compare with text classification. Which is faster?\n",
    "\n",
    "3. **Cost Modeling**: Calculate the cost of running a chatbot with 1 million requests per month.\n",
    "\n",
    "4. **Batch Optimization**: Experiment with different batch sizes. What's the optimal batch size for your hardware?\n",
    "\n",
    "5. **Memory Profiling**: Load multiple models simultaneously and monitor memory usage. When do you run out of RAM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here for exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "✅ **Caching** saves time and bandwidth by reusing downloaded models\n",
    "\n",
    "✅ **Latency** varies by model size, hardware, and optimization\n",
    "\n",
    "✅ **Batch processing** significantly improves throughput\n",
    "\n",
    "✅ **GPU** provides 5-20x speedup for most models\n",
    "\n",
    "✅ **Costs** scale with compute time, not model size\n",
    "\n",
    "✅ **Optimization** (quantization, fast tokenizers) reduces latency and costs\n",
    "\n",
    "✅ **Trade-offs** exist between speed, accuracy, and resource usage\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Try **Notebook 12**: Model Cards and Responsible AI\n",
    "- Explore [HuggingFace Optimization Docs](https://huggingface.co/docs/transformers/performance)\n",
    "- Learn about [Model Compression Techniques](https://huggingface.co/docs/optimum/)\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [HuggingFace Cache Management](https://huggingface.co/docs/huggingface_hub/guides/manage-cache)\n",
    "- [Performance and Optimization](https://huggingface.co/docs/transformers/performance)\n",
    "- [Cloud Pricing Calculators](https://aws.amazon.com/pricing/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}