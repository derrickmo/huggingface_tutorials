{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 10: Ollama Integration with HuggingFace\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Use Ollama to run local LLMs\n",
    "- Integrate TinyLlama with HuggingFace tools\n",
    "- Understand local vs cloud model deployment\n",
    "- Combine Ollama with HuggingFace tokenizers and tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "### Hardware Requirements\n",
    "\n",
    "| Model Option | Model Name | Size | Min RAM | Recommended Setup | Notes |\n",
    "|--------------|------------|------|---------|-------------------|-------|\n",
    "| **CPU/GPU** | TinyLlama (via Ollama) | 637MB | 4GB | 8GB RAM | Fast local inference |\n",
    "\n",
    "### Software Requirements\n",
    "- Python 3.8+\n",
    "- **Ollama installed** ([ollama.com](https://ollama.com))\n",
    "- Libraries: `transformers`, `ollama`\n",
    "\n",
    "### Installation\n",
    "\n",
    "1. **Install Ollama**:\n",
    "   - Visit [ollama.com](https://ollama.com) and download for your OS\n",
    "   - Or use: `curl -fsSL https://ollama.com/install.sh | sh` (Linux/Mac)\n",
    "\n",
    "2. **Pull TinyLlama**:\n",
    "   ```bash\n",
    "   ollama pull tinyllama\n",
    "   ```\n",
    "\n",
    "3. **Install Python package**:\n",
    "   ```bash\n",
    "   pip install ollama\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "**Ollama** is a tool for running large language models locally.\n",
    "\n",
    "**Benefits:**\n",
    "- Privacy: Data stays on your machine\n",
    "- No API costs\n",
    "- Works offline\n",
    "- Easy model management\n",
    "\n",
    "**TinyLlama:**\n",
    "- 1.1B parameter model\n",
    "- Trained on 3 trillion tokens\n",
    "- Fast inference on CPU\n",
    "- Good for educational purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Expected Behaviors\n\n### Prerequisites\n- **Ollama must be installed** from [ollama.com](https://ollama.com)\n- **TinyLlama must be pulled**: `ollama pull tinyllama`\n- **Ollama daemon must be running** in background\n\n### First Time Running\n- **No model download in notebook** (handled by Ollama)\n- TinyLlama: ~637MB downloaded via Ollama CLI\n- HuggingFace tokenizer: ~500KB\n\n### Setup Cell Output\n```\nLibraries imported successfully!\n```\n\n### Checking Ollama Installation\n```python\n# If successful:\n=== AVAILABLE OLLAMA MODELS ===\n  - tinyllama:latest (637 MB)\n\n# If Ollama not running:\nError: ... Connection refused\nMake sure Ollama is installed and running.\n```\n\n### Text Generation Output\n- Returns generated text as string\n- Quality similar to other small LLMs\n- Good for simple tasks, not as powerful as GPT-3.5/4\n\n### Chat Interface\n- Supports system prompts and message history\n- **Streaming**: Text appears token-by-token\n- **Non-streaming**: Returns complete response\n\n### Performance\n- **Short prompt** (10-20 words):\n  - Response time: 2-5 seconds\n  - Varies by system specs\n- **Longer prompt** (100+ words):\n  - Response time: 5-15 seconds\n\n### Response Quality\n- **TinyLlama (1.1B parameters)**:\n  - Good for simple questions\n  - May struggle with complex reasoning\n  - Sometimes repetitive\n  - Can make factual errors\n\n### Temperature Effects\n- **0.3**: Focused, deterministic responses\n- **0.7**: Balanced (recommended)\n- **1.2**: Creative, more varied, sometimes incoherent\n\n### HuggingFace Integration\n- Can use HF tokenizers to count tokens\n- Analyze prompts before sending to Ollama\n- Combine with other HF tools (e.g., summarize then query)\n\n### Common Issues\n- **\"Connection refused\"**: Ollama not running\n  - Solution: Start Ollama daemon\n- **\"Model not found\"**: TinyLlama not pulled\n  - Solution: `ollama pull tinyllama`\n- **Slow responses**: Normal for CPU inference\n  - TinyLlama is optimized for CPU but still takes time\n\n### Ollama vs HuggingFace\n- **Ollama**: Easier setup, optimized for local inference, fewer models\n- **HuggingFace**: More models, more control, steeper learning curve\n\n### Multi-turn Conversations\n- Maintains conversation context\n- Each turn adds to message history\n- Context window: ~2048 tokens for TinyLlama\n\n### Expected Behavior Examples\n- Simple questions: Usually correct\n- Math: May make errors\n- Creative writing: Decent quality\n- Code generation: Basic, often needs editing",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import libraries\nimport random\nimport ollama\nfrom transformers import AutoTokenizer\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set seed for reproducibility\nrandom.seed(1103)\n\nprint(\"Libraries imported successfully!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Ollama Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Ollama is running and list available models\n",
    "try:\n",
    "    models = ollama.list()\n",
    "    print(\"=== AVAILABLE OLLAMA MODELS ===\")\n",
    "    if models.get('models'):\n",
    "        for model in models['models']:\n",
    "            print(f\"  - {model['name']} ({model.get('size', 'unknown')} MB)\")\n",
    "    else:\n",
    "        print(\"No models found. Run: ollama pull tinyllama\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"\\nMake sure Ollama is installed and running.\")\n",
    "    print(\"Visit: https://ollama.com for installation instructions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Ollama Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple text generation\n",
    "def generate_with_ollama(prompt, model=\"tinyllama\"):\n",
    "    \"\"\"\n",
    "    Generate text using Ollama.\n",
    "    \"\"\"\n",
    "    response = ollama.generate(\n",
    "        model=model,\n",
    "        prompt=prompt\n",
    "    )\n",
    "    return response['response']\n",
    "\n",
    "# Test\n",
    "prompt = \"Explain what machine learning is in simple terms.\"\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "\n",
    "response = generate_with_ollama(prompt)\n",
    "print(f\"Response:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat with streaming\n",
    "def chat_with_ollama(messages, model=\"tinyllama\", stream=True):\n",
    "    \"\"\"\n",
    "    Chat with Ollama using message history.\n",
    "    \"\"\"\n",
    "    response = ollama.chat(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        stream=stream\n",
    "    )\n",
    "    \n",
    "    if stream:\n",
    "        full_response = \"\"\n",
    "        for chunk in response:\n",
    "            content = chunk['message']['content']\n",
    "            print(content, end='', flush=True)\n",
    "            full_response += content\n",
    "        print()  # New line\n",
    "        return full_response\n",
    "    else:\n",
    "        return response['message']['content']\n",
    "\n",
    "# Test chat\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What are the benefits of using local LLMs?\"}\n",
    "]\n",
    "\n",
    "print(\"Assistant: \", end='')\n",
    "response = chat_with_ollama(messages, stream=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Ollama with HuggingFace Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HuggingFace tokenizer for TinyLlama\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "\n",
    "print(\"HuggingFace tokenizer loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze token count before sending to Ollama\n",
    "def analyze_and_generate(prompt, max_tokens=100):\n",
    "    \"\"\"\n",
    "    Analyze prompt with HF tokenizer, then generate with Ollama.\n",
    "    \"\"\"\n",
    "    # Tokenize with HuggingFace\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    token_count = len(tokens)\n",
    "    \n",
    "    print(f\"=== TOKEN ANALYSIS ===\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Token count: {token_count}\")\n",
    "    print(f\"Tokens: {tokens[:20]}...\" if token_count > 20 else f\"Tokens: {tokens}\")\n",
    "    \n",
    "    # Generate with Ollama\n",
    "    if token_count < max_tokens:\n",
    "        print(f\"\\n=== GENERATION ===\")\n",
    "        response = generate_with_ollama(prompt)\n",
    "        print(response)\n",
    "        \n",
    "        # Analyze response\n",
    "        response_tokens = tokenizer.encode(response)\n",
    "        print(f\"\\nResponse token count: {len(response_tokens)}\")\n",
    "    else:\n",
    "        print(f\"\\nPrompt too long! ({token_count} > {max_tokens})\")\n",
    "\n",
    "# Test\n",
    "analyze_and_generate(\"What is the capital of France?\", max_tokens=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Multi-turn conversation\n",
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a coding tutor.\"},\n",
    "]\n",
    "\n",
    "questions = [\n",
    "    \"What is a function in Python?\",\n",
    "    \"Can you show me an example?\",\n",
    "    \"How do I add parameters?\"\n",
    "]\n",
    "\n",
    "print(\"=== MULTI-TURN CONVERSATION ===\")\n",
    "for question in questions:\n",
    "    print(f\"\\nUser: {question}\")\n",
    "    conversation.append({\"role\": \"user\", \"content\": question})\n",
    "    \n",
    "    response = ollama.chat(model=\"tinyllama\", messages=conversation)\n",
    "    assistant_message = response['message']['content']\n",
    "    \n",
    "    print(f\"\\nAssistant: {assistant_message}\")\n",
    "    conversation.append({\"role\": \"assistant\", \"content\": assistant_message})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Batch processing\n",
    "prompts = [\n",
    "    \"Summarize: Machine learning is a subset of AI.\",\n",
    "    \"Translate to simple terms: Neural networks process data through layers.\",\n",
    "    \"Complete: The three main types of machine learning are\"\n",
    "]\n",
    "\n",
    "print(\"=== BATCH PROCESSING ===\")\n",
    "for i, prompt in enumerate(prompts, 1):\n",
    "    print(f\"\\n{i}. Prompt: {prompt}\")\n",
    "    response = generate_with_ollama(prompt)\n",
    "    print(f\"   Response: {response[:100]}...\")  # First 100 chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Parameter control\n",
    "def generate_with_params(prompt, temperature=0.7, top_p=0.9, top_k=40):\n",
    "    \"\"\"\n",
    "    Generate with custom parameters.\n",
    "    \"\"\"\n",
    "    response = ollama.generate(\n",
    "        model=\"tinyllama\",\n",
    "        prompt=prompt,\n",
    "        options={\n",
    "            \"temperature\": temperature,\n",
    "            \"top_p\": top_p,\n",
    "            \"top_k\": top_k\n",
    "        }\n",
    "    )\n",
    "    return response['response']\n",
    "\n",
    "# Compare temperatures\n",
    "prompt = \"Write a creative opening for a sci-fi story.\"\n",
    "temps = [0.3, 0.7, 1.2]\n",
    "\n",
    "print(\"=== TEMPERATURE COMPARISON ===\")\n",
    "for temp in temps:\n",
    "    print(f\"\\nTemperature {temp}:\")\n",
    "    response = generate_with_params(prompt, temperature=temp)\n",
    "    print(response[:150])  # First 150 chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "prompt = \"Explain quantum computing in one sentence.\"\n",
    "\n",
    "# Benchmark\n",
    "start_time = time.time()\n",
    "response = generate_with_ollama(prompt)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"=== PERFORMANCE ===\")\n",
    "print(f\"Response: {response}\")\n",
    "print(f\"\\nTime: {end_time - start_time:.2f} seconds\")\n",
    "print(f\"Model: TinyLlama (via Ollama)\")\n",
    "print(f\"Running: Locally on your machine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama vs HuggingFace Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "=== OLLAMA VS HUGGINGFACE TRANSFORMERS ===\n",
    "\n",
    "OLLAMA:\n",
    "+ Easy model management (pull, run, delete)\n",
    "+ Optimized for local inference\n",
    "+ Built-in model quantization\n",
    "+ Simple API\n",
    "+ No code for basic use\n",
    "- Fewer model options\n",
    "- Less fine-tuning control\n",
    "\n",
    "HUGGINGFACE TRANSFORMERS:\n",
    "+ Huge model library (500k+ models)\n",
    "+ Full control over architecture\n",
    "+ Advanced fine-tuning capabilities\n",
    "+ Research-oriented features\n",
    "+ Custom model architectures\n",
    "- More complex setup\n",
    "- Requires more code\n",
    "- Manual optimization needed\n",
    "\n",
    "BEST PRACTICE:\n",
    "Use Ollama for: Quick prototyping, demos, local chatbots\n",
    "Use HuggingFace for: Research, custom models, fine-tuning, production\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Model Comparison**: Pull other Ollama models (llama2, mistral) and compare\n",
    "2. **Context Window**: Test with very long prompts to find context limits\n",
    "3. **System Prompts**: Experiment with different system prompts for various tasks\n",
    "4. **Integration**: Combine Ollama with earlier notebooks (e.g., summarize image captions)\n",
    "5. **Custom Tool**: Build a simple CLI tool using Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here for exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "✅ **Ollama** simplifies running LLMs locally\n",
    "\n",
    "✅ **TinyLlama** is great for learning and prototyping\n",
    "\n",
    "✅ Can combine with **HuggingFace tools** for enhanced functionality\n",
    "\n",
    "✅ **Local inference** provides privacy and offline capability\n",
    "\n",
    "✅ Easy to **manage multiple models** with Ollama\n",
    "\n",
    "## Congratulations!\n",
    "\n",
    "You've completed all 10 HuggingFace tutorial notebooks! You now know how to:\n",
    "- Generate and classify text (NLP)\n",
    "- Classify and detect objects in images (Computer Vision)\n",
    "- Transcribe and generate speech (Audio)\n",
    "- Caption images (Multimodal)\n",
    "- Run local LLMs with Ollama\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Explore [HuggingFace Datasets](https://huggingface.co/datasets)\n",
    "- Learn about [fine-tuning models](https://huggingface.co/docs/transformers/training)\n",
    "- Join the [HuggingFace Forums](https://discuss.huggingface.co/)\n",
    "- Build your own AI application!\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Ollama Documentation](https://github.com/ollama/ollama)\n",
    "- [TinyLlama on HuggingFace](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0)\n",
    "- [Ollama Model Library](https://ollama.com/library)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}