{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Notebook 14: Agentic Workflows - MCP Basics\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand Model Context Protocol (MCP)\n",
    "- Set up MCP servers and clients\n",
    "- Connect local LLMs to external tools\n",
    "- Build your first tool-using agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "### Hardware Requirements\n",
    "\n",
    "| Model Option | Model Name | Size | Min RAM | Recommended Setup | Notes |\n",
    "|--------------|------------|------|---------|-------------------|-------|\n",
    "| **small (CPU-friendly)** | llama3.2:1b | 1.3GB | 8GB | 8GB RAM, CPU | Fast, good for learning |\n",
    "| **large (GPU-optimized)** | llama3.1:8b | 4.7GB | 16GB | 12GB VRAM (RTX 4080) | Better reasoning |\n",
    "| **SOTA (reference only)** | Claude 3.5 Sonnet | API | N/A | API key required | Production-grade |\n",
    "\n",
    "### Software Requirements\n",
    "- Python 3.10+\n",
    "- Ollama installed (see Notebook 10)\n",
    "- Libraries: `mcp`, `ollama`, `pydantic`\n",
    "\n",
    "### Installation\n",
    "\n",
    "```bash\n",
    "# Install MCP SDK\n",
    "pip install mcp\n",
    "\n",
    "# Install Ollama Python library\n",
    "pip install ollama\n",
    "\n",
    "# Pull Ollama models\n",
    "ollama pull llama3.2:1b\n",
    "ollama pull llama3.1:8b  # Optional, if you have resources\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## What is MCP?\n",
    "\n",
    "**Model Context Protocol (MCP)** is an open protocol created by Anthropic for connecting AI assistants to external tools and data sources.\n",
    "\n",
    "### Why MCP?\n",
    "\n",
    "**Traditional Problem:**\n",
    "- Each AI application needs custom integrations for every tool\n",
    "- No standard way to expose tools to LLMs\n",
    "- Tight coupling between LLM and tool code\n",
    "\n",
    "**MCP Solution:**\n",
    "- **Standardized protocol** for tool communication\n",
    "- **Decoupled architecture** - servers expose tools, clients consume them\n",
    "- **Reusable servers** - Write once, use with any MCP client\n",
    "- **Language agnostic** - Servers in Python, clients in any language\n",
    "\n",
    "### MCP Architecture\n",
    "\n",
    "```\n",
    "┌─────────────────┐\n",
    "│   AI Assistant  │  (Your LLM - Ollama, Claude, etc.)\n",
    "│   (MCP Client)  │\n",
    "└────────┬────────┘\n",
    "         │ MCP Protocol (JSON-RPC)\n",
    "         │\n",
    "┌────────▼────────┐\n",
    "│   MCP Server    │  (Exposes tools/resources)\n",
    "│  - Weather API  │\n",
    "│  - Calculator   │\n",
    "│  - File System  │\n",
    "└─────────────────┘\n",
    "```\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **MCP Server** - Exposes tools, resources, and prompts\n",
    "2. **MCP Client** - Connects to servers, invokes tools\n",
    "3. **Tools** - Functions the LLM can call (e.g., `search`, `calculate`)\n",
    "4. **Resources** - Data sources (files, databases, APIs)\n",
    "5. **Prompts** - Reusable prompt templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Expected Behaviors\n",
    "\n",
    "### First Time Running\n",
    "- **Ollama Model Pull**: ~1.3GB for llama3.2:1b (~2-5 minutes)\n",
    "- **MCP Installation**: ~10MB for mcp SDK\n",
    "- Models cached in `~/.ollama/models/`\n",
    "\n",
    "### MCP Server Startup\n",
    "```\n",
    "Starting MCP server...\n",
    "Server running on stdio\n",
    "Registered tools: ['calculator', 'get_weather']\n",
    "```\n",
    "\n",
    "### Tool Invocation\n",
    "- LLM receives tool descriptions\n",
    "- LLM decides when to use tools\n",
    "- Tool results returned to LLM\n",
    "- LLM generates final response\n",
    "\n",
    "### Performance\n",
    "- **llama3.2:1b** (CPU): 2-5 seconds per turn\n",
    "- **llama3.1:8b** (GPU): 1-3 seconds per turn\n",
    "- Tool execution: 100-500ms depending on tool\n",
    "\n",
    "### Common Observations\n",
    "- Smaller models may not always use tools correctly\n",
    "- Clear tool descriptions improve usage\n",
    "- Multi-step reasoning works better with larger models\n",
    "- Tool results should be concise for better understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport random\nimport ollama\nfrom typing import Any\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set seed for reproducibility\nrandom.seed(1103)\n\nprint(\"MCP Tutorial - Setup Complete\")\nprint(f\"Ollama available: {True}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHOOSE YOUR MODEL:\n",
    "\n",
    "# Option 1: small model (CPU-friendly, fast)\n",
    "MODEL_NAME = \"llama3.2:1b\"  # 1.3GB, good for learning\n",
    "\n",
    "# Option 2: large model (GPU-optimized, better reasoning)\n",
    "# MODEL_NAME = \"llama3.1:8b\"  # 4.7GB, more capable\n",
    "\n",
    "print(f\"Selected model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Building Your First MCP Tool\n",
    "\n",
    "Let's create a simple calculator tool that the LLM can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tools in the format Ollama expects\n",
    "tools = [\n",
    "    {\n",
    "        'type': 'function',\n",
    "        'function': {\n",
    "            'name': 'calculator',\n",
    "            'description': 'Perform basic arithmetic operations (add, subtract, multiply, divide)',\n",
    "            'parameters': {\n",
    "                'type': 'object',\n",
    "                'properties': {\n",
    "                    'operation': {\n",
    "                        'type': 'string',\n",
    "                        'description': 'The operation to perform',\n",
    "                        'enum': ['add', 'subtract', 'multiply', 'divide']\n",
    "                    },\n",
    "                    'a': {\n",
    "                        'type': 'number',\n",
    "                        'description': 'First number'\n",
    "                    },\n",
    "                    'b': {\n",
    "                        'type': 'number',\n",
    "                        'description': 'Second number'\n",
    "                    }\n",
    "                },\n",
    "                'required': ['operation', 'a', 'b']\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Tools defined:\")\n",
    "print(f\"  - {tools[0]['function']['name']}: {tools[0]['function']['description']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the calculator function\n",
    "def calculator(operation: str, a: float, b: float) -> float:\n",
    "    \"\"\"Execute calculator operations.\"\"\"\n",
    "    if operation == 'add':\n",
    "        return a + b\n",
    "    elif operation == 'subtract':\n",
    "        return a - b\n",
    "    elif operation == 'multiply':\n",
    "        return a * b\n",
    "    elif operation == 'divide':\n",
    "        if b == 0:\n",
    "            return \"Error: Division by zero\"\n",
    "        return a / b\n",
    "    else:\n",
    "        return \"Error: Unknown operation\"\n",
    "\n",
    "# Map function names to implementations\n",
    "available_functions = {\n",
    "    'calculator': calculator\n",
    "}\n",
    "\n",
    "print(\"Function implementations ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Agent Loop with Tool Calling\n",
    "\n",
    "This is the core agent loop that allows the LLM to use tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(prompt: str, model: str = MODEL_NAME, max_iterations: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Run an agent that can use tools to answer questions.\n",
    "    \n",
    "    Args:\n",
    "        prompt: User question\n",
    "        model: Ollama model to use\n",
    "        max_iterations: Maximum tool-calling iterations\n",
    "    \n",
    "    Returns:\n",
    "        Final answer from the agent\n",
    "    \"\"\"\n",
    "    messages = [{'role': 'user', 'content': prompt}]\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"User: {prompt}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        # Call LLM with tools\n",
    "        response = ollama.chat(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            tools=tools\n",
    "        )\n",
    "        \n",
    "        messages.append(response['message'])\n",
    "        \n",
    "        # Check if LLM wants to use a tool\n",
    "        if not response['message'].get('tool_calls'):\n",
    "            # No tool call, we have final answer\n",
    "            final_answer = response['message']['content']\n",
    "            print(f\"Agent: {final_answer}\")\n",
    "            print(f\"\\n{'='*70}\")\n",
    "            return final_answer\n",
    "        \n",
    "        # Process tool calls\n",
    "        for tool_call in response['message']['tool_calls']:\n",
    "            function_name = tool_call['function']['name']\n",
    "            function_args = tool_call['function']['arguments']\n",
    "            \n",
    "            print(f\"Tool Call: {function_name}({function_args})\")\n",
    "            \n",
    "            # Execute the function\n",
    "            function_to_call = available_functions[function_name]\n",
    "            function_response = function_to_call(**function_args)\n",
    "            \n",
    "            print(f\"Tool Result: {function_response}\\n\")\n",
    "            \n",
    "            # Add tool result to messages\n",
    "            messages.append({\n",
    "                'role': 'tool',\n",
    "                'content': str(function_response)\n",
    "            })\n",
    "    \n",
    "    return \"Maximum iterations reached\"\n",
    "\n",
    "print(\"Agent function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Example 1: Simple Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_agent(\"What is 127 multiplied by 83?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Example 2: Multi-Step Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_agent(\"If I have $100 and buy 3 items at $15 each, how much money do I have left?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Example 3: Multiple Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_agent(\"Calculate (25 + 17) * 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Adding More Tools\n",
    "\n",
    "Let's add a weather tool to demonstrate multiple tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add weather tool\n",
    "tools.append({\n",
    "    'type': 'function',\n",
    "    'function': {\n",
    "        'name': 'get_weather',\n",
    "        'description': 'Get current weather for a city',\n",
    "        'parameters': {\n",
    "            'type': 'object',\n",
    "            'properties': {\n",
    "                'city': {\n",
    "                    'type': 'string',\n",
    "                    'description': 'City name'\n",
    "                }\n",
    "            },\n",
    "            'required': ['city']\n",
    "        }\n",
    "    }\n",
    "})\n",
    "\n",
    "# Implement weather function (mock data for demo)\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get mock weather data.\"\"\"\n",
    "    weather_data = {\n",
    "        'san francisco': 'Sunny, 72°F',\n",
    "        'new york': 'Cloudy, 65°F',\n",
    "        'london': 'Rainy, 58°F',\n",
    "        'tokyo': 'Clear, 75°F'\n",
    "    }\n",
    "    city_lower = city.lower()\n",
    "    return weather_data.get(city_lower, f\"Weather data not available for {city}\")\n",
    "\n",
    "# Update available functions\n",
    "available_functions['get_weather'] = get_weather\n",
    "\n",
    "print(\"Weather tool added\")\n",
    "print(f\"Total tools: {len(tools)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_agent(\"What's the weather in San Francisco?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## Comparison: With vs Without Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without tools - LLM guesses\n",
    "print(\"WITHOUT TOOLS:\")\n",
    "response_no_tools = ollama.chat(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[{'role': 'user', 'content': 'What is 9876 multiplied by 5432?'}]\n",
    ")\n",
    "print(f\"Answer: {response_no_tools['message']['content']}\")\n",
    "print(f\"\\nActual: {9876 * 5432}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# With tools - LLM uses calculator\n",
    "print(\"WITH TOOLS:\")\n",
    "result = run_agent(\"What is 9876 multiplied by 5432?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Understanding Tool Selection\n",
    "\n",
    "The LLM decides which tool to use based on descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test tool selection with different queries\n",
    "test_queries = [\n",
    "    \"What's 42 divided by 7?\",\n",
    "    \"Is it raining in London?\",\n",
    "    \"Calculate the sum of 123 and 456, then tell me the weather in Tokyo\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    result = run_agent(query)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **New Tool**: Add a `get_time` tool that returns the current time for a timezone\n",
    "2. **Error Handling**: Test division by zero and observe how the agent handles it\n",
    "3. **Complex Query**: Ask a question that requires multiple tool calls\n",
    "4. **Tool Description**: Modify tool descriptions and see how it affects tool selection\n",
    "5. **Model Comparison**: Compare llama3.2:1b vs llama3.1:8b tool usage accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here for exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "✅ **MCP** provides a standard protocol for AI-tool integration\n",
    "\n",
    "✅ **Tools** are defined with name, description, and parameters\n",
    "\n",
    "✅ **LLMs decide** when to use tools based on descriptions\n",
    "\n",
    "✅ **Agent loop** handles iterative tool calling\n",
    "\n",
    "✅ **Local models** (Ollama) work well for basic tool use\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Try **Notebook 15**: Building MCP Servers with local LLMs\n",
    "- Explore [MCP documentation](https://modelcontextprotocol.io/)\n",
    "- Learn about [Ollama function calling](https://ollama.com/blog/tool-support)\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Model Context Protocol](https://modelcontextprotocol.io/)\n",
    "- [MCP Specification](https://spec.modelcontextprotocol.io/)\n",
    "- [Ollama Tool Support](https://ollama.com/blog/tool-support)\n",
    "- [MCP GitHub](https://github.com/modelcontextprotocol)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}