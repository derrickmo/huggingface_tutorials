{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Notebook 17: RAG with Local LLMs\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand Retrieval-Augmented Generation (RAG)\n",
    "- Build vector databases with FAISS and ChromaDB\n",
    "- Create embeddings with sentence transformers\n",
    "- Implement semantic search and context injection\n",
    "- Combine retrieval with local LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "### Hardware Requirements\n",
    "\n",
    "| Component | Small (CPU) | Large (GPU) | SOTA (Reference) |\n",
    "|-----------|-------------|-------------|------------------|\n",
    "| **Embedding Model** | all-MiniLM-L6-v2 | all-mpnet-base-v2 | OpenAI text-embedding-3 |\n",
    "| **Model Size** | 80MB | 420MB | API |\n",
    "| **LLM** | llama3.2:1b | llama3.1:8b | Claude 3.5 Sonnet |\n",
    "| **LLM Size** | 1.3GB | 4.7GB | API |\n",
    "| **Min RAM** | 8GB | 12GB | N/A |\n",
    "| **Min VRAM** | N/A (CPU) | 8GB | N/A |\n",
    "| **Performance** | 2-4s per query | 1-2s per query | <1s per query |\n",
    "\n",
    "### Software Requirements\n",
    "- Python 3.10+\n",
    "- Ollama installed with models pulled\n",
    "- Libraries: `sentence-transformers`, `faiss-cpu`, `chromadb`, `ollama`\n",
    "\n",
    "### Installation\n",
    "\n",
    "```bash\n",
    "# Install embedding and vector DB libraries\n",
    "pip install sentence-transformers\n",
    "pip install faiss-cpu  # or faiss-gpu if you have CUDA\n",
    "pip install chromadb\n",
    "pip install ollama\n",
    "\n",
    "# Pull Ollama models\n",
    "ollama pull llama3.2:1b\n",
    "ollama pull llama3.1:8b  # Optional, if you have resources\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Expected Behaviors\n",
    "\n",
    "### First Time Running\n",
    "- **Embedding Model Download**: 80MB (small) or 420MB (large)\n",
    "- **Ollama Model**: Already downloaded from notebooks 14-16\n",
    "- Models cached in `~/.cache/huggingface/` and `~/.ollama/models/`\n",
    "\n",
    "### Vector Database Creation\n",
    "```\n",
    "Creating embeddings for 50 documents...\n",
    "FAISS index created: 50 vectors, 384 dimensions\n",
    "Index saved to: ./rag_index\n",
    "```\n",
    "\n",
    "### RAG Query Execution\n",
    "- **Embedding query**: 50-100ms\n",
    "- **Semantic search**: 10-50ms for 1000 documents\n",
    "- **LLM generation**: 1-4 seconds depending on model\n",
    "- **Total**: 2-5 seconds per query\n",
    "\n",
    "### Common Observations\n",
    "- Embedding quality matters more than LLM size for accuracy\n",
    "- Top-K=3-5 documents usually sufficient for context\n",
    "- Larger context windows (llama3.1:8b) improve answer quality\n",
    "- ChromaDB is easier for persistence, FAISS is faster for search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## What is RAG?\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** combines information retrieval with text generation to answer questions using external knowledge.\n",
    "\n",
    "### Why RAG?\n",
    "\n",
    "**Problem with LLMs alone:**\n",
    "- Knowledge cutoff dates (outdated information)\n",
    "- Hallucinations (making up facts)\n",
    "- No access to private/proprietary data\n",
    "- Cannot cite sources\n",
    "\n",
    "**RAG Solution:**\n",
    "1. **Retrieve** relevant documents from a knowledge base\n",
    "2. **Inject** documents as context into LLM prompt\n",
    "3. **Generate** answer grounded in retrieved facts\n",
    "\n",
    "### RAG Architecture\n",
    "\n",
    "```\n",
    "User Query\n",
    "    ↓\n",
    "[Embedding Model] → Query Vector\n",
    "    ↓\n",
    "[Vector Database] → Semantic Search\n",
    "    ↓\n",
    "Top-K Relevant Documents\n",
    "    ↓\n",
    "[LLM] ← Query + Documents\n",
    "    ↓\n",
    "Grounded Answer\n",
    "```\n",
    "\n",
    "### Key Components\n",
    "\n",
    "1. **Embedding Model** - Converts text to vectors (sentence-transformers)\n",
    "2. **Vector Database** - Stores and searches embeddings (FAISS, ChromaDB)\n",
    "3. **LLM** - Generates answers using retrieved context (Ollama)\n",
    "4. **Knowledge Base** - Your documents/data to search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport random\nimport torch\nimport ollama\nfrom sentence_transformers import SentenceTransformer\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set seed for reproducibility\nrandom.seed(1103)\nnp.random.seed(1103)\ntorch.manual_seed(1103)\n\nprint(\"RAG Tutorial - Setup Complete\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHOOSE YOUR MODELS:\n",
    "\n",
    "# Embedding model (for converting text to vectors)\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"  # small: 80MB, 384 dimensions\n",
    "# EMBEDDING_MODEL = \"sentence-transformers/all-mpnet-base-v2\"  # large: 420MB, 768 dimensions\n",
    "\n",
    "# LLM model (for generating answers)\n",
    "LLM_MODEL = \"llama3.2:1b\"  # small: 1.3GB, CPU-friendly\n",
    "# LLM_MODEL = \"llama3.1:8b\"  # large: 4.7GB, GPU-optimized\n",
    "\n",
    "print(f\"Embedding model: {EMBEDDING_MODEL}\")\n",
    "print(f\"LLM model: {LLM_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Sample Knowledge Base\n",
    "\n",
    "Let's create a sample knowledge base about machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents for our knowledge base\n",
    "documents = [\n",
    "    \"Transformers are neural network architectures that use self-attention mechanisms. They were introduced in the paper 'Attention is All You Need' in 2017.\",\n",
    "    \"BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based model pre-trained on large text corpora. It excels at understanding context in both directions.\",\n",
    "    \"GPT (Generative Pre-trained Transformer) is an autoregressive language model that generates text by predicting the next token. GPT-3 has 175 billion parameters.\",\n",
    "    \"Fine-tuning is the process of adapting a pre-trained model to a specific task by training it on task-specific data. This is more efficient than training from scratch.\",\n",
    "    \"LoRA (Low-Rank Adaptation) is a parameter-efficient fine-tuning technique that adds trainable low-rank matrices to model layers, reducing memory requirements.\",\n",
    "    \"Vector databases store embeddings and enable fast similarity search. Popular options include FAISS, Pinecone, Weaviate, and ChromaDB.\",\n",
    "    \"Semantic search finds documents based on meaning rather than keyword matching. It uses embeddings to represent text as vectors in a high-dimensional space.\",\n",
    "    \"RAG (Retrieval-Augmented Generation) combines information retrieval with text generation. It retrieves relevant documents and uses them as context for generating answers.\",\n",
    "    \"Ollama is a tool for running large language models locally on your machine. It supports models like Llama 3, Mistral, and Phi-3.\",\n",
    "    \"Embeddings are dense vector representations of text that capture semantic meaning. Similar texts have similar embeddings in vector space.\",\n",
    "    \"Attention mechanisms allow models to focus on different parts of the input when generating output. Self-attention computes attention within a single sequence.\",\n",
    "    \"The Model Context Protocol (MCP) standardizes how AI assistants connect to external tools and data sources, enabling reusable integrations.\",\n",
    "    \"Zero-shot learning is when a model performs tasks it wasn't explicitly trained for, using only the task description in the prompt.\",\n",
    "    \"Few-shot learning provides a few examples of a task in the prompt to guide the model's behavior, improving performance without fine-tuning.\",\n",
    "    \"Prompt engineering is the practice of crafting effective prompts to elicit desired behaviors from language models. It's crucial for maximizing model performance.\"\n",
    "]\n",
    "\n",
    "print(f\"Knowledge base: {len(documents)} documents\")\n",
    "print(f\"Sample document: {documents[0][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Method 1: RAG with FAISS\n",
    "\n",
    "FAISS (Facebook AI Similarity Search) is a fast library for similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "print(\"Loading embedding model...\")\n",
    "embedding_model = SentenceTransformer(EMBEDDING_MODEL)\n",
    "print(f\"Embedding dimension: {embedding_model.get_sentence_embedding_dimension()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings for all documents\n",
    "print(f\"Creating embeddings for {len(documents)} documents...\")\n",
    "embeddings = embedding_model.encode(documents, show_progress_bar=True)\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FAISS index\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)  # L2 distance (Euclidean)\n",
    "index.add(embeddings.astype('float32'))\n",
    "\n",
    "print(f\"FAISS index created: {index.ntotal} vectors, {dimension} dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_documents(query, top_k=3):\n",
    "    \"\"\"\n",
    "    Search for relevant documents using semantic similarity.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query\n",
    "        top_k: Number of documents to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        List of (document, score) tuples\n",
    "    \"\"\"\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    distances, indices = index.search(query_embedding.astype('float32'), top_k)\n",
    "    \n",
    "    results = []\n",
    "    for idx, distance in zip(indices[0], distances[0]):\n",
    "        results.append((documents[idx], float(distance)))\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Search function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test semantic search\n",
    "query = \"What are transformers in machine learning?\"\n",
    "results = search_documents(query, top_k=3)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Top 3 relevant documents:\\n\")\n",
    "for i, (doc, score) in enumerate(results, 1):\n",
    "    print(f\"{i}. [Score: {score:.4f}]\")\n",
    "    print(f\"   {doc}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## RAG Query Function\n",
    "\n",
    "Combine retrieval with generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query(question, top_k=3, verbose=True):\n",
    "    \"\"\"\n",
    "    Answer a question using RAG.\n",
    "    \n",
    "    Args:\n",
    "        question: User question\n",
    "        top_k: Number of documents to retrieve\n",
    "        verbose: Print retrieval details\n",
    "    \n",
    "    Returns:\n",
    "        Generated answer\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"Question: {question}\\n\")\n",
    "        print(\"Step 1: Retrieving relevant documents...\")\n",
    "    \n",
    "    results = search_documents(question, top_k=top_k)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Retrieved {len(results)} documents\\n\")\n",
    "    \n",
    "    context = \"\\n\\n\".join([doc for doc, _ in results])\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Step 2: Generating answer with LLM...\\n\")\n",
    "    \n",
    "    prompt = f\"\"\"You are a helpful AI assistant. Answer the question based on the provided context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    response = ollama.chat(\n",
    "        model=LLM_MODEL,\n",
    "        messages=[{'role': 'user', 'content': prompt}]\n",
    "    )\n",
    "    \n",
    "    answer = response['message']['content']\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"=\"*70)\n",
    "        print(\"ANSWER\")\n",
    "        print(\"=\"*70)\n",
    "        print(answer)\n",
    "        print(\"=\"*70)\n",
    "    \n",
    "    return answer\n",
    "\n",
    "print(\"RAG query function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Simple question\n",
    "answer = rag_query(\"What is BERT?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: More complex question\n",
    "answer = rag_query(\"How does LoRA help with fine-tuning?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Comparison question\n",
    "answer = rag_query(\"What's the difference between zero-shot and few-shot learning?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Method 2: RAG with ChromaDB\n",
    "\n",
    "ChromaDB is a more feature-rich vector database with built-in persistence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "print(\"Initializing ChromaDB...\")\n",
    "chroma_client = chromadb.Client(Settings(\n",
    "    persist_directory=\"./chroma_db\",\n",
    "    anonymized_telemetry=False\n",
    "))\n",
    "\n",
    "# Create or get collection\n",
    "try:\n",
    "    collection = chroma_client.get_collection(name=\"ml_knowledge\")\n",
    "    print(f\"Loaded existing collection: {collection.count()} documents\")\n",
    "except:\n",
    "    collection = chroma_client.create_collection(\n",
    "        name=\"ml_knowledge\",\n",
    "        metadata={\"description\": \"Machine learning knowledge base\"}\n",
    "    )\n",
    "    print(\"Created new collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add documents to ChromaDB\n",
    "if collection.count() == 0:\n",
    "    print(f\"Adding {len(documents)} documents to ChromaDB...\")\n",
    "    \n",
    "    collection.add(\n",
    "        documents=documents,\n",
    "        ids=[f\"doc_{i}\" for i in range(len(documents))],\n",
    "        metadatas=[{\"source\": \"tutorial\", \"index\": i} for i in range(len(documents))]\n",
    "    )\n",
    "    \n",
    "    print(f\"Added {collection.count()} documents\")\n",
    "else:\n",
    "    print(f\"Collection already contains {collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query_chroma(question, top_k=3, verbose=True):\n",
    "    \"\"\"\n",
    "    Answer a question using RAG with ChromaDB.\n",
    "    \n",
    "    Args:\n",
    "        question: User question\n",
    "        top_k: Number of documents to retrieve\n",
    "        verbose: Print retrieval details\n",
    "    \n",
    "    Returns:\n",
    "        Generated answer\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"Question: {question}\\n\")\n",
    "        print(\"Step 1: Querying ChromaDB...\")\n",
    "    \n",
    "    results = collection.query(\n",
    "        query_texts=[question],\n",
    "        n_results=top_k\n",
    "    )\n",
    "    \n",
    "    retrieved_docs = results['documents'][0]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Retrieved {len(retrieved_docs)} documents\\n\")\n",
    "    \n",
    "    context = \"\\n\\n\".join(retrieved_docs)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Step 2: Generating answer with LLM...\\n\")\n",
    "    \n",
    "    prompt = f\"\"\"You are a helpful AI assistant. Answer the question based on the provided context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    response = ollama.chat(\n",
    "        model=LLM_MODEL,\n",
    "        messages=[{'role': 'user', 'content': prompt}]\n",
    "    )\n",
    "    \n",
    "    answer = response['message']['content']\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"=\"*70)\n",
    "        print(\"ANSWER\")\n",
    "        print(\"=\"*70)\n",
    "        print(answer)\n",
    "        print(\"=\"*70)\n",
    "    \n",
    "    return answer\n",
    "\n",
    "print(\"ChromaDB RAG function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ChromaDB RAG\n",
    "answer = rag_query_chroma(\"What is RAG and how does it work?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## Comparison: With vs Without RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_question = \"What is the Model Context Protocol?\"\n",
    "\n",
    "print(\"WITHOUT RAG (LLM alone):\")\n",
    "print(\"=\"*70)\n",
    "response_no_rag = ollama.chat(\n",
    "    model=LLM_MODEL,\n",
    "    messages=[{'role': 'user', 'content': test_question}]\n",
    ")\n",
    "print(response_no_rag['message']['content'])\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "print(\"WITH RAG (retrieval + LLM):\")\n",
    "print(\"=\"*70)\n",
    "answer_with_rag = rag_query(test_question, verbose=False)\n",
    "print(answer_with_rag)\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## Practical Application: Building a Custom Knowledge Base\n",
    "\n",
    "Let's build a RAG system for a different domain - company documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Company policy documents\n",
    "company_docs = [\n",
    "    \"Our company offers 20 days of paid vacation per year for all full-time employees. Part-time employees receive prorated vacation days.\",\n",
    "    \"Remote work is available up to 3 days per week for eligible positions. Employees must coordinate with their manager and maintain core hours of 10am-3pm.\",\n",
    "    \"Health insurance coverage begins on the first day of the month following your start date. We offer medical, dental, and vision plans.\",\n",
    "    \"The 401(k) retirement plan has a 4% company match. Employees are eligible after 90 days of employment.\",\n",
    "    \"Performance reviews are conducted twice per year in June and December. Salary adjustments are made following the December review cycle.\",\n",
    "    \"Our parental leave policy provides 16 weeks of paid leave for primary caregivers and 8 weeks for secondary caregivers.\",\n",
    "    \"Professional development budget of $2000 per year is available for conferences, courses, and certifications after 6 months of employment.\",\n",
    "    \"The employee referral program offers a $3000 bonus for successful hires who remain with the company for at least 6 months.\"\n",
    "]\n",
    "\n",
    "print(f\"Company knowledge base: {len(company_docs)} policies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings and index for company docs\n",
    "company_embeddings = embedding_model.encode(company_docs)\n",
    "company_index = faiss.IndexFlatL2(company_embeddings.shape[1])\n",
    "company_index.add(company_embeddings.astype('float32'))\n",
    "\n",
    "print(f\"Company document index created: {company_index.ntotal} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def company_rag_query(question, top_k=2):\n",
    "    \"\"\"RAG for company policy questions.\"\"\"\n",
    "    query_embedding = embedding_model.encode([question])\n",
    "    distances, indices = company_index.search(query_embedding.astype('float32'), top_k)\n",
    "    \n",
    "    context = \"\\n\\n\".join([company_docs[idx] for idx in indices[0]])\n",
    "    \n",
    "    prompt = f\"\"\"You are a helpful HR assistant. Answer the employee's question based on company policies.\n",
    "\n",
    "Company Policies:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    response = ollama.chat(\n",
    "        model=LLM_MODEL,\n",
    "        messages=[{'role': 'user', 'content': prompt}]\n",
    "    )\n",
    "    \n",
    "    return response['message']['content']\n",
    "\n",
    "print(\"Company RAG system ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test company RAG\n",
    "questions = [\n",
    "    \"How many vacation days do I get?\",\n",
    "    \"When does health insurance start?\",\n",
    "    \"What is the 401k match?\"\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"Q: {q}\")\n",
    "    answer = company_rag_query(q)\n",
    "    print(f\"A: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "## Performance Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def benchmark_rag(query, num_runs=3):\n",
    "    \"\"\"Benchmark RAG performance.\"\"\"\n",
    "    times = {'embedding': [], 'search': [], 'generation': [], 'total': []}\n",
    "    \n",
    "    for _ in range(num_runs):\n",
    "        start_total = time.time()\n",
    "        \n",
    "        start = time.time()\n",
    "        query_embedding = embedding_model.encode([query])\n",
    "        times['embedding'].append(time.time() - start)\n",
    "        \n",
    "        start = time.time()\n",
    "        distances, indices = index.search(query_embedding.astype('float32'), 3)\n",
    "        times['search'].append(time.time() - start)\n",
    "        \n",
    "        context = \"\\n\\n\".join([documents[idx] for idx in indices[0]])\n",
    "        prompt = f\"Context: {context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
    "        \n",
    "        start = time.time()\n",
    "        response = ollama.chat(\n",
    "            model=LLM_MODEL,\n",
    "            messages=[{'role': 'user', 'content': prompt}]\n",
    "        )\n",
    "        times['generation'].append(time.time() - start)\n",
    "        \n",
    "        times['total'].append(time.time() - start_total)\n",
    "    \n",
    "    print(\"Performance Breakdown (averages):\")\n",
    "    print(f\"  Embedding:  {np.mean(times['embedding'])*1000:.1f}ms\")\n",
    "    print(f\"  Search:     {np.mean(times['search'])*1000:.1f}ms\")\n",
    "    print(f\"  Generation: {np.mean(times['generation'])*1000:.1f}ms\")\n",
    "    print(f\"  Total:      {np.mean(times['total'])*1000:.1f}ms\")\n",
    "\n",
    "benchmark_rag(\"What is RAG?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-34",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Custom Knowledge Base**: Create a RAG system with your own documents (recipes, study notes, etc.)\n",
    "2. **Tune Top-K**: Experiment with different top_k values (1, 3, 5, 10) and observe answer quality\n",
    "3. **Hybrid Search**: Combine keyword matching with semantic search for better retrieval\n",
    "4. **Citation**: Modify the prompt to make the LLM cite which documents it used\n",
    "5. **Multi-Query**: Implement query expansion (generate multiple related queries for better coverage)\n",
    "6. **Re-ranking**: Add a re-ranking step after initial retrieval to improve relevance\n",
    "7. **Larger Dataset**: Test with 100+ documents and measure performance scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here for exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-36",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "✅ **RAG combines retrieval and generation** to answer questions using external knowledge\n",
    "\n",
    "✅ **Embedding models** convert text to vectors for semantic similarity\n",
    "\n",
    "✅ **Vector databases** (FAISS, ChromaDB) enable fast similarity search\n",
    "\n",
    "✅ **Context injection** grounds LLM responses in retrieved facts\n",
    "\n",
    "✅ **Local LLMs** (Ollama) work well for RAG with proper context\n",
    "\n",
    "✅ **RAG reduces hallucinations** by providing factual grounding\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Try **Advanced RAG Techniques**: HyDE, query expansion, re-ranking\n",
    "- Explore **Production RAG**: LangChain, LlamaIndex frameworks\n",
    "- Learn **Evaluation**: RAGAS, TruLens for RAG quality metrics\n",
    "- Combine with **MCP**: Use RAG as a tool in agentic workflows\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [FAISS Documentation](https://github.com/facebookresearch/faiss)\n",
    "- [ChromaDB Documentation](https://docs.trychroma.com/)\n",
    "- [Sentence Transformers](https://www.sbert.net/)\n",
    "- [RAG Papers](https://arxiv.org/abs/2005.11401)\n",
    "- [Ollama Documentation](https://ollama.ai/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}