{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 08: Audio - Text-to-Speech (TTS)\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Generate speech from text using TTS models\n",
    "- Use SpeechT5 for natural-sounding speech\n",
    "- Control voice characteristics with speaker embeddings\n",
    "- Save and play generated audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "### Hardware Requirements\n",
    "\n",
    "| Model Option | Model Name | Size | Min RAM | Recommended Setup | Notes |\n",
    "|--------------|------------|------|---------|-------------------|-------|\n",
    "| **CPU/GPU** | microsoft/speecht5_tts | 200MB | 4GB | 6GB VRAM (RTX 4080) | Good quality |\n",
    "\n",
    "### Software Requirements\n",
    "- Python 3.8+\n",
    "- Libraries: `transformers`, `torch`, `soundfile`, `datasets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nfrom transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan, set_seed\nfrom datasets import load_dataset\nimport soundfile as sf\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set seed for reproducibility\nset_seed(1103)\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")"
  },
  {
   "cell_type": "markdown",
   "source": "## Expected Behaviors\n\n### First Time Running\n- **Model Download**: ~200MB for SpeechT5 TTS (~2-3 minutes)\n- Also downloads vocoder (~50MB)\n- Downloads speaker embeddings dataset (~100MB)\n- Total: ~350MB first run\n\n### Setup Cell Output\n```\nPyTorch version: 2.x.x\nCUDA available: True/False\n```\n\n### Model Loading\n```\nModel loaded on: cpu (or cuda)\nSpeaker embeddings loaded\n```\n- **CPU**: 10-15 seconds (loads 3 components)\n- **GPU**: 5-8 seconds\n\n### Audio Output\n- Saves as WAV file at 16kHz sample rate\n- File size: ~1.5MB per minute of audio\n- Can be played in Jupyter with `IPython.display.Audio()`\n\n### Speech Quality\n- **Natural sounding** for most English text\n- **Intonation**: Reasonably natural, slight robotic quality\n- **Pronunciation**: 90-95% accurate for common words\n- **Prosody**: Handles punctuation (pauses, emphasis)\n\n### Voice Characteristics\n- Controlled by speaker embeddings\n- Different embeddings = different voice characteristics\n- Current notebooks use single voice (can experiment with others)\n- Voice quality is consistent across generations\n\n### Performance\n- **Short sentence** (10-20 words):\n  - CPU: 3-5 seconds\n  - GPU: 1-2 seconds\n- **Long paragraph** (100+ words):\n  - CPU: 15-25 seconds\n  - GPU: 5-8 seconds\n\n### Text Limitations\n- **Works best with**: Clear, grammatical English\n- **Maximum length**: ~200 words per generation (split longer text)\n- **Numbers**: Speaks digits individually (e.g., \"1234\" â†’ \"one two three four\")\n- **Abbreviations**: May mispronounce (expand them first)\n\n### Common Issues\n- **Monotone speech**: Try different speaker embeddings\n- **Mispronunciations**: Phonetic spelling may help\n- **Unnatural pauses**: Check punctuation\n- **Cutoff**: Text too long, split into chunks\n\n### Expected Output Files\n- `output.wav`, `hello.wav`, etc.\n- Saved in notebook directory\n- Can open with any audio player\n\n### Quality Factors\n- Punctuation affects pacing and intonation\n- ALL CAPS may sound overly emphasized\n- Question marks add rising intonation",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"microsoft/speecht5_tts\"\n",
    "VOCODER_NAME = \"microsoft/speecht5_hifigan\"\n",
    "\n",
    "print(f\"Selected model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model components\n",
    "print(\"Loading TTS model...\")\n",
    "processor = SpeechT5Processor.from_pretrained(MODEL_NAME)\n",
    "model = SpeechT5ForTextToSpeech.from_pretrained(MODEL_NAME)\n",
    "vocoder = SpeechT5HifiGan.from_pretrained(VOCODER_NAME)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "vocoder = vocoder.to(device)\n",
    "\n",
    "print(f\"Model loaded on: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load speaker embeddings\n",
    "embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
    "speaker_embeddings = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0).to(device)\n",
    "\n",
    "print(\"Speaker embeddings loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_speech(text, output_file=\"output.wav\"):\n",
    "    \"\"\"\n",
    "    Convert text to speech and save to file.\n",
    "    \"\"\"\n",
    "    # Process text\n",
    "    inputs = processor(text=text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate speech\n",
    "    with torch.no_grad():\n",
    "        speech = model.generate_speech(\n",
    "            inputs[\"input_ids\"], \n",
    "            speaker_embeddings, \n",
    "            vocoder=vocoder\n",
    "        )\n",
    "    \n",
    "    # Save audio\n",
    "    speech_np = speech.cpu().numpy()\n",
    "    sf.write(output_file, speech_np, samplerate=16000)\n",
    "    \n",
    "    print(f\"Audio saved to: {output_file}\")\n",
    "    return speech_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Generate speech\n",
    "text = \"Hello, welcome to the HuggingFace tutorial on text-to-speech!\"\n",
    "\n",
    "audio = text_to_speech(text, \"hello.wav\")\n",
    "print(f\"\\nGenerated {len(audio)} audio samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate multiple phrases\n",
    "phrases = [\n",
    "    \"Artificial intelligence is transforming the world.\",\n",
    "    \"Machine learning models can now understand and generate speech.\",\n",
    "    \"This technology has many practical applications.\"\n",
    "]\n",
    "\n",
    "print(\"=== GENERATING SPEECH FOR MULTIPLE PHRASES ===\")\n",
    "for i, phrase in enumerate(phrases, 1):\n",
    "    output_file = f\"phrase_{i}.wav\"\n",
    "    text_to_speech(phrase, output_file)\n",
    "    print(f\"  {i}. {phrase[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different speaker embeddings\n",
    "def try_different_voices(text, num_voices=3):\n",
    "    \"\"\"\n",
    "    Generate speech with different voice characteristics.\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== GENERATING WITH DIFFERENT VOICES ===\")\n",
    "    \n",
    "    for i in range(min(num_voices, len(embeddings_dataset))):\n",
    "        speaker_emb = torch.tensor(embeddings_dataset[i][\"xvector\"]).unsqueeze(0).to(device)\n",
    "        \n",
    "        inputs = processor(text=text, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            speech = model.generate_speech(\n",
    "                inputs[\"input_ids\"],\n",
    "                speaker_emb,\n",
    "                vocoder=vocoder\n",
    "            )\n",
    "        \n",
    "        output_file = f\"voice_{i+1}.wav\"\n",
    "        sf.write(output_file, speech.cpu().numpy(), samplerate=16000)\n",
    "        print(f\"  Voice {i+1} saved to: {output_file}\")\n",
    "\n",
    "# Test\n",
    "try_different_voices(\"This is a test of different voice characteristics.\", num_voices=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## State-of-the-Art Open Models (Not Covered)\n\nWhile SpeechT5 is a solid foundation for TTS, there are several cutting-edge models that offer superior naturalness, multilingual support, voice cloning capabilities, and even emotional expression. These represent the frontier of text-to-speech technology.\n\n### Top SOTA Text-to-Speech Models\n\n#### 1. ðŸŽ¤ XTTS v2 (Coqui AI)\n**Multilingual TTS with voice cloning**\n- **Why it's special**: Zero-shot voice cloning from 3-6 seconds of audio, 16 languages\n- **Performance**: Near-human naturalness, real-time capable, excellent prosody\n- **Model Card**: [coqui/XTTS-v2](https://huggingface.co/coqui/XTTS-v2)\n- **Paper**: [XTTS: A Massively Multilingual Zero-Shot Text-to-Speech Model](https://arxiv.org/abs/2406.04904)\n- **Size**: 1.8GB\n\n#### 2. ðŸŽµ Bark (Suno AI)\n**GPT-style TTS with music, sound effects, and emotions**\n- **Why it's special**: Generates laughter, music, sound effects, multiple languages, emotional speech\n- **Performance**: Highly expressive, can include non-speech sounds, ~10s for short sentences\n- **Model Card**: [suno/bark](https://huggingface.co/suno/bark)\n- **GitHub**: [suno-ai/bark](https://github.com/suno-ai/bark)\n- **Size**: 2.8GB (full model)\n\n#### 3. ðŸ”Š VITS (Variational Inference TTS)\n**End-to-end neural TTS with high quality**\n- **Why it's special**: Single-stage model (no vocoder needed), fast inference, excellent quality\n- **Performance**: Real-time factor < 0.1 (10x faster than real-time), natural prosody\n- **Model Card**: Multiple variants on HuggingFace (language-specific)\n- **Paper**: [Conditional Variational Autoencoder with Adversarial Learning](https://arxiv.org/abs/2106.06103)\n- **Size**: ~400MB\n\n#### 4. ðŸŒ YourTTS\n**Zero-shot multi-speaker and multilingual TTS**\n- **Why it's special**: Zero-shot voice cloning, 1107 speakers, multilingual support\n- **Performance**: High speaker similarity, good cross-lingual capabilities\n- **Model Card**: [coqui/YourTTS](https://huggingface.co/models?search=yourtts)\n- **Paper**: [YourTTS: Towards Zero-Shot Multi-Speaker TTS](https://arxiv.org/abs/2112.02418)\n- **Size**: ~500MB\n\n### Why Not Covered?\n\nThese advanced models require:\n- **GPU Memory**: 8-16GB VRAM for real-time inference\n- **Generation Time**: 10-60 seconds per sentence on CPU\n- **Disk Space**: 1.8-2.8GB per model\n- **Complex Setup**: Voice cloning requires reference audio preprocessing\n- **Specialized Use Cases**: Features like emotion/music may not be needed for basic TTS\n\nSpeechT5 provides excellent quality for standard TTS use cases!\n\n### Learning Path Recommendation\n\n1. **Start here**: Master SpeechT5 (this notebook)\n2. **Voice cloning**: Try XTTS v2 if you have GPU and need custom voices\n3. **Expressiveness**: Experiment with Bark for emotional/creative applications\n4. **Speed focus**: Use VITS for fast, high-quality production TTS\n5. **Research**: Explore YourTTS for multilingual voice transfer\n\n### Benchmarks & Quality Metrics\n\n- **Mean Opinion Score (MOS)** - Naturalness (1-5 scale):\n  - SpeechT5: 4.0-4.2\n  - XTTS v2: 4.3-4.5\n  - Bark: 4.2-4.4 (with expressiveness)\n  - VITS: 4.1-4.3\n  - YourTTS: 4.0-4.2\n\n- **Real-Time Factor (RTF)** - Lower is faster:\n  - SpeechT5: ~0.3 (GPU), ~2.0 (CPU)\n  - XTTS v2: ~0.2 (GPU), ~8.0 (CPU)\n  - VITS: ~0.1 (GPU), ~1.5 (CPU)\n  - Bark: ~1.0 (GPU), ~15.0 (CPU)\n\n- **Speaker Similarity** (voice cloning):\n  - XTTS v2: 85-90%\n  - YourTTS: 80-85%\n  - Bark: N/A (not designed for cloning specific voices)\n\n- **Explore benchmarks**: [Papers With Code - Speech Synthesis](https://paperswithcode.com/task/speech-synthesis)\n\n### Quick Comparison Table\n\n| Model | Size | Speed | Quality | Languages | Special Features |\n|-------|------|-------|---------|-----------|------------------|\n| **SpeechT5** â­ | 200MB | Fast | Good | English | Learning-friendly |\n| **VITS** | 400MB | Very Fast | Great | Multi* | Production-ready |\n| **YourTTS** | 500MB | Fast | Good | 16+ | Zero-shot cloning |\n| **XTTS v2** | 1.8GB | Medium | Excellent | 16 | Voice cloning, multilingual |\n| **Bark** | 2.8GB | Slow | Great | Multi | Music, emotions, SFX |\n\n*Separate models per language\n\n### Language Support\n\n| Model | English | Multi-language | Voice Cloning | Emotion |\n|-------|---------|----------------|---------------|---------|\n| **SpeechT5** | âœ… | âŒ | âŒ | âŒ |\n| **VITS** | âœ… | âœ…* | âŒ | âŒ |\n| **YourTTS** | âœ… | âœ… | âœ… | âŒ |\n| **XTTS v2** | âœ… | âœ… (16 langs) | âœ… | âš ï¸ (limited) |\n| **Bark** | âœ… | âœ… | âŒ | âœ… |\n\n### Special Capabilities\n\n**ðŸŒŸ What makes these models unique:**\n\n- **XTTS v2**: Clone anyone's voice from 6 seconds of audio\n- **Bark**: Add `[laughs]`, `[music]`, `[sigh]` tags for rich audio\n- **VITS**: Fastest inference, ideal for interactive applications\n- **YourTTS**: Cross-lingual voice transfer (speak any language in any voice)\n\n### Use Case Guide\n\n**Choose based on your needs:**\n\n- **Standard TTS (podcasts, audiobooks)**: SpeechT5 or VITS\n- **Voice cloning (characters, celebrities)**: XTTS v2\n- **Interactive apps (chatbots, assistants)**: VITS (fastest)\n- **Creative content (storytelling, entertainment)**: Bark\n- **Multilingual content**: XTTS v2 or YourTTS\n- **Emotional speech**: Bark\n\n### Example Capabilities\n\n**Bark special syntax:**\n```python\n# Laughter\n\"Hello [laughs] this is amazing!\"\n\n# Music notation\n\"And then... [music plays]\"\n\n# Different speakers in one generation\n\"WOMAN: Hello there. MAN: Nice to meet you!\"\n```\n\n**XTTS v2 voice cloning:**\n```python\n# Clone voice from reference audio\nspeaker_wav = \"path/to/voice_sample.wav\"  # 6+ seconds\ntts.tts_to_file(\"Hello world\", speaker_wav=speaker_wav)\n```\n\n**ðŸ’¡ Tip**: For production applications requiring custom voices, XTTS v2 is the gold standard. For creative/expressive content, Bark is unmatched. For speed and simplicity, stick with VITS or SpeechT5!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Long Text**: Generate speech for a paragraph (100+ words)\n",
    "2. **Voice Comparison**: Try all available speaker embeddings and compare\n",
    "3. **Custom Text**: Generate speech for your own text\n",
    "4. **Punctuation Effect**: Test how punctuation affects speech prosody\n",
    "5. **Multiple Languages**: Does the model handle non-English text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here for exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "âœ… **SpeechT5** generates natural-sounding speech\n",
    "\n",
    "âœ… **Speaker embeddings** control voice characteristics\n",
    "\n",
    "âœ… **Vocoder** (HiFiGAN) converts features to waveform\n",
    "\n",
    "âœ… Generated audio is saved as WAV files at 16kHz\n",
    "\n",
    "âœ… Works well for English text\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Try **Notebook 09**: Multimodal Image-to-Text\n",
    "- Explore other TTS models on [HuggingFace Hub](https://huggingface.co/models?pipeline_tag=text-to-speech)\n",
    "- Learn about voice cloning and custom speaker embeddings\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [SpeechT5 Paper](https://arxiv.org/abs/2110.07205)\n",
    "- [TTS Task Guide](https://huggingface.co/docs/transformers/tasks/text-to-speech)\n",
    "- [SpeechT5 Model Card](https://huggingface.co/microsoft/speecht5_tts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}