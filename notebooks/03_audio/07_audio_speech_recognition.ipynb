{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 07: Audio - Automatic Speech Recognition (ASR)\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Convert speech to text using ASR models\n",
    "- Use Whisper model for transcription\n",
    "- Handle different audio formats\n",
    "- Process audio files and microphone input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Prerequisites\n\n### Hardware Requirements\n\n| Model Option | Model Name | Size | Min RAM | Recommended Setup | Notes |\n|--------------|------------|------|---------|-------------------|-------|\n| **small (CPU-friendly)** | openai/whisper-tiny | 72MB | 4GB | 4GB RAM, CPU | Fast, 99+ languages, ~90% accuracy |\n| **large (GPU-optimized)** | openai/whisper-small | 483MB | 6GB | 8GB VRAM (RTX 4080) | Better accuracy, production-grade |\n\n### Software Requirements\n- Python 3.8+\n- Libraries: `transformers`, `torch`, `soundfile`, `librosa`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nfrom transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline, set_seed\nimport soundfile as sf\nimport requests\nfrom io import BytesIO\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set seed for reproducibility\nset_seed(1103)\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")"
  },
  {
   "cell_type": "markdown",
   "source": "## Expected Behaviors\n\n### First Time Running\n- **Model Download**: ~72MB for Whisper small model (~1-2 minutes)\n- Smallest model in this tutorial!\n- Very fast even on CPU\n\n### Setup Cell Output\n```\nPyTorch version: 2.x.x\nCUDA available: True/False\n```\n\n### Model Loading\n```\nLoading openai/whisper-tiny...\n```\n- **CPU**: 2-4 seconds\n- **GPU**: 1-2 seconds\n\n### Transcription Output Format\n```python\n{'text': 'I have a dream that one day this nation will rise up...'}\n```\n\n### With Timestamps\n```python\n{\n  'text': 'Full transcription...',\n  'chunks': [\n    {'timestamp': [0.0, 2.5], 'text': 'I have a dream'},\n    {'timestamp': [2.5, 5.0], 'text': 'that one day'}\n  ]\n}\n```\n\n### Transcription Quality\n- **small model** (clear speech, quiet background): 85-90% accuracy\n- **large model** (clear speech, quiet background): 92-95% accuracy\n- **Noisy environment**: 75-85% accuracy (small), 85-90% (large)\n- **Multiple speakers**: May struggle, best with single speaker\n- **Accents**: Handles various accents reasonably well\n\n### Language Support\n- **Whisper supports 99+ languages**!\n- Automatically detects language (no need to specify)\n- Quality varies by language (best for English)\n\n### Performance\n- **30-second audio clip**:\n  - small model (CPU): 5-10 seconds (~32x realtime)\n  - small model (GPU): 1-2 seconds\n  - large model (CPU): 20-30 seconds\n  - large model (GPU): 3-5 seconds\n- **5-minute audio**:\n  - small model (CPU): 1-2 minutes\n  - small model (GPU): 10-20 seconds\n  - large model (GPU): 30-60 seconds\n\n### Audio Format Support\n- WAV, MP3, FLAC, OGG, M4A\n- Automatically resamples to 16kHz\n- Mono or stereo (converted to mono)\n\n### Common Observations\n- **Punctuation**: Added automatically (mostly accurate)\n- **Capitalization**: Generally correct\n- **Filler words** (\"um\", \"uh\"): Often transcribed\n- **Background music**: May confuse transcription\n- **Long silences**: Handled gracefully\n\n### Model Comparison\n- **small model**: Fast, good for real-time, ~85-90% accurate, CPU-friendly\n- **large model**: 5x slower, ~92-95% accurate, production-grade",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# CHOOSE YOUR MODEL:\n\n# Option 1: small model (CPU-friendly, fast, 99+ languages)\nMODEL_NAME = \"openai/whisper-tiny\"  # 72MB, ~32x realtime on CPU\n\n# Option 2: large model (GPU-optimized, production-grade)\n# MODEL_NAME = \"openai/whisper-small\"  # 483MB, better accuracy, multilingual\n\nprint(f\"Selected model: {MODEL_NAME}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create ASR pipeline\nprint(f\"Loading {MODEL_NAME}...\")\nasr = pipeline(\n    \"automatic-speech-recognition\",\n    model=MODEL_NAME,\n    device=0 if torch.cuda.is_available() else -1\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Transcribe a sample audio file\n",
    "# Using HuggingFace's audio sample\n",
    "sample_audio_url = \"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\"\n",
    "\n",
    "# Download and transcribe\n",
    "result = asr(sample_audio_url)\n",
    "\n",
    "print(\"=== TRANSCRIPTION ===\")\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcribe with timestamps\n",
    "result_with_timestamps = asr(\n",
    "    sample_audio_url,\n",
    "    return_timestamps=True\n",
    ")\n",
    "\n",
    "print(\"\\n=== TRANSCRIPTION WITH TIMESTAMPS ===\")\n",
    "print(f\"Full text: {result_with_timestamps['text']}\")\n",
    "print(\"\\nChunks:\")\n",
    "for chunk in result_with_timestamps.get('chunks', []):\n",
    "    print(f\"  [{chunk['timestamp'][0]:.2f}s - {chunk['timestamp'][1]:.2f}s]: {chunk['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process local audio files (if any)\n",
    "import os\n",
    "\n",
    "sample_data_path = \"../sample_data\"\n",
    "\n",
    "if os.path.exists(sample_data_path):\n",
    "    audio_files = [f for f in os.listdir(sample_data_path) \n",
    "                   if f.lower().endswith(('.wav', '.mp3', '.flac', '.ogg'))]\n",
    "    \n",
    "    if audio_files:\n",
    "        print(\"=== TRANSCRIBING LOCAL AUDIO FILES ===\")\n",
    "        for audio_file in audio_files[:3]:  # Limit to 3\n",
    "            audio_path = os.path.join(sample_data_path, audio_file)\n",
    "            result = asr(audio_path)\n",
    "            print(f\"\\n{audio_file}:\")\n",
    "            print(f\"  {result['text']}\")\n",
    "    else:\n",
    "        print(\"No audio files found. Add .wav, .mp3, or .flac files to sample_data/ to test!\")\n",
    "else:\n",
    "    print(\"sample_data/ not found. Add audio files there to test transcription.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Using LibriSpeech ASR dummy dataset (small test dataset for speech recognition)\nfrom datasets import load_dataset\n\nprint(\"Loading LibriSpeech ASR dummy dataset...\")\n# Load the test dataset (very small, just for testing)\ndataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n\nprint(f\"Loaded {len(dataset)} audio samples\\n\")\n\n# Transcribe a couple of examples\nprint(\"=== LibriSpeech Dataset Transcription ===\")\nfor i in range(min(2, len(dataset))):\n    sample = dataset[i]\n    \n    # Get audio array and sampling rate\n    audio_array = sample['audio']['array']\n    sampling_rate = sample['audio']['sampling_rate']\n    true_text = sample['text']\n    \n    # Transcribe\n    result = asr({\"array\": audio_array, \"sampling_rate\": sampling_rate})\n    predicted_text = result['text']\n    \n    print(f\"\\nSample {i+1}:\")\n    print(f\"  True text:      {true_text}\")\n    print(f\"  Predicted text: {predicted_text}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Exercises\n\n1. **Custom Audio**: Record your own audio and transcribe it\n2. **Language Support**: Test with non-English audio (Whisper supports 99+ languages)\n3. **Model Comparison**: Compare small vs large model accuracy\n4. **Long Audio**: Test with longer audio files (5+ minutes)\n5. **Noisy Audio**: How does the model handle background noise?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here for exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Key Takeaways\n\n✅ **Whisper** is a powerful multilingual ASR model\n\n✅ Supports **99+ languages** out of the box\n\n✅ Can provide **timestamps** for each segment\n\n✅ Works with various **audio formats** (WAV, MP3, FLAC)\n\n✅ small model is surprisingly good for most use cases\n\n## Next Steps\n\n- Try **Notebook 08**: Text-to-Speech for audio generation\n- Explore [audio models](https://huggingface.co/models?pipeline_tag=automatic-speech-recognition)\n- Learn about fine-tuning for domain-specific vocabulary\n\n## Resources\n\n- [Whisper Paper](https://arxiv.org/abs/2212.04356)\n- [ASR Task Guide](https://huggingface.co/docs/transformers/tasks/asr)\n- [Whisper Model Card](https://huggingface.co/openai/whisper-tiny)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}