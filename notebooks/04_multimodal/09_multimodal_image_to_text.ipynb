{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 09: Multimodal - Image-to-Text (Image Captioning)\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Generate text descriptions from images\n",
    "- Use BLIP (Bootstrapping Language-Image Pre-training)\n",
    "- Understand multimodal models\n",
    "- Apply to image accessibility and content understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Prerequisites\n\n### Hardware Requirements\n\n| Model Option | Model Name | Size | Min RAM | Recommended Setup | Notes |\n|--------------|------------|------|---------|-------------------|-------|\n| **small (CPU-friendly)** | Salesforce/blip-image-captioning-base | 990MB | 6GB | 6GB RAM, CPU | Good quality |\n| **large (GPU-optimized)** | Salesforce/blip-image-captioning-large | 1.9GB | 8GB | 10GB VRAM (RTX 4080) | Better captions |\n\n### Software Requirements\n- Python 3.8+\n- Libraries: `transformers`, `torch`, `PIL`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nfrom transformers import AutoProcessor, BlipForConditionalGeneration, set_seed\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set seed for reproducibility\nset_seed(1103)\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")"
  },
  {
   "cell_type": "markdown",
   "source": "## Expected Behaviors\n\n### First Time Running\n- **Model Download**: ~990MB for BLIP-base (~5-8 minutes)\n- Large model combining vision and language\n- Cached for subsequent runs\n\n### Setup Cell Output\n```\nPyTorch version: 2.x.x\nCUDA available: True/False\n```\n\n### Model Loading\n```\nModel loaded on: cpu (or cuda)\n```\n- **CPU**: 15-20 seconds (large multimodal model)\n- **GPU**: 8-12 seconds\n\n### Caption Output\n- Returns natural language description\n- Example: `\"a cat sitting on a couch looking at the camera\"`\n\n### Caption Quality\n- **Clear, single-subject images**: Very accurate, descriptive\n- **Complex scenes**: Captures main elements, may miss details\n- **Multiple objects**: Describes most prominent objects\n- **Actions**: Often captures what's happening in scene\n\n### Expected Caption Length\n- **Unconditional**: 5-15 words typically\n- **With prompt**: Longer, more specific descriptions\n- Controlled by `max_length` parameter\n\n### Performance\n- **Single image**:\n  - CPU: 5-8 seconds\n  - GPU: 1-2 seconds\n- **Batch of 5 images**:\n  - CPU: 20-30 seconds\n  - GPU: 4-6 seconds\n\n### Caption Style\n- **Factual and descriptive**\n- Uses common language\n- Focuses on visible elements\n- Sometimes includes colors, positions, activities\n\n### Conditional Captioning\n- Can provide text prompts to guide captions\n- Example prompts: \"a photograph of\", \"this image shows\"\n- Helps steer caption style and content\n\n### Sampling for Variety\n- `do_sample=True` generates diverse captions\n- Same image can produce different valid captions\n- Useful for creative applications\n\n### Common Observations\n- Accurate for common objects/scenes (people, animals, vehicles)\n- May hallucinate details not actually present\n- Sometimes generic for unusual images\n- Better on photos than drawings/artwork\n\n### Multimodal Understanding\n- Combines vision (what's in image) + language (how to describe it)\n- Trained on millions of image-caption pairs\n- Can describe relationships (\"person holding phone\")",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# CHOOSE YOUR MODEL:\n\n# Option 1: small model (CPU-friendly)\nMODEL_NAME = \"Salesforce/blip-image-captioning-base\"  # 990MB\n\n# Option 2: large model (GPU-optimized, better quality)\n# MODEL_NAME = \"Salesforce/blip-image-captioning-large\"  # 1.9GB\n\nprint(f\"Selected model: {MODEL_NAME}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and processor\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME)\n",
    "model = BlipForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model loaded on: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_from_url(url):\n",
    "    \"\"\"Load an image from a URL.\"\"\"\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "    return img\n",
    "\n",
    "def generate_caption(image, max_length=50):\n",
    "    \"\"\"\n",
    "    Generate a caption for an image.\n",
    "    \"\"\"\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    generated_ids = model.generate(**inputs, max_length=max_length)\n",
    "    caption = processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Caption a single image\n",
    "image_url = \"https://images.unsplash.com/photo-1518791841217-8f162f1e1131?w=500\"\n",
    "image = load_image_from_url(image_url)\n",
    "\n",
    "print(f\"Image size: {image.size}\")\n",
    "\n",
    "caption = generate_caption(image)\n",
    "\n",
    "print(f\"\\n=== GENERATED CAPTION ===\")\n",
    "print(caption)\n",
    "\n",
    "# Display image (in Jupyter)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate conditional captions\n",
    "def generate_conditional_caption(image, prompt_text):\n",
    "    \"\"\"\n",
    "    Generate caption conditioned on a text prompt.\n",
    "    \"\"\"\n",
    "    inputs = processor(images=image, text=prompt_text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    generated_ids = model.generate(**inputs)\n",
    "    caption = processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return caption\n",
    "\n",
    "# Test with prompts\n",
    "prompts = [\n",
    "    \"a photograph of\",\n",
    "    \"this image shows\",\n",
    "    \"the picture depicts\"\n",
    "]\n",
    "\n",
    "print(\"\\n=== CONDITIONAL CAPTIONS ===\")\n",
    "for prompt in prompts:\n",
    "    caption = generate_conditional_caption(image, prompt)\n",
    "    print(f\"\\nPrompt: '{prompt}'\")\n",
    "    print(f\"Caption: {caption}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caption multiple images\n",
    "test_urls = [\n",
    "    \"https://images.unsplash.com/photo-1506905925346-21bda4d32df4?w=500\",  # mountain\n",
    "    \"https://images.unsplash.com/photo-1546527868-ccb7ee7dfa6a?w=500\",  # car\n",
    "    \"https://images.unsplash.com/photo-1551782450-a2132b4ba21d?w=500\",  # burger\n",
    "    \"https://images.unsplash.com/photo-1552053831-71594a27632d?w=500\"   # dog\n",
    "]\n",
    "\n",
    "print(\"\\n=== MULTIPLE IMAGE CAPTIONS ===\")\n",
    "for i, url in enumerate(test_urls, 1):\n",
    "    try:\n",
    "        img = load_image_from_url(url)\n",
    "        caption = generate_caption(img)\n",
    "        print(f\"\\n{i}. {caption}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n{i}. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate multiple captions for same image (with sampling)\n",
    "def generate_multiple_captions(image, num_captions=3):\n",
    "    \"\"\"\n",
    "    Generate multiple diverse captions for an image.\n",
    "    \"\"\"\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    captions = []\n",
    "    for _ in range(num_captions):\n",
    "        generated_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_length=50,\n",
    "            num_beams=5,\n",
    "            do_sample=True,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        caption = processor.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        captions.append(caption)\n",
    "    \n",
    "    return captions\n",
    "\n",
    "# Test\n",
    "image = load_image_from_url(\"https://images.unsplash.com/photo-1506905925346-21bda4d32df4?w=500\")\n",
    "captions = generate_multiple_captions(image, num_captions=3)\n",
    "\n",
    "print(\"\\n=== MULTIPLE CAPTION VARIATIONS ===\")\n",
    "for i, caption in enumerate(captions, 1):\n",
    "    print(f\"{i}. {caption}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local images\n",
    "import os\n",
    "\n",
    "sample_data_path = \"../sample_data\"\n",
    "\n",
    "if os.path.exists(sample_data_path):\n",
    "    image_files = [f for f in os.listdir(sample_data_path) \n",
    "                   if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    \n",
    "    if image_files:\n",
    "        print(\"\\n=== CAPTIONING LOCAL IMAGES ===\")\n",
    "        for img_file in image_files[:3]:\n",
    "            img_path = os.path.join(sample_data_path, img_file)\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            caption = generate_caption(img)\n",
    "            print(f\"\\n{img_file}: {caption}\")\n",
    "    else:\n",
    "        print(\"\\nNo images in sample_data/. Add some to test!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## State-of-the-Art Open Models (Not Covered)\n\nWhile BLIP is excellent for image captioning, there are powerful vision-language models that go far beyond simple captioning. These models can answer questions, follow instructions, engage in visual reasoning, and handle complex multimodal tasks.\n\n### Top SOTA Vision-Language Models\n\n#### 1. üëÅÔ∏è LLaVA (Microsoft/Wisconsin-Madison)\n**Large Language and Vision Assistant with instruction tuning**\n- **Why it's special**: Can answer questions about images, follow complex instructions, visual reasoning\n- **Performance**: 85.1% accuracy on ScienceQA, strong zero-shot capabilities\n- **Model Card**: [llava-hf/llava-1.5-7b-hf](https://huggingface.co/llava-hf/llava-1.5-7b-hf)\n- **Paper**: [Visual Instruction Tuning](https://arxiv.org/abs/2304.08485)\n- **Size**: 13GB (7B parameters)\n\n#### 2. üîç BLIP-2 (Salesforce)\n**Bootstrapping Language-Image Pre-training with frozen LLMs**\n- **Why it's special**: Efficient training by freezing vision and language models, Q-Former architecture\n- **Performance**: State-of-the-art on VQA, image captioning, and image-text retrieval\n- **Model Card**: [Salesforce/blip2-opt-2.7b](https://huggingface.co/Salesforce/blip2-opt-2.7b)\n- **Paper**: [BLIP-2: Bootstrapping Language-Image Pre-training](https://arxiv.org/abs/2301.12597)\n- **Size**: 5.5GB (2.7B parameters)\n\n#### 3. üìö InstructBLIP (Salesforce)\n**Instruction-aware vision-language model**\n- **Why it's special**: Follows natural language instructions for diverse vision-language tasks\n- **Performance**: Excellent on instruction-following benchmarks, flexible task handling\n- **Model Card**: [Salesforce/instructblip-vicuna-7b](https://huggingface.co/Salesforce/instructblip-vicuna-7b)\n- **Paper**: [InstructBLIP: Towards General-purpose Vision-Language Models](https://arxiv.org/abs/2305.06500)\n- **Size**: 13GB (7B parameters)\n\n#### 4. üåê Qwen-VL (Alibaba Cloud)\n**Multilingual vision-language model**\n- **Why it's special**: Strong multilingual support (English + Chinese), grounding, OCR capabilities\n- **Performance**: 78.5% on TextVQA, excellent on Chinese benchmarks\n- **Model Card**: [Qwen/Qwen-VL-Chat](https://huggingface.co/Qwen/Qwen-VL-Chat)\n- **Paper**: [Qwen-VL: A Versatile Vision-Language Model](https://arxiv.org/abs/2308.12966)\n- **Size**: 20GB (9.6B parameters)\n\n#### 5. üß† CogVLM (Zhipu AI)\n**Visual expert language model**\n- **Why it's special**: Achieves SOTA on many VQA benchmarks, strong visual grounding\n- **Performance**: 92.5% on TextVQA, 87.7% on ScienceQA\n- **Model Card**: [THUDM/cogvlm-chat-hf](https://huggingface.co/THUDM/cogvlm-chat-hf)\n- **Paper**: [CogVLM: Visual Expert for Pretrained Language Models](https://arxiv.org/abs/2311.03079)\n- **Size**: 20GB (17B parameters)\n\n### Why Not Covered?\n\nThese advanced models require:\n- **GPU Memory**: 24-80GB VRAM (A100/H100 GPUs or multi-GPU setup)\n- **Inference Time**: 5-20 seconds per image-text pair\n- **Disk Space**: 5-20GB per model\n- **Complex Prompting**: Need careful instruction design for best results\n- **Computational Resources**: Quantization (4-bit/8-bit) often needed\n\nBLIP provides an excellent foundation for learning vision-language concepts!\n\n### Learning Path Recommendation\n\n1. **Start here**: Master BLIP (this notebook)\n2. **Next level**: Try BLIP-2 for visual question answering\n3. **Instruction following**: Explore InstructBLIP for diverse tasks\n4. **Conversational**: Experiment with LLaVA for image chat\n5. **Advanced**: Try CogVLM for state-of-the-art performance\n\n### Benchmarks & Leaderboards\n\n- **VQAv2** (Visual Question Answering):\n  - BLIP-base: 77.5% accuracy\n  - BLIP-2: 82.2% accuracy\n  - InstructBLIP: 82.8% accuracy\n  - LLaVA-1.5: 80.0% accuracy\n  - CogVLM: 83.6% accuracy\n\n- **TextVQA** (Reading text in images):\n  - BLIP-base: 67.5%\n  - BLIP-2: 71.7%\n  - Qwen-VL: 78.5%\n  - CogVLM: 92.5%\n\n- **Image Captioning** (CIDEr score on COCO):\n  - BLIP-base: 136.7\n  - BLIP-2: 144.5\n  - InstructBLIP: 142.8\n\n- **Explore rankings**: [Papers With Code - Visual Question Answering](https://paperswithcode.com/task/visual-question-answering)\n\n### Quick Comparison Table\n\n| Model | Size | Speed | VQA Score | Capabilities | Best For |\n|-------|------|-------|-----------|--------------|----------|\n| **BLIP** ‚≠ê | 990MB | Fast | 77.5% | Captioning, retrieval | Learning basics |\n| **BLIP-2** | 5.5GB | Medium | 82.2% | VQA, captioning | Efficient VL tasks |\n| **InstructBLIP** | 13GB | Slow | 82.8% | Instruction following | Flexible task handling |\n| **LLaVA** | 13GB | Slow | 80.0% | Visual chat, reasoning | Conversational AI |\n| **Qwen-VL** | 20GB | Very Slow | 78.5% | Multilingual, OCR | Chinese + English |\n| **CogVLM** | 20GB | Very Slow | 83.6% | SOTA performance | Research, benchmarks |\n\n### Capabilities Comparison\n\n| Model | Captioning | VQA | Instructions | Reasoning | OCR | Multilingual |\n|-------|------------|-----|--------------|-----------|-----|--------------|\n| **BLIP** | ‚úÖ | ‚ö†Ô∏è | ‚ùå | ‚ùå | ‚ùå | ‚ùå |\n| **BLIP-2** | ‚úÖ | ‚úÖ | ‚ö†Ô∏è | ‚ö†Ô∏è | ‚ö†Ô∏è | ‚ùå |\n| **InstructBLIP** | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚ö†Ô∏è | ‚ùå |\n| **LLaVA** | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚ö†Ô∏è | ‚ö†Ô∏è |\n| **Qwen-VL** | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ |\n| **CogVLM** | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚úÖ | ‚ùå |\n\n### Example Capabilities\n\n**What these models can do beyond BLIP:**\n\n**LLaVA:**\n```python\n\"Describe this image in detail.\"\n\"What's unusual about this image?\"\n\"Count the number of people in this image.\"\n```\n\n**InstructBLIP:**\n```python\n\"Question: Is this safe to eat? Answer:\"\n\"Describe the emotion of the person in this photo.\"\n\"List all the objects you can see.\"\n```\n\n**Qwen-VL:**\n```python\n\"ËøôÂº†ÂõæÁâáÈáåÊúâ‰ªÄ‰πàÔºü\" (What's in this image? - Chinese)\n\"Read the text in this image.\"\n\"Where is the cat in relation to the sofa?\"\n```\n\n**CogVLM:**\n```python\n\"Analyze the scientific diagram and explain the process.\"\n\"What equations are shown in this math problem?\"\n```\n\n### Use Case Guide\n\n**Choose based on your application:**\n\n- **Simple captioning**: BLIP (this notebook)\n- **Visual Q&A**: BLIP-2 or InstructBLIP\n- **Chatbot with images**: LLaVA\n- **Chinese language**: Qwen-VL\n- **Document understanding**: CogVLM or Qwen-VL\n- **Research/benchmarks**: CogVLM (best performance)\n\n### Hardware Requirements\n\n| Model | Min VRAM | Recommended | Quantization Option |\n|-------|----------|-------------|---------------------|\n| **BLIP** | 4GB | 8GB | Not needed |\n| **BLIP-2** | 12GB | 16GB | 8-bit: 8GB |\n| **InstructBLIP** | 24GB | 32GB | 4-bit: 12GB |\n| **LLaVA** | 24GB | 32GB | 4-bit: 12GB |\n| **Qwen-VL** | 32GB | 40GB | 4-bit: 16GB |\n| **CogVLM** | 40GB | 48GB | 4-bit: 20GB |\n\n**üí° Tip**: For production applications with complex vision-language needs, LLaVA offers the best balance of capability and accessibility. For research requiring SOTA performance, CogVLM is unmatched. For beginners and simple tasks, BLIP is perfect!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Diverse Images**: Test with various image types (animals, landscapes, objects, people)\n",
    "2. **Quality Assessment**: Compare base vs large model captions\n",
    "3. **Custom Images**: Caption your own photos\n",
    "4. **Caption Length**: Experiment with `max_length` parameter\n",
    "5. **Batch Processing**: Process multiple images efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here for exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "‚úÖ **BLIP** bridges vision and language for image captioning\n",
    "\n",
    "‚úÖ **Multimodal models** process both images and text\n",
    "\n",
    "‚úÖ Can generate **unconditional** or **conditional** captions\n",
    "\n",
    "‚úÖ Useful for **accessibility** and **content understanding**\n",
    "\n",
    "‚úÖ Sampling generates diverse captions for same image\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Try **Notebook 10**: Ollama Integration\n",
    "- Explore [vision-language models](https://huggingface.co/models?pipeline_tag=image-to-text)\n",
    "- Learn about Visual Question Answering (VQA)\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [BLIP Paper](https://arxiv.org/abs/2201.12086)\n",
    "- [Image-to-Text Guide](https://huggingface.co/docs/transformers/tasks/image_captioning)\n",
    "- [BLIP Model Card](https://huggingface.co/Salesforce/blip-image-captioning-base)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}