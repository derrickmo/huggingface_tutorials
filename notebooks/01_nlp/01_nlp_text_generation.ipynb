{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 01: Natural Language Processing - Text Generation\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand text generation using transformer models\n",
    "- Load and use pre-trained language models from HuggingFace\n",
    "- Generate coherent text continuations\n",
    "- Experiment with generation parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "### Hardware Requirements\n",
    "\n",
    "| Model Option | Model Name | Size | Min RAM | Recommended Setup | Notes |\n",
    "|--------------|------------|------|---------|-------------------|-------|\n",
    "| **CPU (Small)** | distilgpt2 | 82MB | 2GB | 4GB RAM, CPU | Fast, educational |\n",
    "| **GPU (Medium)** | gpt2-medium | 1.5GB | 4GB | 8GB VRAM (RTX 4080) | Better quality |\n",
    "\n",
    "### Software Requirements\n",
    "- Python 3.8+\n",
    "- Libraries: `transformers`, `torch`\n",
    "- See `requirements.txt` for full list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "**Text Generation** is the task of producing coherent text based on a given prompt. It's one of the most popular applications of transformer models.\n",
    "\n",
    "**Use Cases:**\n",
    "- Creative writing assistance\n",
    "- Code completion\n",
    "- Chatbots and conversational AI\n",
    "- Content generation\n",
    "\n",
    "**How it works:**\n",
    "1. You provide a text prompt (e.g., \"Once upon a time\")\n",
    "2. The model predicts the next token (word/subword)\n",
    "3. The process repeats to generate a sequence\n",
    "4. Various decoding strategies control generation quality"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Expected Behaviors\n\nWhen you run this notebook, here's what you should see:\n\n### First Time Running\n- **Model Download**: First run will download the model (~82MB for distilgpt2, ~1.5GB for gpt2-medium)\n  - Downloads go to `~/.cache/huggingface/hub/`\n  - Progress bar shows download status\n  - Subsequent runs use cached model (much faster!)\n\n### Setup Cell Output\n```\nPyTorch version: 2.x.x\nCUDA available: True/False\nGPU: NVIDIA GeForce RTX 4080 (if you have GPU)\n```\n\n### Model Loading\n```\nLoading distilgpt2...\nModel loaded successfully!\n```\n- **CPU**: Takes 5-10 seconds\n- **GPU**: Takes 2-5 seconds\n\n### Text Generation Examples\n- Generated text should be **grammatically coherent** but may not always be factually accurate\n- **Temperature 0.3**: More repetitive, focused text\n- **Temperature 0.7**: Balanced creativity\n- **Temperature 1.2**: More random, creative text\n\n### Common Outputs\n- Text continues naturally from your prompt\n- May include unexpected topics or tangents (this is normal!)\n- Quality improves with larger models (gpt2-medium > distilgpt2)\n\n### Performance\n- **CPU (distilgpt2)**: ~2-5 seconds per generation\n- **GPU (distilgpt2)**: ~0.5-1 second per generation\n- **GPU (gpt2-medium)**: ~1-2 seconds per generation\n\n### Troubleshooting\n- **\"CUDA out of memory\"**: Use CPU model option or restart kernel\n- **Slow generation**: First run downloads model; subsequent runs are faster\n- **Repetitive text**: Try increasing temperature or adjusting top_k/top_p",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import required libraries\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, set_seed\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set seed for reproducibility\nset_seed(1103)\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "Choose one of the following models based on your hardware:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHOOSE YOUR MODEL:\n",
    "\n",
    "# Option 1: CPU-friendly (recommended for beginners)\n",
    "MODEL_NAME = \"distilgpt2\"  # 82MB, fast on CPU\n",
    "\n",
    "# Option 2: GPU-optimized (uncomment if you have RTX 4080 or similar)\n",
    "# MODEL_NAME = \"gpt2-medium\"  # 1.5GB, better quality, needs GPU\n",
    "\n",
    "print(f\"Selected model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Using Pipeline (Simplest)\n",
    "\n",
    "The `pipeline` API is the easiest way to use HuggingFace models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create a text generation pipeline\nprint(f\"Loading {MODEL_NAME}...\")\ngenerator = pipeline(\n    \"text-generation\",\n    model=MODEL_NAME,\n    device=0 if torch.cuda.is_available() else -1  # 0 for GPU, -1 for CPU\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text from a prompt\n",
    "prompt = \"Once upon a time in a distant galaxy\"\n",
    "\n",
    "result = generator(\n",
    "    prompt,\n",
    "    max_length=50,        # Maximum length of generated text\n",
    "    num_return_sequences=1,  # Number of different outputs\n",
    "    temperature=0.7,      # Creativity (0.1=conservative, 2.0=creative)\n",
    "    do_sample=True        # Enable random sampling\n",
    ")\n",
    "\n",
    "print(\"Generated text:\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Multiple Variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 3 different continuations\n",
    "prompt = \"The future of artificial intelligence is\"\n",
    "\n",
    "results = generator(\n",
    "    prompt,\n",
    "    max_length=40,\n",
    "    num_return_sequences=3,\n",
    "    temperature=0.8,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "print(\"\\n=== Generated Variations ===\")\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\n{i}. {result['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Using Model and Tokenizer Directly (Advanced)\n",
    "\n",
    "For more control, load the model and tokenizer separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model loaded on: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text with more control\n",
    "prompt = \"In the year 2050, technology will\"\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate\n",
    "outputs = model.generate(\n",
    "    inputs.input_ids,\n",
    "    max_length=60,\n",
    "    temperature=0.7,\n",
    "    top_k=50,           # Consider top 50 tokens\n",
    "    top_p=0.95,         # Nucleus sampling threshold\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Decode output\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"\\nGenerated: {generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Generation Parameters\n",
    "\n",
    "Let's experiment with different parameters to see their effects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_temperatures(prompt, temperatures=[0.3, 0.7, 1.2]):\n",
    "    \"\"\"\n",
    "    Compare text generation with different temperature values.\n",
    "    Lower temperature = more conservative, higher = more creative\n",
    "    \"\"\"\n",
    "    print(f\"Prompt: '{prompt}'\\n\")\n",
    "    \n",
    "    for temp in temperatures:\n",
    "        result = generator(\n",
    "            prompt,\n",
    "            max_length=40,\n",
    "            temperature=temp,\n",
    "            do_sample=True,\n",
    "            num_return_sequences=1\n",
    "        )\n",
    "        print(f\"Temperature {temp}:\")\n",
    "        print(f\"{result[0]['generated_text']}\\n\")\n",
    "\n",
    "# Test it\n",
    "compare_temperatures(\"The secret to happiness is\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Story Beginning Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "story_prompts = [\n    \"Once upon a time in a small village,\",\n    \"The detective looked at the evidence and realized\",\n    \"On the first day of summer vacation,\"\n]\n\nfor prompt in story_prompts:\n    result = generator(prompt, max_length=50, temperature=0.8, do_sample=True)\n    print(f\"{result[0]['generated_text']}\\n\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Code Comment Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_prompt = \"This function calculates the\"\n",
    "\n",
    "result = generator(\n",
    "    code_prompt,\n",
    "    max_length=30,\n",
    "    temperature=0.5,  # Lower temperature for more focused output\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison\n",
    "\n",
    "Let's measure generation speed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "prompt = \"Artificial intelligence is\"\n",
    "\n",
    "# Measure time\n",
    "start_time = time.time()\n",
    "result = generator(prompt, max_length=50, do_sample=True)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Generated text: {result[0]['generated_text']}\")\n",
    "print(f\"\\nTime taken: {end_time - start_time:.2f} seconds\")\n",
    "print(f\"Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Try these challenges to deepen your understanding:\n",
    "\n",
    "1. **Experiment with parameters**: Generate text with different `temperature`, `top_k`, and `top_p` values. What happens?\n",
    "\n",
    "2. **Longer generation**: Try generating longer sequences (e.g., `max_length=100`). Does quality degrade?\n",
    "\n",
    "3. **Domain-specific prompts**: Test the model with prompts from different domains (technical, creative, conversational). How does it perform?\n",
    "\n",
    "4. **Compare models**: If you have GPU access, compare `distilgpt2` with `gpt2-medium`. What differences do you notice?\n",
    "\n",
    "5. **Batch generation**: Generate text for multiple prompts at once using a list of prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here for exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## State-of-the-Art Open Models (Not Covered)\n\nWhile this notebook focuses on GPT-2 for educational purposes, here are **state-of-the-art open-source text generation models** you should know about:\n\n### Large Language Models (7B+ parameters)\n\n**ü¶ô Llama 2 & Llama 3** (Meta)\n- Sizes: 7B, 13B, 70B parameters\n- Best for: General-purpose text generation, chat, instruction following\n- [Model Card](https://huggingface.co/meta-llama) | [Paper](https://arxiv.org/abs/2307.09288)\n- Note: Requires 16GB+ GPU for 7B model\n\n**üåä Mistral & Mixtral** (Mistral AI)\n- Mistral 7B: Efficient, outperforms Llama 2 13B\n- Mixtral 8x7B: Mixture of Experts, exceptional performance\n- [Model Card](https://huggingface.co/mistralai) | [Paper](https://arxiv.org/abs/2401.04088)\n\n**üéØ Qwen 2** (Alibaba)\n- Sizes: 0.5B to 72B parameters\n- Strong multilingual capabilities\n- [Model Card](https://huggingface.co/Qwen) | [Blog](https://qwenlm.github.io/)\n\n**üíé Gemma** (Google)\n- Sizes: 2B, 7B parameters\n- Excellent efficiency and safety features\n- [Model Card](https://huggingface.co/google/gemma-7b) | [Blog](https://blog.google/technology/developers/gemma-open-models/)\n\n**üî¨ Phi-3** (Microsoft)\n- Sizes: 3.8B (mini), 7B (small), 14B (medium)\n- Exceptional performance for size, optimized for edge devices\n- [Model Card](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct)\n\n### Specialized Models\n\n**üíª CodeLlama** (Meta)\n- Specialized for code generation\n- [Model Card](https://huggingface.co/codellama/CodeLlama-7b-hf)\n\n**üó£Ô∏è Zephyr** (HuggingFace)\n- Fine-tuned for helpful, harmless conversations\n- [Model Card](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta)\n\n### Why Not Covered Here?\n\nThese models require:\n- **Large GPU memory**: 16GB+ VRAM for 7B models\n- **Longer download times**: 10-40GB model files\n- **More compute**: Slower on consumer hardware\n\n**Learning Path**:\n1. ‚úÖ Start with GPT-2 (this notebook) to learn fundamentals\n2. Move to fine-tuning (Notebook 13) with LoRA for efficient training\n3. Graduate to 7B+ models when you have GPU resources\n\n### Where to Find More\n\n- [HuggingFace Text Generation Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)\n- [Papers with Code - Text Generation](https://paperswithcode.com/task/text-generation)\n- [Ollama](https://ollama.ai/) - Run LLMs locally (see Notebook 10)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "‚úÖ **Pipeline API** is the easiest way to use HuggingFace models\n",
    "\n",
    "‚úÖ **Temperature** controls creativity: lower = safer, higher = more random\n",
    "\n",
    "‚úÖ **Max length** determines how much text to generate\n",
    "\n",
    "‚úÖ Models are downloaded once and cached locally\n",
    "\n",
    "‚úÖ GPU acceleration significantly speeds up generation\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Try **Notebook 02**: Text Classification for sentiment analysis\n",
    "- Explore larger models like `gpt2-large` if you have more resources\n",
    "- Check out [HuggingFace Model Hub](https://huggingface.co/models?pipeline_tag=text-generation) for more text generation models\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Transformers Documentation - Text Generation](https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
    "- [How to Generate Text](https://huggingface.co/blog/how-to-generate)\n",
    "- [GPT-2 Model Card](https://huggingface.co/gpt2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}