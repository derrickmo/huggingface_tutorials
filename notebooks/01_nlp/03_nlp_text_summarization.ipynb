{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 03: Natural Language Processing - Text Summarization\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand automatic text summarization\n",
    "- Use seq2seq models for summarization\n",
    "- Generate concise summaries from long documents\n",
    "- Compare extractive vs abstractive summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "### Hardware Requirements\n",
    "\n",
    "| Model Option | Model Name | Size | Min RAM | Recommended Setup | Notes |\n",
    "|--------------|------------|------|---------|-------------------|-------|\n",
    "| **CPU (Small)** | sshleifer/distilbart-cnn-12-6 | 1.2GB | 4GB | 4GB RAM, CPU | Faster inference |\n",
    "| **GPU (Medium)** | facebook/bart-large-cnn | 1.6GB | 6GB | 8GB VRAM (RTX 4080) | Better quality |\n",
    "\n",
    "### Software Requirements\n",
    "- Python 3.8+\n",
    "- Libraries: `transformers`, `torch`\n",
    "- See `requirements.txt` for full list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "**Text Summarization** condenses long documents into shorter versions while preserving key information.\n",
    "\n",
    "**Types:**\n",
    "- **Extractive**: Selects important sentences from the original text\n",
    "- **Abstractive**: Generates new sentences that capture the meaning (what we'll use)\n",
    "\n",
    "**Use Cases:**\n",
    "- News article summarization\n",
    "- Research paper abstracts\n",
    "- Meeting notes\n",
    "- Document digests\n",
    "\n",
    "**How it works:**\n",
    "1. Encoder reads and understands the full text\n",
    "2. Decoder generates a concise summary\n",
    "3. Model was trained on pairs of (document, summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Expected Behaviors\n\n### First Time Running\n- **Model Download**: ~1.2GB for distilbart (~3-5 minutes download)\n- Largest model so far in the NLP notebooks\n- Cached for future runs in `~/.cache/huggingface/hub/`\n\n### Setup Cell Output\n```\nPyTorch version: 2.x.x\nCUDA available: True/False\n```\n\n### Model Loading Time\n- **CPU**: 10-15 seconds to load model into memory\n- **GPU**: 5-10 seconds\n- Loading message appears: `Loading sshleifer/distilbart-cnn-12-6...`\n\n### Summarization Output\n```python\n{\n  'summary_text': 'Large language models have demonstrated impressive capabilities...'\n}\n```\n\n### Summary Quality Expectations\n- **Good summaries**: Capture main points from original text\n- **Abstractive**: Uses new words/phrases, not just copying sentences\n- **Coherent**: Reads naturally, not fragmented\n- **Length**: Typically 20-30% of original text length\n\n### Performance Benchmarks\n- **Short text** (100 words):\n  - CPU: 2-4 seconds\n  - GPU: 0.5-1 second\n- **Medium text** (300 words):\n  - CPU: 5-8 seconds\n  - GPU: 1-2 seconds\n- **Long text** (500+ words):\n  - CPU: 10-15 seconds\n  - GPU: 2-4 seconds\n\n### Beam Search Impact\n- **num_beams=1**: Fastest, lower quality\n- **num_beams=4**: Good balance (default)\n- **num_beams=8**: Best quality, 2x slower\n\n### Common Observations\n- Summaries may occasionally omit minor details (expected)\n- Works best with factual, structured text (news, articles)\n- Creative/narrative text may have mixed results\n- Very short inputs (<50 words) may produce minimal summaries",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import required libraries\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline, set_seed\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set seed for reproducibility\nset_seed(1103)\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHOOSE YOUR MODEL:\n",
    "\n",
    "# Option 1: CPU-friendly (recommended for beginners)\n",
    "MODEL_NAME = \"sshleifer/distilbart-cnn-12-6\"  # 1.2GB, distilled BART\n",
    "\n",
    "# Option 2: GPU-optimized (uncomment if you have RTX 4080 or similar)\n",
    "# MODEL_NAME = \"facebook/bart-large-cnn\"  # 1.6GB, better quality\n",
    "\n",
    "print(f\"Selected model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Using Pipeline (Simplest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create summarization pipeline\nprint(f\"Loading {MODEL_NAME}...\")\nsummarizer = pipeline(\n    \"summarization\",\n    model=MODEL_NAME,\n    device=0 if torch.cuda.is_available() else -1\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example article\n",
    "article = \"\"\"\n",
    "The field of artificial intelligence has seen remarkable progress in recent years, \n",
    "particularly in the domain of natural language processing. Large language models, \n",
    "which are trained on vast amounts of text data, have demonstrated impressive capabilities \n",
    "in understanding and generating human-like text. These models use transformer architectures, \n",
    "which employ attention mechanisms to process sequential data more effectively than previous \n",
    "approaches. The transformer architecture was introduced in the landmark paper \"Attention Is All \n",
    "You Need\" in 2017. Since then, models like BERT, GPT, and their variants have revolutionized \n",
    "numerous NLP tasks including translation, summarization, question answering, and text generation. \n",
    "However, these models also raise important concerns about computational costs, environmental impact, \n",
    "and potential misuse. Researchers are now focusing on making these models more efficient, \n",
    "interpretable, and aligned with human values.\n",
    "\"\"\"\n",
    "\n",
    "# Generate summary\n",
    "summary = summarizer(article, max_length=130, min_length=30, do_sample=False)\n",
    "\n",
    "print(\"Original text length:\", len(article.split()))\n",
    "print(\"\\n=== SUMMARY ===\")\n",
    "print(summary[0]['summary_text'])\n",
    "print(\"\\nSummary length:\", len(summary[0]['summary_text'].split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controlling Summary Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summaries of different lengths\n",
    "lengths = [(50, 20), (100, 40), (150, 60)]  # (max_length, min_length)\n",
    "\n",
    "print(\"=== Summaries of Different Lengths ===\")\n",
    "for max_len, min_len in lengths:\n",
    "    summary = summarizer(article, max_length=max_len, min_length=min_len, do_sample=False)\n",
    "    print(f\"\\n[{min_len}-{max_len} tokens]:\")\n",
    "    print(summary[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Using Model and Tokenizer Directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Move to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model loaded on: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary with more control\n",
    "text = \"\"\"\n",
    "Climate change is one of the most pressing challenges facing humanity today. \n",
    "Rising global temperatures are causing ice caps to melt, sea levels to rise, \n",
    "and extreme weather events to become more frequent. Scientists warn that without \n",
    "immediate action to reduce greenhouse gas emissions, the consequences could be \n",
    "catastrophic. Renewable energy sources like solar and wind power offer promising \n",
    "solutions, but transitioning away from fossil fuels requires coordinated global \n",
    "effort and significant investment. Many countries have committed to achieving \n",
    "net-zero emissions by 2050, but current progress suggests more ambitious actions \n",
    "are needed.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n",
    "\n",
    "# Generate summary\n",
    "summary_ids = model.generate(\n",
    "    inputs.input_ids,\n",
    "    max_length=100,\n",
    "    min_length=30,\n",
    "    length_penalty=2.0,  # Encourages longer summaries\n",
    "    num_beams=4,         # Beam search for better quality\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# Decode\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"=== Climate Change Summary ===\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: News Article Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_article = \"\"\"\n",
    "Tech giant announces breakthrough in quantum computing. The company revealed a new \n",
    "quantum processor that achieved quantum supremacy, solving complex problems that would \n",
    "take classical computers thousands of years to complete. The 70-qubit processor \n",
    "demonstrated error rates low enough for practical applications. Industry experts believe \n",
    "this development could revolutionize fields like drug discovery, financial modeling, and \n",
    "cryptography. However, practical quantum computers for everyday use are still years away. \n",
    "The company plans to make the technology available to researchers through cloud access \n",
    "next year. Competitors are racing to develop their own quantum systems, with several \n",
    "other tech companies announcing similar initiatives. The quantum computing market is \n",
    "expected to grow to $65 billion by 2030 according to industry analysts.\n",
    "\"\"\"\n",
    "\n",
    "summary = summarizer(news_article, max_length=60, min_length=25, do_sample=False)\n",
    "\n",
    "print(\"=== NEWS SUMMARY ===\")\n",
    "print(summary[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Meeting Notes Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meeting_notes = \"\"\"\n",
    "The quarterly team meeting began with a review of the project timeline. Sarah reported \n",
    "that the mobile app development is on track for the March release. However, backend \n",
    "integration is facing delays due to API compatibility issues. Mike volunteered to work \n",
    "with the infrastructure team to resolve these problems by next week. The marketing team \n",
    "presented their campaign strategy, proposing a budget increase of 15% for social media \n",
    "advertising. After discussion, the team agreed to the proposal pending approval from \n",
    "upper management. Action items include: Sarah to complete user testing by Friday, \n",
    "Mike to fix API issues by next Tuesday, and Jennifer to submit the budget proposal \n",
    "by end of week. The next meeting is scheduled for two weeks from today.\n",
    "\"\"\"\n",
    "\n",
    "summary = summarizer(meeting_notes, max_length=80, min_length=30, do_sample=False)\n",
    "\n",
    "print(\"=== MEETING SUMMARY ===\")\n",
    "print(summary[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Batch Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize multiple documents\n",
    "documents = [\n",
    "    \"\"\"\n",
    "    A new study shows that regular exercise can significantly improve cognitive function \n",
    "    in older adults. Researchers followed 500 participants over three years, finding that \n",
    "    those who engaged in moderate aerobic exercise for 30 minutes daily showed 25% better \n",
    "    memory retention than the control group.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Electric vehicle sales reached a record high this quarter, with major manufacturers \n",
    "    reporting 40% year-over-year growth. The surge is attributed to improved battery \n",
    "    technology, expanded charging infrastructure, and government incentives. Industry \n",
    "    analysts predict EVs will account for 50% of new car sales by 2030.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    A recent archaeological discovery in Egypt has uncovered a previously unknown pharaoh's \n",
    "    tomb. The tomb, dating back 3,400 years, contains remarkably well-preserved artifacts \n",
    "    and hieroglyphics that provide new insights into ancient Egyptian society. Experts \n",
    "    believe this finding could rewrite parts of Egyptian history.\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "summaries = summarizer(documents, max_length=50, min_length=20, do_sample=False)\n",
    "\n",
    "print(\"=== BATCH SUMMARIZATION ===\")\n",
    "for i, (doc, summary) in enumerate(zip(documents, summaries), 1):\n",
    "    print(f\"\\nDocument {i}:\")\n",
    "    print(f\"Summary: {summary['summary_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## State-of-the-Art Open Models (Not Covered)\n\nWhile we've used DistilBART and BART for this tutorial, there are several more powerful state-of-the-art summarization models available. These models offer better quality but require significantly more computational resources.\n\n### Top SOTA Summarization Models\n\n#### 1. ðŸ“„ PEGASUS (Google)\n**Google's state-of-the-art abstractive summarization model**\n- **Why it's special**: Pre-trained specifically for summarization using \"gap sentence generation\"\n- **Performance**: Often achieves best ROUGE scores on benchmark datasets\n- **Model Card**: [google/pegasus-xsum](https://huggingface.co/google/pegasus-xsum)\n- **Paper**: [PEGASUS: Pre-training with Extracted Gap-sentences](https://arxiv.org/abs/1912.08777)\n- **Size**: ~2.3GB (570M parameters)\n\n#### 2. ðŸ”§ T5-Large/XL (Google)\n**Text-to-Text Transfer Transformer for summarization**\n- **Why it's special**: Frames all NLP tasks as text-to-text, extremely versatile\n- **Performance**: Excellent on diverse summarization tasks (news, scientific papers, dialogues)\n- **Model Card**: [t5-large](https://huggingface.co/t5-large), [t5-3b](https://huggingface.co/t5-3b)\n- **Paper**: [Exploring the Limits of Transfer Learning](https://arxiv.org/abs/1910.10683)\n- **Size**: 3GB (T5-large), 11GB (T5-3B)\n\n#### 3. ðŸŽ¯ BART-Large (Facebook)\n**Full-sized BART for higher quality summaries**\n- **Why it's special**: Larger version of the model we used, better coherence and accuracy\n- **Performance**: Strong baseline for abstractive summarization\n- **Model Card**: [facebook/bart-large-cnn](https://huggingface.co/facebook/bart-large-cnn)\n- **Paper**: [BART: Denoising Sequence-to-Sequence Pre-training](https://arxiv.org/abs/1910.13461)\n- **Size**: 1.6GB (406M parameters)\n\n#### 4. ðŸ“š LED (Longformer Encoder-Decoder)\n**Specialized for long document summarization**\n- **Why it's special**: Can handle documents up to 16,384 tokens (vs 1,024 for BART)\n- **Performance**: Best for scientific papers, legal documents, books\n- **Model Card**: [allenai/led-large-16384](https://huggingface.co/allenai/led-large-16384)\n- **Paper**: [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150)\n- **Size**: 1.6GB (406M parameters)\n\n#### 5. ðŸ“– LongT5\n**For very long document summarization**\n- **Why it's special**: Extends T5 to handle 16K+ tokens efficiently\n- **Performance**: Excellent on long-form content like research papers\n- **Model Card**: [google/long-t5-tglobal-large](https://huggingface.co/google/long-t5-tglobal-large)\n- **Paper**: [LongT5: Efficient Text-To-Text Transformer for Long Sequences](https://arxiv.org/abs/2112.07916)\n- **Size**: 3GB (770M parameters)\n\n### Why Not Covered?\n\nThese models require:\n- **GPU Memory**: 12-24GB VRAM (RTX 3090/4090 or better)\n- **Inference Time**: 10-60 seconds per summary (vs 2-5 seconds for DistilBART)\n- **Disk Space**: 1.6-11GB per model\n- **RAM**: 16-32GB system memory\n\nFor learning purposes, DistilBART provides 90% of the quality at 10% of the resource cost!\n\n### Learning Path Recommendation\n\n1. **Start here**: Master DistilBART (this notebook)\n2. **Next step**: Try BART-large if you have 8GB+ VRAM\n3. **Advanced**: Experiment with PEGASUS for news summarization\n4. **Specialized**: Use LED/LongT5 for long documents only when needed\n\n### Benchmarks & Leaderboards\n\n- **ROUGE Scores** (CNN/DailyMail dataset):\n  - DistilBART: ROUGE-2 ~19.5\n  - BART-large: ROUGE-2 ~21.3\n  - PEGASUS: ROUGE-2 ~21.9\n  - T5-3B: ROUGE-2 ~22.5\n\n- **Explore benchmarks**: [Papers With Code - Text Summarization](https://paperswithcode.com/task/text-summarization)\n\n### Quick Comparison Table\n\n| Model | Size | Speed | Quality | Best For |\n|-------|------|-------|---------|----------|\n| **DistilBART** â­ | 1.2GB | Fast | Good | Learning, CPU inference |\n| **BART-large** | 1.6GB | Medium | Great | General summarization |\n| **PEGASUS** | 2.3GB | Slow | Excellent | News articles |\n| **T5-Large** | 3GB | Slow | Excellent | Diverse tasks |\n| **LED** | 1.6GB | Very Slow | Great | Long documents (10K+ words) |\n| **LongT5** | 3GB | Very Slow | Excellent | Very long documents |\n\n**ðŸ’¡ Tip**: For production applications with quality requirements, PEGASUS or T5 are worth the computational investment!"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Length Experiment**: Try different `max_length` values. When does the summary become too short or too long?\n",
    "\n",
    "2. **Beam Search**: Experiment with `num_beams` parameter. Compare quality with 1, 4, and 8 beams.\n",
    "\n",
    "3. **Custom Articles**: Find a news article online and summarize it. How well does the model do?\n",
    "\n",
    "4. **Model Comparison**: If you have GPU, compare `distilbart` with `bart-large-cnn`. Notice quality differences?\n",
    "\n",
    "5. **Long Documents**: Test with very long documents (500+ words). Does the model handle them well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here for exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "âœ… **Abstractive summarization** generates new sentences rather than extracting existing ones\n",
    "\n",
    "âœ… **Length parameters** control summary size (`max_length`, `min_length`)\n",
    "\n",
    "âœ… **Beam search** (`num_beams`) improves quality at the cost of speed\n",
    "\n",
    "âœ… **Batch processing** is efficient for multiple documents\n",
    "\n",
    "âœ… BART models are specifically trained for summarization tasks\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Try **Notebook 04**: Image Classification for computer vision\n",
    "- Explore other summarization models on [HuggingFace Hub](https://huggingface.co/models?pipeline_tag=summarization)\n",
    "- Learn about extractive summarization approaches\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Summarization Task Guide](https://huggingface.co/docs/transformers/tasks/summarization)\n",
    "- [BART Paper](https://arxiv.org/abs/1910.13461)\n",
    "- [Summarization Metrics](https://huggingface.co/spaces/evaluate-metric/rouge)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}