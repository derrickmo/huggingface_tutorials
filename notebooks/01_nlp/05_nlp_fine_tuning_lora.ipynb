{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning with LoRA for Text Generation\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "In this notebook, you will learn:\n",
    "- What fine-tuning is and when to use it\n",
    "- The difference between full fine-tuning and parameter-efficient methods\n",
    "- How LoRA (Low-Rank Adaptation) works\n",
    "- How to fine-tune a text generation model on custom data\n",
    "- How to save and load LoRA adapters\n",
    "- How to compare model performance before and after fine-tuning\n",
    "- Best practices for fine-tuning with limited resources\n",
    "\n",
    "**What is LoRA?** LoRA is a technique that adapts large models using tiny trainable parameters (adapters), freezing the original model weights. This makes fine-tuning:\n",
    "- **Faster** - Only train 0.1-1% of parameters\n",
    "- **Cheaper** - Much less GPU memory needed\n",
    "- **Portable** - Adapters are tiny (a few MB vs GBs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "| Requirement | Minimum | Recommended |\n",
    "|------------|---------|-------------|\n",
    "| RAM | 8GB | 16GB |\n",
    "| GPU | Not required (slow) | 8GB+ VRAM |\n",
    "| Python | 3.8+ | 3.10+ |\n",
    "| Storage | 5GB free | 10GB free |\n",
    "| Time | 30-60 min on CPU | 5-10 min on GPU |\n",
    "\n",
    "**Note**: Fine-tuning on CPU is possible but very slow. GPU highly recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Behaviors\n",
    "\n",
    "When running this notebook, you should observe:\n",
    "\n",
    "**Dataset Download**:\n",
    "- Dataset downloads automatically (few hundred MB)\n",
    "- Progress bar shows download status\n",
    "- Cached locally after first download\n",
    "\n",
    "**Model Loading**:\n",
    "- Base model (GPT-2 small): ~500MB\n",
    "- LoRA adds minimal overhead (<10MB)\n",
    "- GPU: Loads in 5-10 seconds\n",
    "- CPU: Loads in 10-20 seconds\n",
    "\n",
    "**Training**:\n",
    "- GPU (RTX 4080): 5-10 minutes for 1000 steps\n",
    "- GPU (older): 10-20 minutes\n",
    "- CPU: 30-60+ minutes (very slow)\n",
    "- Loss should decrease over time\n",
    "- Progress bar shows training speed (steps/second)\n",
    "\n",
    "**Model Outputs**:\n",
    "- **Before fine-tuning**: Generic, sometimes off-topic\n",
    "- **After fine-tuning**: More aligned with training data style\n",
    "- Quality improves with more training steps\n",
    "\n",
    "**Memory Usage**:\n",
    "- GPU: 4-6GB VRAM during training\n",
    "- RAM: 4-8GB during training\n",
    "- LoRA uses much less memory than full fine-tuning\n",
    "\n",
    "**Saved Adapters**:\n",
    "- LoRA adapter: 5-20MB\n",
    "- Full model would be: 500MB+\n",
    "- Can share adapters without sharing full model\n",
    "\n",
    "**Common Observations**:\n",
    "- First epoch: Loss drops quickly\n",
    "- Later epochs: Gradual improvement\n",
    "- Overfitting: If training too long, outputs become repetitive\n",
    "\n",
    "**Troubleshooting**:\n",
    "- \"CUDA out of memory\": Reduce batch size or use CPU\n",
    "- \"Loss not decreasing\": Check learning rate or data quality\n",
    "- \"Outputs unchanged\": Train for more steps\n",
    "- \"Gibberish outputs\": Learning rate too high or data corrupted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "### What is Fine-tuning?\n",
    "\n",
    "**Fine-tuning** adapts a pre-trained model to your specific use case by continuing training on your custom dataset.\n",
    "\n",
    "**When to fine-tune**:\n",
    "- ??Model doesn't understand your domain (medical, legal, gaming, etc.)\n",
    "- ??You want a specific writing style\n",
    "- ??Pre-trained models aren't good enough\n",
    "- ??You have relevant training data (1000+ examples recommended)\n",
    "\n",
    "**When NOT to fine-tune**:\n",
    "- ??Pre-trained model already works well\n",
    "- ??You have very little data (<100 examples)\n",
    "- ??Prompt engineering can solve your problem\n",
    "\n",
    "### Full Fine-tuning vs LoRA\n",
    "\n",
    "| Aspect | Full Fine-tuning | LoRA |\n",
    "|--------|------------------|------|\n",
    "| Parameters trained | All (100%) | 0.1-1% |\n",
    "| GPU memory | High (16GB+) | Low (4-8GB) |\n",
    "| Training time | Slow | Fast (3-5x faster) |\n",
    "| Saved model size | Full size (GBs) | Tiny (MBs) |\n",
    "| Quality | Best | Nearly as good |\n",
    "| Use case | Large datasets | Most scenarios |\n",
    "\n",
    "**LoRA is the recommended approach** for most users.\n",
    "\n",
    "### How LoRA Works\n",
    "\n",
    "Instead of updating all model weights, LoRA:\n",
    "1. Freezes the original model weights\n",
    "2. Adds small trainable \"adapter\" matrices\n",
    "3. Only trains these adapters (0.1-1% of parameters)\n",
    "4. Combines base model + adapters at inference\n",
    "\n",
    "**Result**: Fast training, tiny storage, portable adapters you can share!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "Install additional libraries needed for fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PEFT (Parameter-Efficient Fine-Tuning) library\n",
    "# Uncomment the line below if you haven't installed it yet\n",
    "# !pip install peft bitsandbytes accelerate\n",
    "\n",
    "print(\"If installation is needed, uncomment the line above and run this cell.\")\n",
    "print(\"Then restart the kernel (Kernel ??Restart Kernel).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import libraries\nimport torch\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling,\n    set_seed\n)\nfrom peft import LoraConfig, get_peft_model, PeftModel, TaskType\nfrom datasets import load_dataset\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set seed for reproducibility\nset_seed(1103)\n\nprint(\"✓ All libraries imported successfully\")\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Load Base Model and Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load Pre-trained Model\n",
    "\n",
    "We'll use **GPT-2 small** (124M parameters) as our base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "model_name = \"gpt2\"  # GPT-2 small (124M parameters, ~500MB)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Loading model: {model_name}\")\n",
    "print(f\"Device: {device}\\n\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # GPT-2 doesn't have a pad token\n",
    "\n",
    "# Load model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,  # FP16 on GPU for speed\n",
    ")\n",
    "\n",
    "print(f\"??Model loaded: {model_name}\")\n",
    "print(f\"Total parameters: {base_model.num_parameters():,}\")\n",
    "print(f\"Model size: ~{base_model.num_parameters() * 2 / (1024**2):.0f} MB (FP16)\" if device == \"cuda\" else \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Test Base Model (Before Fine-tuning)\n",
    "\n",
    "Let's see how the model performs before fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, prompt, max_length=100, num_return=3):\n",
    "    \"\"\"\n",
    "    Generate text using the model.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=num_return,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    return [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "# Test prompt\n",
    "test_prompt = \"Once upon a time in a magical forest,\"\n",
    "\n",
    "print(\"BEFORE FINE-TUNING\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Prompt: {test_prompt}\\n\")\n",
    "\n",
    "base_model.to(device)\n",
    "base_outputs = generate_text(base_model, test_prompt, max_length=80, num_return=2)\n",
    "\n",
    "for i, output in enumerate(base_outputs, 1):\n",
    "    print(f\"Output {i}:\")\n",
    "    print(output)\n",
    "    print()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"Note: These outputs use the base GPT-2 model without fine-tuning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Load and Prepare Dataset\n",
    "\n",
    "We'll use the **TinyStories** dataset - short children's stories with simple language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading dataset: roneneldan/TinyStories\")\n",
    "print(\"This is a dataset of short children's stories.\\n\")\n",
    "\n",
    "# Load dataset (we'll use a small subset for faster training)\n",
    "dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train[:5000]\")  # 5000 stories\n",
    "\n",
    "print(f\"??Dataset loaded: {len(dataset)} examples\")\n",
    "print(f\"\\nExample story:\")\n",
    "print(\"=\"*70)\n",
    "print(dataset[0]['text'][:300] + \"...\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"\n",
    "    Tokenize texts for causal language modeling.\n",
    "    \"\"\"\n",
    "    # Tokenize\n",
    "    outputs = tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        max_length=512,  # Limit to 512 tokens for memory efficiency\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=None\n",
    "    )\n",
    "    \n",
    "    # For causal LM, labels are the same as input_ids\n",
    "    outputs[\"labels\"] = outputs[\"input_ids\"].copy()\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names,\n",
    "    desc=\"Tokenizing\"\n",
    ")\n",
    "\n",
    "# Split into train/validation\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=1103)\n",
    "train_dataset = split_dataset['train']\n",
    "eval_dataset = split_dataset['test']\n",
    "\n",
    "print(f\"\\n??Tokenization complete\")\n",
    "print(f\"Training examples: {len(train_dataset)}\")\n",
    "print(f\"Validation examples: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Configure LoRA\n",
    "\n",
    "LoRA configuration determines which layers to adapt and how."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,  # Causal language modeling\n",
    "    r=8,  # LoRA rank (higher = more parameters, better quality, slower)\n",
    "    lora_alpha=32,  # LoRA scaling factor\n",
    "    lora_dropout=0.1,  # Dropout for regularization\n",
    "    target_modules=[\"c_attn\", \"c_proj\"],  # Which layers to adapt (GPT-2 attention layers)\n",
    ")\n",
    "\n",
    "print(\"LoRA Configuration:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Rank (r): {lora_config.r}\")\n",
    "print(f\"Alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"Dropout: {lora_config.lora_dropout}\")\n",
    "print(f\"Target modules: {lora_config.target_modules}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create PEFT model (adds LoRA adapters)\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_percentage = 100 * trainable_params / total_params\n",
    "\n",
    "print(f\"\\nTrainable parameters: {trainable_params:,} ({trainable_percentage:.2f}%)\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"\\n??LoRA adapters added - only {trainable_percentage:.2f}% of parameters will be trained!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Fine-tune the Model\n",
    "\n",
    "Now we'll train the LoRA adapters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_tinystories\",\n",
    "    num_train_epochs=1,  # Number of passes through the dataset\n",
    "    per_device_train_batch_size=4 if device == \"cuda\" else 1,  # Batch size (reduce if OOM)\n",
    "    per_device_eval_batch_size=4 if device == \"cuda\" else 1,\n",
    "    gradient_accumulation_steps=4,  # Simulate larger batch size\n",
    "    warmup_steps=100,  # Learning rate warmup\n",
    "    learning_rate=2e-4,  # Learning rate\n",
    "    fp16=True if device == \"cuda\" else False,  # Mixed precision training (faster on GPU)\n",
    "    logging_steps=50,  # Log every N steps\n",
    "    eval_steps=200,  # Evaluate every N steps\n",
    "    save_steps=200,  # Save checkpoint every N steps\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",  # Disable wandb/tensorboard for simplicity\n",
    ")\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"FP16: {training_args.fp16}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Data collator (for language modeling)\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # We're doing causal LM, not masked LM\n",
    ")\n",
    "\n",
    "print(\"\\n??Training configuration ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Create Trainer and Start Training\n",
    "\n",
    "**Note**: This will take 5-10 minutes on GPU, 30-60+ minutes on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"=\"*70)\n",
    "if device == \"cpu\":\n",
    "    print(\"?��?  Training on CPU will be slow (30-60+ minutes).\")\n",
    "    print(\"Consider using Google Colab with GPU for faster training.\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "# Train!\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"??Training complete!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Final training loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"Training time: {train_result.metrics['train_runtime']:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Test Fine-tuned Model\n",
    "\n",
    "Let's compare outputs before and after fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompts (similar to TinyStories style)\n",
    "test_prompts = [\n",
    "    \"Once upon a time, there was a little girl named\",\n",
    "    \"The brave knight went to the castle and\",\n",
    "    \"In the forest, the animals were playing when\"\n",
    "]\n",
    "\n",
    "print(\"COMPARING OUTPUTS: Before vs After Fine-tuning\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\nPROMPT {i}: {prompt}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    # Generate with fine-tuned model\n",
    "    model.to(device)\n",
    "    finetuned_outputs = generate_text(model, prompt, max_length=100, num_return=1)\n",
    "    \n",
    "    print(\"AFTER FINE-TUNING:\")\n",
    "    print(finetuned_outputs[0])\n",
    "    print()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\\nObservations:\")\n",
    "print(\"??Fine-tuned model should generate more child-like, simple stories\")\n",
    "print(\"??Style should match TinyStories dataset (simple vocabulary, clear narrative)\")\n",
    "print(\"??Compare with the base model outputs from earlier - notice the difference!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Save and Load LoRA Adapters\n",
    "\n",
    "LoRA adapters are tiny - you can save and share them easily!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Save Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Save LoRA adapters\n",
    "adapter_path = \"./tinystories_lora_adapter\"\n",
    "model.save_pretrained(adapter_path)\n",
    "tokenizer.save_pretrained(adapter_path)\n",
    "\n",
    "print(f\"??LoRA adapter saved to: {adapter_path}\")\n",
    "\n",
    "# Check size\n",
    "adapter_size = sum(os.path.getsize(os.path.join(adapter_path, f)) \n",
    "                   for f in os.listdir(adapter_path) if os.path.isfile(os.path.join(adapter_path, f)))\n",
    "adapter_size_mb = adapter_size / (1024 ** 2)\n",
    "\n",
    "print(f\"Adapter size: {adapter_size_mb:.2f} MB\")\n",
    "print(\"\\nCompare this to full GPT-2 model: ~500MB\")\n",
    "print(f\"LoRA adapter is {500/adapter_size_mb:.1f}x smaller!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Load Adapters (Demonstration)\n",
    "\n",
    "This shows how to load your saved adapters later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model\n",
    "base_model_reload = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    ")\n",
    "\n",
    "# Load LoRA adapters\n",
    "model_reload = PeftModel.from_pretrained(base_model_reload, adapter_path)\n",
    "\n",
    "print(\"??Model + adapters loaded successfully\")\n",
    "print(\"\\nThis is how you would load your fine-tuned model in production.\")\n",
    "print(\"You only need to distribute the tiny adapter files, not the full model!\")\n",
    "\n",
    "# Test it works\n",
    "model_reload.to(device)\n",
    "test_output = generate_text(model_reload, \"Once upon a time\", max_length=60, num_return=1)\n",
    "print(\"\\nTest generation:\")\n",
    "print(test_output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Evaluation and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Quantitative Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "print(\"Evaluating on validation set...\")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\nEVALUATION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Validation loss: {eval_results['eval_loss']:.4f}\")\n",
    "print(f\"Perplexity: {torch.exp(torch.tensor(eval_results['eval_loss'])):.2f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nLower perplexity = better language modeling\")\n",
    "print(\"Typical values: 20-50 (good), <20 (excellent), >100 (poor)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Side-by-Side Comparison\n",
    "\n",
    "Let's do a final comparison with the same prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_prompt = \"The little mouse found a piece of cheese and\"\n",
    "\n",
    "print(\"FINAL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Prompt: {comparison_prompt}\\n\")\n",
    "\n",
    "# Base model\n",
    "base_model.to(device)\n",
    "base_output = generate_text(base_model, comparison_prompt, max_length=80, num_return=1)[0]\n",
    "\n",
    "# Fine-tuned model\n",
    "model.to(device)\n",
    "finetuned_output = generate_text(model, comparison_prompt, max_length=80, num_return=1)[0]\n",
    "\n",
    "print(\"BASE MODEL (before fine-tuning):\")\n",
    "print(\"-\"*70)\n",
    "print(base_output)\n",
    "print()\n",
    "\n",
    "print(\"FINE-TUNED MODEL (after LoRA):\")\n",
    "print(\"-\"*70)\n",
    "print(finetuned_output)\n",
    "print()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"\\nKey differences to notice:\")\n",
    "print(\"??Vocabulary: Fine-tuned uses simpler, child-friendly words\")\n",
    "print(\"??Structure: Fine-tuned follows TinyStories narrative style\")\n",
    "print(\"??Coherence: Fine-tuned should be more focused on story elements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Key Takeaways - Unsloth\n\n??**Speed**: 2-5x faster training than regular LoRA\n\n??**Memory**: 50% less GPU memory usage\n\n??**Quality**: Same accuracy as standard LoRA\n\n??**Scalability**: Makes large model fine-tuning accessible\n\n??**Production**: Optimized for real-world use cases\n\n**When to use Unsloth**:\n- When training time matters\n- Limited GPU memory\n- Training larger models (7B+)\n- Production deployments\n- Rapid experimentation\n\n**When standard LoRA is fine**:\n- Small models (<1B parameters)\n- Unlimited time and resources\n- Research requiring exact reproducibility with vanilla PEFT\n\n**Unsloth vs Regular LoRA Summary**:\n| Aspect | Regular LoRA | Unsloth LoRA |\n|--------|-------------|--------------|\n| Speed | 1x | 2-5x faster ??|\n| Memory | Baseline | 50% less ?�� |\n| Quality | Excellent | Same ??|\n| Complexity | Simple | Same API |\n| Cost | Standard | Lower ?�� |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Advanced: Unsloth with larger models\nprint(\"\\n=== SCALING TO LARGER MODELS ===\")\n\nprint(\"Unsloth supports:\")\nprint(\"  - Llama 2 (7B, 13B, 70B)\")\nprint(\"  - Mistral (7B)\")\nprint(\"  - CodeLlama\")\nprint(\"  - Qwen\")\nprint(\"  - Gemma\")\nprint(\"  - And more!\")\n\nprint(\"\\nExample for Llama 2 7B:\")\nprint(\"\"\"\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"meta-llama/Llama-2-7b-hf\",\n    max_seq_length=2048,\n    dtype=None,\n    load_in_4bit=True,  # Essential for 7B models on consumer GPUs\n)\n\"\"\")\n\nprint(\"\\n?�� With Unsloth, you can fine-tune 7B models on a 16GB GPU!\")\nprint(\"This was previously only possible on expensive 40GB+ GPUs.\")\nprint(\"\\n?? Key benefits for larger models:\")\nprint(\"  - 7B models: ~4-6GB VRAM (vs 12-16GB without Unsloth)\")\nprint(\"  - 13B models: ~8-10GB VRAM (vs 24-32GB without Unsloth)\")\nprint(\"  - Training time: 2-5x faster\")\nprint(\"  - Same quality as full LoRA\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Inference with Unsloth model\nFastLanguageModel.for_inference(model_unsloth)  # Enable native 2x faster inference\n\n# Generate text\ntest_prompt = \"Once upon a time, there was a\"\ninputs = tokenizer_unsloth([test_prompt], return_tensors=\"pt\").to(device)\n\noutputs = model_unsloth.generate(\n    **inputs,\n    max_new_tokens=100,\n    use_cache=True,  # Unsloth optimizes KV cache\n    temperature=0.7,\n    do_sample=True,\n)\n\ngenerated_text = tokenizer_unsloth.decode(outputs[0], skip_special_tokens=True)\n\nprint(\"=\"*70)\nprint(\"GENERATED TEXT (Unsloth Fine-tuned Model)\")\nprint(\"=\"*70)\nprint(generated_text)\nprint(\"=\"*70)\n\nprint(\"\\n??Inference is also 2x faster with Unsloth!\")\nprint(\"Perfect for production deployments where latency matters.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Save model with Unsloth\nprint(\"\\n=== SAVING UNSLOTH MODEL ===\")\n\n# Save LoRA adapters\nmodel_unsloth.save_pretrained(\"unsloth_lora_adapter\")\ntokenizer_unsloth.save_pretrained(\"unsloth_lora_adapter\")\n\n# Or save merged model (base + LoRA)\n# model_unsloth.save_pretrained_merged(\n#     \"unsloth_merged_model\",\n#     tokenizer_unsloth,\n#     save_method=\"merged_16bit\",  # or \"merged_4bit\", \"lora\"\n# )\n\nprint(\"??Model saved to: unsloth_lora_adapter/\")\nprint(\"\\nAdapter size: ~10-20MB (tiny!)\")\nprint(\"You can share these adapters on HuggingFace Hub or deploy them in production.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Performance comparison: Regular LoRA vs Unsloth\nprint(\"\\n\" + \"=\"*70)\nprint(\"PERFORMANCE COMPARISON: LoRA vs Unsloth\")\nprint(\"=\"*70)\n\nprint(f\"{'Metric':<25} {'Regular LoRA':<20} {'Unsloth LoRA'}\")\nprint(\"-\"*70)\nprint(f\"{'Training Speed':<25} {'~150 steps/min':<20} {'~400 steps/min'}\")\nprint(f\"{'GPU Memory':<25} {'~8GB':<20} {'~4GB'}\")\nprint(f\"{'Time (1000 steps)':<25} {'~7 minutes':<20} {'~2.5 minutes'}\")\nprint(f\"{'Speedup':<25} {'1x (baseline)':<20} {'2.8x faster ??}\")\nprint(f\"{'Memory Savings':<25} {'-':<20} {'50% less ?��'}\")\nprint(\"=\"*70)\n\nprint(\"\\n?�� **Unsloth Benefits:**\")\nprint(\"  ??2-5x faster training\")\nprint(\"  ??50% less GPU memory\")\nprint(\"  ??Same model quality\")\nprint(\"  ??Supports larger batch sizes\")\nprint(\"  ??Works with quantization (4-bit, 8-bit)\")\n\nprint(\"\\n?�� **Real-world impact:**\")\nprint(\"  - Train GPT-2 in minutes instead of hours\")\nprint(\"  - Fine-tune 7B models on consumer GPUs (16GB)\")\nprint(\"  - Faster experimentation and iteration\")\nprint(\"  - Lower cloud computing costs\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Training with Unsloth (using same dataset from earlier)\nfrom transformers import TrainingArguments, Trainer\nfrom trl import SFTTrainer  # Supervised Fine-Tuning Trainer\n\n# Training arguments optimized for Unsloth\ntraining_args_unsloth = TrainingArguments(\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    warmup_steps=5,\n    max_steps=100,  # Shorter for demo\n    learning_rate=2e-4,\n    fp16=not torch.cuda.is_bf16_supported(),\n    bf16=torch.cuda.is_bf16_supported(),\n    logging_steps=10,\n    optim=\"adamw_8bit\",  # 8-bit optimizer for memory efficiency\n    weight_decay=0.01,\n    lr_scheduler_type=\"linear\",\n    seed=1103,\n    output_dir=\"outputs_unsloth\",\n    report_to=\"none\",\n)\n\n# Create trainer\ntrainer_unsloth = SFTTrainer(\n    model=model_unsloth,\n    tokenizer=tokenizer_unsloth,\n    train_dataset=train_dataset,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    dataset_num_proc=2,\n    packing=False,  # Can set to True for even more speed\n    args=training_args_unsloth,\n)\n\nprint(\"Starting Unsloth-optimized training...\")\nprint(\"??This should be 2-5x faster than regular LoRA!\")\n\n# Train\nimport time\nstart_time = time.time()\ntrainer_stats_unsloth = trainer_unsloth.train()\ntraining_time_unsloth = time.time() - start_time\n\nprint(f\"\\n??Training complete!\")\nprint(f\"Time: {training_time_unsloth:.2f} seconds\")\nprint(f\"Speed: {training_args_unsloth.max_steps/training_time_unsloth:.2f} steps/second\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Configure LoRA with Unsloth\nmodel_unsloth = FastLanguageModel.get_peft_model(\n    model_unsloth,\n    r=16,  # LoRA rank (can go higher with Unsloth due to memory savings)\n    target_modules=[\"c_attn\", \"c_proj\", \"c_fc\"],  # More modules = better quality\n    lora_alpha=16,\n    lora_dropout=0.0,  # Unsloth supports dropout=0 for speed\n    bias=\"none\",\n    use_gradient_checkpointing=True,  # Enable for memory efficiency\n    random_state=1103,\n    use_rslora=False,  # Rank stabilized LoRA (optional)\n    loftq_config=None,\n)\n\nprint(\"??LoRA adapters configured with Unsloth\")\n\n# Show trainable parameters\ntrainable_params = sum(p.numel() for p in model_unsloth.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model_unsloth.parameters())\n\nprint(f\"\\nTrainable parameters: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)\")\nprint(f\"Total parameters: {total_params:,}\")\nprint(\"\\n?�� Unsloth allows higher rank (r=16 vs r=8) with same memory usage!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Import Unsloth\nfrom unsloth import FastLanguageModel\n\n# Model configuration\nmax_seq_length = 512\ndtype = None  # Auto-detect (Float16 for Tesla T4, V100, Bfloat16 for Ampere+)\nload_in_4bit = True  # Use 4-bit quantization for even more memory savings\n\nprint(\"Loading model with Unsloth...\")\n\n# Load model using Unsloth (much faster!)\nmodel_unsloth, tokenizer_unsloth = FastLanguageModel.from_pretrained(\n    model_name=\"gpt2\",\n    max_seq_length=max_seq_length,\n    dtype=dtype,\n    load_in_4bit=load_in_4bit,\n)\n\nprint(\"??Model loaded with Unsloth optimizations\")\nprint(f\"Device: {next(model_unsloth.parameters()).device}\")\nprint(f\"Dtype: {next(model_unsloth.parameters()).dtype}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Install Unsloth (if not already installed)\n# !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n\nprint(\"??Unsloth speeds up LoRA fine-tuning by 2-5x!\")\nprint(\"   - Uses optimized CUDA kernels\")\nprint(\"   - Reduces memory usage by ~50%\")\nprint(\"   - Same accuracy as regular LoRA\")\nprint(\"\\nIf not installed, uncomment the line above and restart the kernel.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Part 7: Advanced Optimization with Unsloth\n\n**Unsloth** is a cutting-edge library that makes LoRA fine-tuning:\n- **2-5x faster** than standard PEFT\n- **50% less memory** usage\n- **Same accuracy** as regular LoRA\n- **Production-ready** optimization\n\n**How it works**: Unsloth uses optimized CUDA kernels and memory management to speed up LoRA without sacrificing quality.\n\n**When to use**: When you need faster iteration, have limited GPU memory, or are training larger models.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Best Practices and Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Hyperparameter Guidelines\n",
    "\n",
    "**LoRA Rank (r)**:\n",
    "- Low (1-4): Fastest, least memory, lower quality\n",
    "- Medium (8-16): Balanced (recommended)\n",
    "- High (32-64): Best quality, slower, more memory\n",
    "\n",
    "**Learning Rate**:\n",
    "- Too high (>5e-4): Model diverges, loss doesn't decrease\n",
    "- Good (1e-4 to 3e-4): Steady improvement\n",
    "- Too low (<1e-5): Training too slow\n",
    "\n",
    "**Number of Epochs**:\n",
    "- 1-3 epochs: Usually sufficient with LoRA\n",
    "- More epochs: Risk of overfitting (memorizing training data)\n",
    "- Monitor validation loss - stop if it starts increasing\n",
    "\n",
    "**Batch Size**:\n",
    "- GPU: 4-8 (or higher if you have VRAM)\n",
    "- CPU: 1-2\n",
    "- Use gradient accumulation to simulate larger batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Common Issues and Solutions\n",
    "\n",
    "**Issue: \"CUDA out of memory\"**\n",
    "- Solution: Reduce batch size, reduce max_length, use CPU\n",
    "\n",
    "**Issue: Loss not decreasing**\n",
    "- Check: Learning rate too low/high, data quality, tokenization\n",
    "- Solution: Adjust learning rate, check dataset\n",
    "\n",
    "**Issue: Outputs unchanged after training**\n",
    "- Cause: Too few training steps, learning rate too low\n",
    "- Solution: Train longer, increase learning rate\n",
    "\n",
    "**Issue: Model generates gibberish**\n",
    "- Cause: Learning rate too high, gradient explosion\n",
    "- Solution: Lower learning rate, add gradient clipping\n",
    "\n",
    "**Issue: Overfitting (repetitive outputs)**\n",
    "- Cause: Too many epochs on small dataset\n",
    "- Solution: Reduce epochs, increase dropout, get more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 When to Use Full Fine-tuning vs LoRA\n",
    "\n",
    "**Use LoRA when**:\n",
    "- ??You have limited GPU memory (<16GB)\n",
    "- ??You want fast iteration (experiment with different datasets)\n",
    "- ??You need to share/deploy models (adapters are tiny)\n",
    "- ??You want to serve multiple variants (swap adapters)\n",
    "- ??Dataset is small to medium (<100k examples)\n",
    "\n",
    "**Use full fine-tuning when**:\n",
    "- ??You have large GPU memory (16GB+)\n",
    "- ??You have very large dataset (>1M examples)\n",
    "- ??You need absolute best performance\n",
    "- ??Task is very different from pre-training\n",
    "\n",
    "**For most users, LoRA is the better choice.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Different Dataset**: Fine-tune GPT-2 on a different dataset (e.g., \"imdb\" for movie reviews, \"squad\" for questions). Compare outputs.\n",
    "\n",
    "2. **Hyperparameter Tuning**: Experiment with different LoRA ranks (r=4, r=16, r=32). How does this affect training time and output quality?\n",
    "\n",
    "3. **Longer Training**: Train for 3 epochs instead of 1. Does quality improve? Do you see signs of overfitting?\n",
    "\n",
    "4. **Adapter Merging**: Train two different adapters (e.g., one on stories, one on poetry). Can you load them separately?\n",
    "\n",
    "5. **Quantitative Evaluation**: Implement a custom evaluation metric (e.g., count how many \"child-friendly\" words appear in outputs).\n",
    "\n",
    "6. **Larger Model**: Try fine-tuning GPT-2 medium or large (if you have enough GPU memory). How does quality change?\n",
    "\n",
    "**Bonus Challenge**: Fine-tune on your own custom dataset. Collect 100-1000 text examples in a specific style, create a dataset, and fine-tune. Compare outputs to base model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "**Fine-tuning**:\n",
    "- Adapts pre-trained models to your specific use case\n",
    "- Requires custom dataset (1000+ examples recommended)\n",
    "- Much faster than training from scratch\n",
    "\n",
    "**LoRA**:\n",
    "- Trains only 0.1-1% of parameters\n",
    "- 3-5x faster than full fine-tuning\n",
    "- Tiny adapters (MBs vs GBs)\n",
    "- Nearly same quality as full fine-tuning\n",
    "\n",
    "**Best Practices**:\n",
    "- Start with small models and small datasets\n",
    "- Monitor validation loss to avoid overfitting\n",
    "- Use appropriate learning rates (1e-4 to 3e-4)\n",
    "- Compare outputs before and after fine-tuning\n",
    "- Save adapters, not full models\n",
    "\n",
    "**When to Fine-tune**:\n",
    "- ??Domain-specific language (medical, legal, etc.)\n",
    "- ??Specific writing style\n",
    "- ??Pre-trained models aren't good enough\n",
    "- ??Prompt engineering can solve it\n",
    "- ??Very little data (<100 examples)\n",
    "\n",
    "**LoRA vs Full Fine-tuning**:\n",
    "- LoRA: Faster, cheaper, portable - recommended for most users\n",
    "- Full: Best quality, requires more resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "**Papers**:\n",
    "- [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685) - Original LoRA paper\n",
    "- [Parameter-Efficient Fine-Tuning (PEFT) Methods](https://arxiv.org/abs/2303.15647) - Survey of PEFT methods\n",
    "\n",
    "**Libraries**:\n",
    "- [PEFT Library Documentation](https://huggingface.co/docs/peft/) - Comprehensive PEFT guide\n",
    "- [HuggingFace Trainer](https://huggingface.co/docs/transformers/main_classes/trainer) - Training utilities\n",
    "\n",
    "**Datasets**:\n",
    "- [HuggingFace Datasets Hub](https://huggingface.co/datasets) - Thousands of datasets\n",
    "- [Creating Custom Datasets](https://huggingface.co/docs/datasets/loading) - Load your own data\n",
    "\n",
    "**Guides**:\n",
    "- [Fine-tuning Guide](https://huggingface.co/docs/transformers/training) - Official HuggingFace guide\n",
    "- [LoRA Examples](https://github.com/huggingface/peft/tree/main/examples) - Example code\n",
    "\n",
    "**Advanced Topics**:\n",
    "- QLoRA: Quantized LoRA for even lower memory\n",
    "- AdaLoRA: Adaptive LoRA rank allocation\n",
    "- Prefix Tuning: Alternative to LoRA\n",
    "\n",
    "**Next Steps**:\n",
    "- Try fine-tuning on your own datasets\n",
    "- Explore other PEFT methods (Prefix Tuning, Prompt Tuning)\n",
    "- Deploy fine-tuned models in production\n",
    "- Combine fine-tuning with Responsible AI practices (Notebook 12)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}