{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 02: Natural Language Processing - Text Classification\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Understand text classification and sentiment analysis\n",
    "- Use pre-trained models for classification tasks\n",
    "- Classify text into predefined categories\n",
    "- Apply models to real-world use cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "### Hardware Requirements\n",
    "\n",
    "| Model Option | Model Name | Size | Min RAM | Recommended Setup | Notes |\n",
    "|--------------|------------|------|---------|-------------------|-------|\n",
    "| **CPU (Small)** | distilbert-base-uncased-finetuned-sst-2-english | 268MB | 2GB | 4GB RAM, CPU | Fast, accurate |\n",
    "| **GPU (Medium)** | bert-base-uncased | 440MB | 4GB | 6GB VRAM (RTX 4080) | More versatile |\n",
    "\n",
    "### Software Requirements\n",
    "- Python 3.8+\n",
    "- Libraries: `transformers`, `torch`\n",
    "- See `requirements.txt` for full list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "**Text Classification** assigns predefined categories or labels to text. Common applications include:\n",
    "\n",
    "**Use Cases:**\n",
    "- **Sentiment Analysis**: Positive, negative, neutral\n",
    "- **Topic Classification**: Sports, politics, technology\n",
    "- **Spam Detection**: Spam or not spam\n",
    "- **Intent Recognition**: For chatbots and virtual assistants\n",
    "\n",
    "**How it works:**\n",
    "1. Text is tokenized and encoded\n",
    "2. Model processes the text through transformer layers\n",
    "3. Output layer produces probability scores for each class\n",
    "4. Highest probability determines the predicted class"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Expected Behaviors\n\n### First Time Running\n- **Model Download**: ~268MB for distilbert (1-3 minutes depending on internet speed)\n- Models cached in `~/.cache/huggingface/hub/`\n- Subsequent runs load instantly from cache\n\n### Setup Cell Output\n```\nPyTorch version: 2.x.x\nCUDA available: True/False\n```\n\n### Model Loading\n```\nLoading distilbert-base-uncased-finetuned-sst-2-english...\nModel loaded successfully!\n```\n- Takes 2-5 seconds on CPU, faster on GPU\n\n### Classification Results Format\n```python\n[{'label': 'POSITIVE', 'score': 0.9998}]\n```\n- **label**: Either 'POSITIVE' or 'NEGATIVE' for sentiment analysis\n- **score**: Confidence between 0 and 1 (higher = more confident)\n\n### Expected Accuracy\n- **Clear sentiment** (e.g., \"I love this!\"): 95-99% confidence\n- **Neutral sentiment** (e.g., \"It's okay\"): 50-70% confidence (may vary)\n- **Mixed sentiment**: Model picks dominant sentiment\n\n### Batch Processing\n- Processing 30 texts should take:\n  - **CPU**: 2-5 seconds\n  - **GPU**: 0.5-1 second\n\n### Zero-Shot Classification\n- Downloads larger model (~1.6GB for bart-large-mnli)\n- Can classify into any categories you provide\n- No additional training needed!\n\n### Common Observations\n- Very positive/negative texts get 95%+ confidence\n- Neutral texts often get 60-80% confidence (acceptable)\n- Emojis and exclamation marks influence predictions\n- Model handles typos reasonably well",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import required libraries\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline, set_seed\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set seed for reproducibility\nset_seed(1103)\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "Choose one of the following models based on your hardware:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHOOSE YOUR MODEL:\n",
    "\n",
    "# Option 1: CPU-friendly (recommended for beginners)\n",
    "MODEL_NAME = \"distilbert-base-uncased-finetuned-sst-2-english\"  # 268MB, sentiment analysis\n",
    "\n",
    "# Option 2: GPU-optimized (uncomment if you have RTX 4080 or similar)\n",
    "# MODEL_NAME = \"bert-base-uncased\"  # 440MB, needs fine-tuning for specific tasks\n",
    "# Note: bert-base-uncased is a base model; for direct classification, use fine-tuned variants\n",
    "\n",
    "print(f\"Selected model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Using Pipeline (Simplest)\n",
    "\n",
    "The `pipeline` API provides an easy interface for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create a sentiment analysis pipeline\nprint(f\"Loading {MODEL_NAME}...\")\nclassifier = pipeline(\n    \"sentiment-analysis\",\n    model=MODEL_NAME,\n    device=0 if torch.cuda.is_available() else -1\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify a single text\n",
    "text = \"I absolutely love this product! It exceeded all my expectations.\"\n",
    "\n",
    "result = classifier(text)\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Prediction: {result[0]['label']}\")\n",
    "print(f\"Confidence: {result[0]['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Classify multiple texts at once (more efficient)\ntexts = [\n    \"This is the worst experience I've ever had.\",\n    \"The movie was okay, nothing special.\",\n    \"Absolutely fantastic! Highly recommend!\",\n    \"I'm not sure how I feel about this.\",\n    \"Terrible service and poor quality.\"\n]\n\nresults = classifier(texts)\n\nprint(\"\\n=== Batch Sentiment Analysis ===\")\nfor text, result in zip(texts, results):\n    print(f\"\\n{result['label']} ({result['score']:.4f})\")\n    print(f\"   Text: {text}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Using Model and Tokenizer Directly (Advanced)\n",
    "\n",
    "For more control and understanding, load components separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Move to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model loaded on: {device}\")\n",
    "print(f\"Number of labels: {model.config.num_labels}\")\n",
    "print(f\"Label mapping: {model.config.id2label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify with detailed output\n",
    "import torch.nn.functional as F\n",
    "\n",
    "text = \"The customer support was incredibly helpful and responsive.\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "\n",
    "# Get predictions\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probabilities = F.softmax(logits, dim=-1)\n",
    "\n",
    "# Get predicted class\n",
    "predicted_class = torch.argmax(probabilities, dim=-1).item()\n",
    "confidence = probabilities[0][predicted_class].item()\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"\\nPredicted class: {model.config.id2label[predicted_class]}\")\n",
    "print(f\"Confidence: {confidence:.4f}\")\n",
    "print(f\"\\nAll probabilities:\")\n",
    "for idx, prob in enumerate(probabilities[0]):\n",
    "    print(f\"  {model.config.id2label[idx]}: {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Product Review Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze product reviews\n",
    "reviews = [\n",
    "    \"Great product! Works exactly as described.\",\n",
    "    \"Disappointed with the quality. Would not recommend.\",\n",
    "    \"Decent for the price, but could be better.\",\n",
    "    \"Exceeded my expectations! Will buy again.\",\n",
    "    \"Arrived damaged and customer service was unhelpful.\"\n",
    "]\n",
    "\n",
    "results = classifier(reviews)\n",
    "\n",
    "# Calculate statistics\n",
    "positive_count = sum(1 for r in results if r['label'] == 'POSITIVE')\n",
    "negative_count = len(results) - positive_count\n",
    "\n",
    "print(\"=== Product Review Analysis ===\")\n",
    "print(f\"\\nTotal reviews: {len(reviews)}\")\n",
    "print(f\"Positive: {positive_count} ({positive_count/len(reviews)*100:.1f}%)\")\n",
    "print(f\"Negative: {negative_count} ({negative_count/len(reviews)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n=== Detailed Results ===\")\n",
    "for review, result in zip(reviews, results):\n",
    "    print(f\"\\n{result['label']} ({result['score']:.3f}): {review}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Social Media Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Analyze social media posts\nposts = [\n    \"Just launched our new feature! So excited to share this with everyone! üöÄ\",\n    \"Another day, another bug. This is getting frustrating.\",\n    \"Thanks for the amazing support team! Issue resolved quickly.\",\n    \"Can't believe how slow the app has become lately.\",\n    \"Love the new update! Everything runs so smoothly now.\"\n]\n\nresults = classifier(posts)\n\nprint(\"=== Social Media Sentiment ===\")\nfor post, result in zip(posts, results):\n    print(f\"\\n[{result['score']:.2f}] {result['label']}: {post}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Interactive Sentiment Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def analyze_sentiment(text):\n    \"\"\"\n    Analyze sentiment with detailed feedback.\n    \"\"\"\n    result = classifier(text)[0]\n    \n    # Interpret confidence\n    confidence = result['score']\n    if confidence > 0.9:\n        strength = \"Very confident\"\n    elif confidence > 0.7:\n        strength = \"Confident\"\n    else:\n        strength = \"Uncertain\"\n    \n    print(f\"\\nText: {text}\")\n    print(f\"Sentiment: {result['label']}\")\n    print(f\"Confidence: {confidence:.4f} ({strength})\")\n    \n    return result\n\n# Test with different texts\ntest_texts = [\n    \"I'm having the best day ever!\",\n    \"This is completely unacceptable.\",\n    \"It's fine, I guess.\"\n]\n\nfor text in test_texts:\n    analyze_sentiment(text)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Other Classification Tasks\n",
    "\n",
    "HuggingFace has models for various classification tasks beyond sentiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-shot classification (classify without training on specific labels)\n",
    "from transformers import pipeline\n",
    "\n",
    "zero_shot_classifier = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=\"facebook/bart-large-mnli\",\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "text = \"I love playing basketball and watching NBA games.\"\n",
    "candidate_labels = [\"sports\", \"technology\", \"politics\", \"entertainment\"]\n",
    "\n",
    "result = zero_shot_classifier(text, candidate_labels)\n",
    "\n",
    "print(f\"Text: {text}\\n\")\n",
    "print(\"Classification results:\")\n",
    "for label, score in zip(result['labels'], result['scores']):\n",
    "    print(f\"  {label}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Using the SST-2 dataset (Stanford Sentiment Treebank)\n# Dataset size: ~7MB, 67k training examples, 872 validation examples\nfrom datasets import load_dataset\n\nprint(\"Loading SST-2 dataset...\")\ndataset = load_dataset(\"sst2\", split=\"validation\")\n\n# Test on a few examples from the dataset\nsample_texts = dataset['sentence'][:5]\nsample_labels = dataset['label'][:5]  # 0=negative, 1=positive\n\nprint(f\"Loaded {len(dataset)} validation examples\\n\")\n\nresults = classifier(sample_texts)\n\nprint(\"=== SST-2 Dataset Classification ===\")\nfor i, (text, true_label, pred) in enumerate(zip(sample_texts, sample_labels, results)):\n    true_sentiment = \"POSITIVE\" if true_label == 1 else \"NEGATIVE\"\n    match = \"‚úì\" if pred['label'] == true_sentiment else \"‚úó\"\n    \n    print(f\"\\n{match} Example {i+1}:\")\n    print(f\"   Text: {text}\")\n    print(f\"   True label: {true_sentiment}\")\n    print(f\"   Predicted: {pred['label']} ({pred['score']:.4f})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Try these challenges to deepen your understanding:\n",
    "\n",
    "1. **Custom Dataset**: Create your own list of texts and analyze their sentiment. Calculate the percentage of positive vs negative.\n",
    "\n",
    "2. **Confidence Threshold**: Filter results to only show predictions with confidence > 0.8. How many are left?\n",
    "\n",
    "3. **Multi-class Classification**: Try using a different model like `cardiffnlp/twitter-roberta-base-emotion` for emotion detection (joy, sadness, anger, etc.)\n",
    "\n",
    "4. **Comparison**: Compare results from `distilbert` vs `bert-base-uncased` (if you have GPU). Are there differences?\n",
    "\n",
    "5. **Real Data**: If you have access to real reviews or tweets, analyze them with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here for exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## State-of-the-Art Open Models (Not Covered)\n\nWhile this notebook uses DistilBERT and BERT for educational purposes, here are **state-of-the-art open-source classification models** you should know about:\n\n### Large Classification Models\n\n**ü§ñ RoBERTa** (Facebook/Meta)\n- Robustly Optimized BERT approach\n- Outperforms BERT on most benchmarks\n- Sizes: base (125M), large (355M)\n- [Model Card](https://huggingface.co/roberta-base) | [Paper](https://arxiv.org/abs/1907.11692)\n- Note: Larger and slower than DistilBERT\n\n**üéØ DeBERTa** (Microsoft)\n- Decoding-enhanced BERT with disentangled attention\n- State-of-the-art on many NLU benchmarks\n- Sizes: base (140M), large (350M), XLarge (900M), V3-large (434M)\n- [Model Card](https://huggingface.co/microsoft/deberta-v3-base) | [Paper](https://arxiv.org/abs/2006.03654)\n- Excellent for: High-accuracy classification tasks\n\n**‚ö° ELECTRA** (Google)\n- More efficient pre-training method\n- Better performance with less compute\n- [Model Card](https://huggingface.co/google/electra-base-discriminator) | [Paper](https://arxiv.org/abs/2003.10555)\n\n### Specialized Classification Models\n\n**üê¶ Twitter-RoBERTa** (Cardiff NLP)\n- Fine-tuned on 58M tweets\n- Excellent for social media sentiment\n- Supports emotion detection (joy, sadness, anger, fear, etc.)\n- [Model Card](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest)\n\n**üì∞ FinBERT** (ProsusAI)\n- Specialized for financial sentiment analysis\n- Trained on financial news and reports\n- [Model Card](https://huggingface.co/ProsusAI/finbert)\n\n**üè• BioBERT** (DMiS Lab)\n- Pre-trained on biomedical literature\n- Best for medical/scientific text classification\n- [Model Card](https://huggingface.co/dmis-lab/biobert-v1.1)\n\n**‚öñÔ∏è Legal-BERT** (SOTA NLP)\n- Specialized for legal document analysis\n- Trained on legal corpora\n- [Model Card](https://huggingface.co/nlpaueb/legal-bert-base-uncased)\n\n### Modern Instruction-Following Models\n\n**ü¶ô Llama-based Classifiers**\n- Fine-tuned Llama 2/3 for classification\n- Can handle complex, nuanced classification tasks\n- Example: [Llama-2-7b-chat-hf with classification adapters](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)\n- Note: Requires 16GB+ GPU\n\n**üåä Mistral-based Classifiers**\n- Efficient 7B parameter classifiers\n- Excellent instruction-following capabilities\n- Can perform zero-shot classification via prompting\n- [Model Card](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)\n\n### Why Not Covered Here?\n\nThese models require:\n- **More compute**: RoBERTa-large needs 8GB+ VRAM\n- **Slower inference**: Larger models take longer\n- **Domain-specific data**: Some need fine-tuning for your use case\n- **Advanced techniques**: May require knowledge of fine-tuning\n\n**Learning Path**:\n1. ‚úÖ Start with DistilBERT (this notebook) - fast, accurate baseline\n2. Try RoBERTa or DeBERTa for better accuracy (if you have GPU)\n3. Use domain-specific models (Twitter-RoBERTa, FinBERT) for specialized tasks\n4. Fine-tune your own classifier (see Notebook 13) for custom categories\n\n### Benchmarks & Leaderboards\n\n- [GLUE Benchmark](https://gluebenchmark.com/leaderboard) - General Language Understanding\n- [SuperGLUE](https://super.gluebenchmark.com/) - More challenging NLU tasks\n- [Papers with Code - Text Classification](https://paperswithcode.com/task/text-classification)\n\n### Practical Recommendations\n\n| Use Case | Recommended Model | Why |\n|----------|------------------|-----|\n| General sentiment | DistilBERT (this notebook) | Fast, accurate, easy |\n| High accuracy needed | DeBERTa-V3 | Best performance |\n| Social media analysis | Twitter-RoBERTa | Domain-specific |\n| Financial text | FinBERT | Specialized vocabulary |\n| Limited resources | DistilBERT, ELECTRA-small | Efficient |\n| Production deployment | RoBERTa-base, DeBERTa-base | Good balance |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "‚úÖ **Text classification** assigns predefined labels to text\n",
    "\n",
    "‚úÖ **Pre-trained models** work well out-of-the-box for common tasks\n",
    "\n",
    "‚úÖ **Batch processing** is more efficient than processing one at a time\n",
    "\n",
    "‚úÖ **Confidence scores** indicate model certainty\n",
    "\n",
    "‚úÖ **Zero-shot classification** works without task-specific training\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Try **Notebook 03**: Text Summarization\n",
    "- Explore [HuggingFace Models](https://huggingface.co/models?pipeline_tag=text-classification) for more classification models\n",
    "- Learn about fine-tuning models on custom datasets\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Text Classification Guide](https://huggingface.co/docs/transformers/tasks/sequence_classification)\n",
    "- [Sentiment Analysis Tutorial](https://huggingface.co/blog/sentiment-analysis-python)\n",
    "- [Zero-Shot Classification](https://huggingface.co/tasks/zero-shot-classification)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}